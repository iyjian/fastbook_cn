{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Stateful' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f68c5f23545f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#hide\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install -Uqq fastbook'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfastbook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# fastbook.setup_book()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fastbook/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"0.0.16\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpkgutil\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfastai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhierarchy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fastai/vision/all.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0maugment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fastai/vision/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxresnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0munet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtvm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fastai/vision/models/unet.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mtorch_basics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fastai/callback/hook.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fastai/basics.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlearner\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fastai/callback/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mfuncs_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mas_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStateful\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mGetAttr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;34m\"Basic class handling tweaks of the training loop by changing a `Learner` in various events\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0m_default\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrun_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrun_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'learn'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Stateful' is not defined"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "!pip install -Uqq fastbook\n",
    "import fastbook\n",
    "fastbook.setup_book()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastbook import *"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[[chapter_intro]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Deep Learning Journey\n",
    "\n",
    "# 你的深度学习之旅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello, and thank you for letting us join you on your deep learning journey, however far along that you may be! In this chapter, we will tell you a little bit more about what to expect in this book, introduce the key concepts behind deep learning, and train our first models on different tasks. It doesn't matter if you don't come from a technical or a mathematical background (though it's okay if you do too!); we wrote this book to make deep learning accessible to as many people as possible.\n",
    "\n",
    "你好，谢谢你让我们加入你的深度学习之旅，不管你可以走多远！在本章中，我们将向你介绍本书的更多动机，介绍深度学习背后的关键概念，并在不同的任务中训练我们的第一个模型。如果你没有技术或数学背景也没关系 (当然你有也没关系!); 我们写这本书是为了让尽可能多的人能够接触到深度学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Is for Everyone\n",
    "\n",
    "## 深度学习适合所有人"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of people assume that you need all kinds of hard-to-find stuff to get great results with deep learning, but as you'll see in this book, those people are wrong. <<myths>> is a list of a few thing you *absolutely don't need* to do world-class deep learning.\n",
    "\n",
    "很多人以为要用深度学习得出好的结果就必须得用到那些很难找到的东西，但是你将在这本书里发现，那些人都是错的。“所谓的真相”其实是一系列你在进行顶级难度的深度学习时 *绝对不需要* 的事情。\n",
    "\n",
    "```asciidoc\n",
    "[[myths]]\n",
    ".What you don't need to do deep learning\n",
    "[options=\"header\"]\n",
    "|======\n",
    "| Myth (don't need) | Truth\n",
    "| Lots of math | Just high school math is sufficient\n",
    "| Lots of data | We've seen record-breaking results with <50 items of data\n",
    "| Lots of expensive computers | You can get what you need for state of the art work for free\n",
    "|======\n",
    "```\n",
    "\n",
    "\n",
    "```asciidoc\n",
    "[[所谓的真相]]\n",
    ".深度学习中你不需要做的事情\n",
    "[options = \"header\"]\n",
    "|======\n",
    "| 所谓的真相(不需要)  | 真相\n",
    "| 很多数学知识       | 高中数学就够了\n",
    "| 大量数据          | 我们已经看到了破纪录的由<50项数据获得的结果\n",
    "| 大量昂贵的计算机   | 最尖端工作所需的东西你都可以免费获得\n",
    "|======\n",
    "```\n",
    "\n",
    "Deep learning is a computer technique to extract and transform data–-with use cases ranging from human speech recognition to animal imagery classification–-by using multiple layers of neural networks. Each of these layers takes its inputs from previous layers and progressively refines them. The layers are trained by algorithms that minimize their errors and improve their accuracy. In this way, the network learns to perform a specified task. We will discuss training algorithms in detail in the next section.\n",
    "\n",
    "深度学习是一种抽取和转换数据的计算机技术--我们将从人类语音识别到动物图像分类作为案例——这一技术使用多层神经网络。每一层都把上一层的信息作为输入，并逐步的提炼。这些层由算法进行训练，通过最小化误差的方法来提高它们的准确性。通过这种方式，网络可以学习到执行特定的任务。我们将在下一节中详细讨论训练的算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning has power, flexibility, and simplicity. That's why we believe it should be applied across many disciplines. These include the social and physical sciences, the arts, medicine, finance, scientific research, and many more. To give a personal example, despite having no background in medicine, Jeremy started Enlitic, a company that uses deep learning algorithms to diagnose illness and disease. Within months of starting the company, it was announced that its algorithm could identify malignant tumors [more accurately than radiologists](https://www.nytimes.com/2016/02/29/technology/the-promise-of-artificial-intelligence-unfolds-in-small-steps.html).\n",
    "\n",
    "深度学习非常强大、灵活且简单。这就是为什么我们认为它应该应用于多种学科。其中包括社会和物理科学、艺术、医学、金融、科学研究等等。举个例子，尽管没有医学背景，Jeremy还是创办了Enlitic公司，该公司使用深度学习算法来诊断疾病。在创办公司的几个月内，宣布其算法可以 “比放射科医生更准确” ( https://www.nytimes.com/2016/02/29/technology/the-promise-of-artificial-intelligence-unfolds-in-small-steps.html )。\n",
    "\n",
    "Here's a list of some of the thousands of tasks in different areas at which deep learning, or methods heavily using deep learning, is now the best in the world:\n",
    "\n",
    "下面列出了在不同领域中成千上万个任务中的一些，在这些领域中，深度学习或大量使用深度学习的方法现今是世界上最好的：\n",
    "\n",
    "- Natural language processing (NLP):: Answering questions; speech recognition; summarizing documents; classifying documents; finding names, dates, etc. in documents; searching for articles mentioning a concept\n",
    "- Computer vision:: Satellite and drone imagery interpretation (e.g., for disaster resilience); face recognition; image captioning; reading traffic signs; locating pedestrians and vehicles in autonomous vehicles\n",
    "- Medicine:: Finding anomalies in radiology images, including CT, MRI, and X-ray images; counting features in pathology slides; measuring features in ultrasounds; diagnosing diabetic retinopathy\n",
    "- Biology:: Folding proteins; classifying proteins; many genomics tasks, such as tumor-normal sequencing and classifying clinically actionable genetic mutations; cell classification; analyzing protein/protein interactions\n",
    "- Image generation:: Colorizing images; increasing image resolution; removing noise from images; converting images to art in the style of famous artists\n",
    "- Recommendation systems:: Web search; product recommendations; home page layout\n",
    "- Playing games:: Chess, Go, most Atari video games, and many real-time strategy games\n",
    "- Robotics:: Handling objects that are challenging to locate (e.g., transparent, shiny, lacking texture) or hard to pick up\n",
    "- Other applications:: Financial and logistical forecasting, text to speech, and much more...\n",
    "\n",
    "\n",
    "- 自然语言处理 (NLP):: 回答问题; 语音识别; 汇总文档; 对文档进行分类; 在文档中查找姓名、日期等; 搜索提及某个概念的文章\n",
    "- 计算机视象:: 卫星和无人机图像解读 (例如，抗灾能力); 人脸识别; 图像字幕; 阅读交通标志; 在自动驾驶车辆时定位行人和车辆\n",
    "- 医学:: 在放射影像中发现异常，包括CT，MRI和x光片; 计算病理学切片中的特征; 测量超声特征; 诊断糖尿病性视网膜病变\n",
    "- 生物学:: 蛋白质折叠; 蛋白质分类; 各种基因组学任务，如肿瘤正常测序以及临床可操作的基因突变分类; 细胞分类; 蛋白质/蛋白质相互作用分析\n",
    "- 图像生成:: 图像着色; 增强图像分辨率; 消除图像中的噪点; 将图像转换为著名艺术家风格的艺术\n",
    "- 推荐系统:: 网页搜索; 产品推荐; 主页布局\n",
    "- 玩游戏:: 国际象棋，围棋，大多数雅达利视频游戏，以及各种实时战略游戏\n",
    "- 机器人技术:: 处理难以识别的物体 (例如，透明、闪光、缺乏纹理) 或难以提取的物体\n",
    "- 其他应用:: 财务和物流预测，文本转化语音，等等..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is remarkable is that deep learning has such varied application yet nearly all of deep learning is based on a single type of model, the neural network.\n",
    "\n",
    "\n",
    "值得注意的是，深度学习有如此多样的应用，但几乎所有的深度学习都基于同一个类型的模型，神经网络。\n",
    "\n",
    "But neural networks are not in fact completely new. In order to have a wider perspective on the field, it is worth it to start with a bit of history.\n",
    "\n",
    "但是神经网络实际上并不是全新的事物。 为了从更广泛的视角了解该领域，我们需要从了解一点它的历史开始。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks: A Brief History\n",
    "\n",
    "## 神经网络: 简史"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 1943 Warren McCulloch, a neurophysiologist, and Walter Pitts, a logician, teamed up to develop a mathematical model of an artificial neuron. In their [paper](https://link.springer.com/article/10.1007/BF02478259) \"A Logical Calculus of the Ideas Immanent in Nervous Activity\" they declared that:\n",
    "\n",
    "\n",
    "1943，神经生理学家Warren McCulloch和逻辑学家Walter Pitts合作开发了人工神经元的数学模型。在他们的[论文](https://link.springer.com/article/10.1007/BF02478259) 《神经活动中固有思想的逻辑演算》中，他们宣称:\n",
    "\n",
    "> : Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms.\n",
    "\n",
    ">: 由于神经活动的 “全有或无” 特征，可以使用命题逻辑来处理神经事件及其之间的关系。可以看出，每个网络的行为都可以用这些术语来描述。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "McCulloch and Pitts realized that a simplified model of a real neuron could be represented using simple addition and thresholding, as shown in <<neuron>>. Pitts was self-taught, and by age 12, had received an offer to study at Cambridge University with the great Bertrand Russell. He did not take up this invitation, and indeed throughout his life did not accept any offers of advanced degrees or positions of authority. Most of his famous work was done while he was homeless. Despite his lack of an officially recognized position and increasing social isolation, his work with McCulloch was influential, and was taken up by a psychologist named Frank Rosenblatt.\n",
    "\n",
    "McCulloch和Pitts意识到一个真实神经元的简化模型可以用简单的加法和阈值表示，如 《神经元》所示。Pitts是自学成才的，12岁时，他收到了一份邀请，与伟大的Bertrand Russell一起在剑桥大学学习。他没有接受这一邀请，事实上，在他的一生中，他没有接受任何高级学位或权威职位的邀请。他大部分著名的工作都是在他无家可归的时候完成的。尽管他缺乏官方认可的职位，并且日益与社会隔离，但他与McCulloch的合作颇具影响力，并被一位名叫Frank Rosenblatt的心理学家采用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Natural and artificial neurons\" width=\"500\" caption=\"Natural and artificial neurons\" src=\"images/chapter7_neuron.png\" id=\"neuron\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rosenblatt further developed the artificial neuron to give it the ability to learn. Even more importantly, he worked on building the first device that actually used these principles, the Mark I Perceptron. In \"The Design of an Intelligent Automaton\" Rosenblatt wrote about this work: \"We are now about to witness the birth of such a machine–-a machine capable of perceiving, recognizing and identifying its surroundings without any human training or control.\" The perceptron was built, and was able to successfully recognize simple shapes.\n",
    "\n",
    "\n",
    "Rosenblatt进一步开发了人工神经元，使其具有学习能力。更重要的是，他致力于构建第一个实际使用这些原理的设备，Mark I感知器。在 《智能自动机的设计》 中，Rosenblatt写道: “我们现在即将见证这样一台机器的诞生——— 一台能够感知的机器，可以在没有任何人类训练或控制的情况下认知和识别周围环境。\"感知器已经建成，并且能够成功认知简单的形状。\n",
    "\n",
    "An MIT professor named Marvin Minsky (who was a grade behind Rosenblatt at the same high school!), along with Seymour Papert, wrote a book called _Perceptrons_ (MIT Press), about Rosenblatt's invention. They showed that a single layer of these devices was unable to learn some simple but critical mathematical functions (such as XOR). In the same book, they also showed that using multiple layers of the devices would allow these limitations to be addressed. Unfortunately, only the first of these insights was widely recognized. As a result, the global academic community nearly entirely gave up on neural networks for the next two decades.\n",
    "\n",
    "一位名叫Marvin Minsky的麻省理工学院教授 (他是Rosenblatt同一所高中的学弟!)，与Seymour Papert一起，写了一本名为《感知器》(麻省理工学院出版社) 的书，内容是关于Rosenblatt的发明。他们表明，这些设备的单层无法学习一些简单但关键的数学函数 (如XOR)。在同一本书中，他们还表明，使用多层设备可以解决这些限制。不幸的是，这些见解中只有第一个得到了广泛认可。结果，全球学术界在接下来的二十年里几乎完全放弃了神经网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the most pivotal work in neural networks in the last 50 years was the multi-volume *Parallel Distributed Processing* (PDP) by David Rumelhart, James McClellan, and the PDP Research Group, released in 1986 by MIT Press. Chapter 1 lays out a similar hope to that shown by Rosenblatt:\n",
    "\n",
    "\n",
    "也许过去50年里神经网络中最关键的工作是多卷《并行分布式处理》(PDP)，由David Rumelhart，James McClellan和PDP研究小组所著，于1986年由麻省理工学院出版社发行。第一章提出了一个与Rosenblatt类似的期待:\n",
    "\n",
    "> : People are smarter than today's computers because the brain employs a basic computational architecture that is more suited to deal with a central aspect of the natural information processing tasks that people are so good at. ...We will introduce a computational framework for modeling cognitive processes that seems… closer than other frameworks to the style of computation as it might be done by the brain.\n",
    "\n",
    "\n",
    ">: 人们比今天的计算机更聪明，因为大脑采用了一种基本的计算架构，这种架构更适合处理人们擅长的自然信息处理任务的核心方面内容...我们将引入一个计算构架，来对认知过程进行建模，该构架似乎...比其他构架更接近于大脑可以完成的计算方式。\n",
    "\n",
    "The premise that PDP is using here is that traditional computer programs work very differently to brains, and that might be why computer programs had been (at that point) so bad at doing things that brains find easy (such as recognizing objects in pictures). The authors claimed that the PDP approach was \"closer \n",
    "than other frameworks\" to how the brain works, and therefore it might be better able to handle these kinds of tasks.\n",
    "\n",
    "\n",
    "并行分布式处理在这里使用的前提是传统的计算机程序与大脑的工作方式非常不同，这可能就是为什么计算机程序 (在当时) 不擅长做大脑容易发现的事情 (比如识别图片中的物体)。作者声称并行分布式处理方法 “比其他构架更接近“大脑的工作方式，因此它可能能够更好地处理这些任务。\n",
    "\n",
    "In fact, the approach laid out in PDP is very similar to the approach used in today's neural networks. The book defined parallel distributed processing as requiring:\n",
    "\n",
    "\n",
    "事实上，并行分布式处理中列出的方法与当今神经网络中使用的方法非常相似。本书将并行分布式处理定义为需要:\n",
    "\n",
    "1. A set of *processing units*\n",
    "1. A *state of activation*\n",
    "1. An *output function* for each unit \n",
    "1. A *pattern of connectivity* among units \n",
    "1. A *propagation rule* for propagating patterns of activities through the network of connectivities \n",
    "1. An *activation rule* for combining the inputs impinging on a unit with the current state of that unit to produce an output for the unit\n",
    "1. A *learning rule* whereby patterns of connectivity are modified by experience \n",
    "1. An *environment* within which the system must operate\n",
    "\n",
    "\n",
    "1. 一套 *处理单元*\n",
    "1. 一种 *激活状态*\n",
    "1. 每个单元的 *输出功能*\n",
    "1. 一种 *单元间连通的模式*\n",
    "1. 一个 *传播规则*，用于连接网络传播活动模式\n",
    "1. 一个 *激活规则*，用于将冲击在一个单元上的输入与该单元的当前状态相结合，以产生该单元的输出\n",
    "1. 一个 *学习规则*，从而根据经验修改连接方式\n",
    "1. 一个 *运行环境*，系统必须在其中运行 \n",
    "\n",
    "We will see in this book that modern neural networks handle each of these requirements.\n",
    "\n",
    "\n",
    "在本书中，我们将看到现代神经网络可以处理所有这些要求。\n",
    "\n",
    "In the 1980's most models were built with a second layer of neurons, thus avoiding the problem that had been identified by Minsky and Papert (this was their \"pattern of connectivity among units,\" to use the framework above). And indeed, neural networks were widely used during the '80s and '90s for real, practical projects. However, again a misunderstanding of the theoretical issues held back the field. In theory, adding just one extra layer of neurons was enough to allow any mathematical function to be approximated with these neural networks, but in practice such networks were often too big and too slow to be useful.\n",
    "\n",
    "\n",
    "在20世纪80年代，大多数模型都是用第二层神经元构建的，从而避免了Minsky和Papert指出的问题 (使用上面的框架，这是他们的 “单元间连接模型”，)。事实上，神经网络在80年代和90年代被广泛用于实际的实践项目。然而，对理论问题的误解再次阻碍了这个领域的发展。理论上，只增加一层额外的神经元就足以让任何数学函数与这些神经网络近似，但是在实践中，这种网络通常太大太慢，没有用。\n",
    "\n",
    "Although researchers showed 30 years ago that to get practical good performance you need to use even more layers of neurons, it is only in the last decade that this principle has been more widely appreciated and applied. Neural networks are now finally living up to their potential, thanks to the use of more layers, coupled with the capacity to do so due to improvements in computer hardware, increases in data availability, and algorithmic tweaks that allow neural networks to be trained faster and more easily. We now have what Rosenblatt promised: \"a machine capable of perceiving, recognizing, and identifying its surroundings without any human training or control.\"\n",
    "\n",
    "\n",
    "尽管研究人员在30年前就表明，为了获得良好的实用性能，需要使用更多层神经元，但直到最近十年，这一原则才得到更广泛的认可和应用。现在，神经网络终于发挥了它们的潜力，这要归功于更多层的使用，再加上计算机硬件的改进，数据可用性的增加，以及算法调整，使得神经网络能够更快、更容易地得到训练。我们于是有了Rosenblatt所承诺的: “一台无需任何人类训练或控制，就能够感受、认知和识别周围环境的机器。”\n",
    "\n",
    "This is what you will learn how to build in this book. But first, since we are going to be spending a lot of time together, let's get to know each other a bit… \n",
    "\n",
    "这就是你将在本书中学习如何构建的内容。但是首先，既然要在一起度过很多时间，让我们先互相了解一下……"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who We Are\n",
    "\n",
    "## 我们是谁"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are Sylvain and Jeremy, your guides on this journey. We hope that you will find us well suited for this position.\n",
    "\n",
    "\n",
    "我们是Sylvain和Jeremy，是你们这次学习旅程的向导。希望你会发现我们非常适合此职位。\n",
    "\n",
    "Jeremy has been using and teaching machine learning for around 30 years. He started using neural networks 25 years ago. During this time, he has led many companies and projects that have machine learning at their core, including founding the first company to focus on deep learning and medicine, Enlitic, and taking on the role of President and Chief Scientist of the world's largest machine learning community, Kaggle. He is the co-founder, along with Dr. Rachel Thomas, of fast.ai, the organization that built the course this book is based on.\n",
    "\n",
    "\n",
    "Jeremy已经使用和教授机器学习大约30年了。他25年前就开始使用神经网络了.在此期间，他主导了许多以机器学习为核心的公司和项目，包括创建了第一家专注于深度学习和医学的公司，Enlitic，并担任世界上最大的机器学习社区卡歌的总裁兼首席科学家。他同Rachel Thomas博士一起为fast.ai的联合创始人，也是本书所基于的课程的组织机构。\n",
    "\n",
    "From time to time you will hear directly from us, in sidebars like this one from Jeremy:\n",
    "\n",
    "你会不时地直接从我们这里听到，在侧边栏里Jeremy这样说:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> J: Hi everybody, I'm Jeremy! You might be interested to know that I do not have any formal technical education. I completed a BA, with a major in philosophy, and didn't have great grades. I was much more interested in doing real projects, rather than theoretical studies, so I worked full time at a management consulting firm called McKinsey and Company throughout my university years. If you're somebody who would rather get their hands dirty building stuff than spend years learning abstract concepts, then you will understand where I am coming from! Look out for sidebars from me to find information most suited to people with a less mathematical or formal technical background—that is, people like me…\n",
    "\n",
    "> J: 大家好，我是Jerymy!你可能会有兴趣知道，其实我没有受过任何正规的技术教育。我获得了文学学士学位，主修哲学，但是成绩不好。比起理论研究，我更感兴趣的是做真正的项目。所以我在大学期间一直在一家名为McKinsey and Company的管理咨询公司做全职工作。如果你是一个更愿意动手去做而不是花几年时间学习抽象概念的人，那么你就会明白我是什么人!寻找我的侧边栏，找到最适合数学或正式技术背景较低的人 -- 也就是说，像我这样的人所需要的信息……"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sylvain, on the other hand, knows a lot about formal technical education. In fact, he has written 10 math textbooks, covering the entire advanced French maths curriculum!\n",
    "\n",
    "另一方面，Sylvain非常了解正规的技术教育。事实上，他已经写了10本数学教科书，涵盖了整个法国高级数学课程!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> S: Unlike Jeremy, I have not spent many years coding and applying machine learning algorithms. Rather, I recently came to the machine learning world, by watching Jeremy's fast.ai course videos. So, if you are somebody who has not opened a terminal and written commands at the command line, then you will understand where I am coming from! Look out for sidebars from me to find information most suited to people with a more mathematical or formal technical background, but less real-world coding experience—that is, people like me…\n",
    "\n",
    "> S: 与Jeremy不同，我没有花很多年时间编码和应用机器学习算法。相反，我最近通过观看杰里米的fast.ai课程视频来到了机器学习世界。所以，如果你是一个没有打开过终端并在命令行写命令的人，那么你就会明白我是什么人!寻找我的侧边栏，找到最适合有更多数学或正式技术背景但没有太多真实编码经验的人-- 也就是说，像我这样的人所需要的信息……"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fast.ai course has been studied by hundreds of thousands of students, from all walks of life, from all parts of the world. Sylvain stood out as the most impressive student of the course that Jeremy had ever seen, which led to him joining fast.ai, and then becoming the coauthor, along with Jeremy, of the fastai software library.\n",
    "\n",
    "\n",
    "来自世界各地的数十万各行各业的学生已经学习了fast.ai课程。Sylvain是Jeremy在课程中见过的最令人印象深刻的学生，因此他很快加入fast.ai，然后与杰里米一起成为fastai软件库的合著者。\n",
    "\n",
    "All this means that between us you have the best of both worlds: the people who know more about the software than anybody else, because they wrote it; an expert on math, and an expert on coding and machine learning; and also people who understand both what it feels like to be a relative outsider in math, and a relative outsider in coding and machine learning.\n",
    "\n",
    "\n",
    "所有这些意味着在我们之间，你拥有两全其美的优势: 那些比任何人都更了解软件的人，因为他们写了软件; 数学专家，编码和机器学习专家;以及既了解数学的相对局外人又熟悉编码和机器学习的相对局外人的人。\n",
    "\n",
    "Anybody who has watched sports knows that if you have a two-person commentary team then you also need a third person to do \"special comments.\" Our special commentator is Alexis Gallagher. Alexis has a very diverse background: he has been a researcher in mathematical biology, a screenplay writer, an improv performer, a McKinsey consultant (like Jeremy!), a Swift coder, and a CTO.\n",
    "\n",
    "任何看过体育的人都知道，如果你有一个两人评论团队，那么你也需要第三个人来做 “特别评论”。我们的特别评论员是Alexis Gallagher。Alexis有着非常多样化的背景: 他曾是数学生物学的研究员、剧本作家、即兴表演演员、McKinsey顾问 (跟Jeremy一样!)，一个Swift编程员和一个首席技术官。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A: I've decided it's time for me to learn about this AI stuff! After all, I've tried pretty much everything else… But I don't really have a background in building machine learning models. Still… how hard can it be? I'm going to be learning throughout this book, just like you are. Look out for my sidebars for learning tips that I found helpful on my journey, and hopefully you will find helpful too.\n",
    "\n",
    "> A: 我决定是时候了解人工智能了!毕竟，我已经尝试了几乎所有其他的东西…… 但是我并没有建立机器学习模型的背景。还有…… 能有多难？我会像你一样在这本书里学习。注意我的侧边栏，查看我在学习过程中发现有用的学习技巧，希望你也能用得上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Learn Deep Learning\n",
    "\n",
    "## 如何学习深度学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Harvard professor David Perkins, who wrote _Making Learning Whole_ (Jossey-Bass), has much to say about teaching. The basic idea is to teach the *whole game*. That means that if you're teaching baseball, you first take people to a baseball game or get them to play it. You don't teach them how to wind twine to make a baseball from scratch, the physics of a parabola, or the coefficient of friction of a ball on a bat.\n",
    "\n",
    "\n",
    "哈佛大学教授David Perkins写了《让学习变得完整》 (Jossey-Bass)，关于教学，他有很多要说的。基本的想法是教 *整个游戏*。意思就是如果你在教棒球，你首先会带人们去看棒球比赛或者让他们去玩棒球。你不会教他们如何缠绕麻绳从头开始制作棒球，抛物线的物理学，或者球棒上的摩擦系数。\n",
    "\n",
    "Paul Lockhart, a Columbia math PhD, former Brown professor, and K-12 math teacher, imagines in the influential [essay](https://www.maa.org/external_archive/devlin/LockhartsLament.pdf) \"A Mathematician's Lament\" a nightmare world where music and art are taught the way math is taught. Children are not allowed to listen to or play music until they have spent over a decade mastering music notation and theory, spending classes transposing sheet music into a different key. In art class, students study colors and applicators, but aren't allowed to actually paint until college. Sound absurd? This is how math is taught–-we require students to spend years doing rote memorization and learning dry, disconnected *fundamentals* that we claim will pay off later, long after most of them quit the subject.\n",
    "\n",
    "\n",
    "Paul Lockhart，哥伦比亚大学数学博士，前布朗大学教授，K-12数学老师，在其极具影响力的[文章](https://www.maa.org/external_archive/devlin/LockhartsLament.pdf) 中想象“数学家之哀”，那是一个噩梦般的世界，音乐和艺术的教学方式与数学相同。孩子们不允许听音乐或演奏音乐，直到他们花了十多年的时间掌握音乐符号和理论，花费课程将乐谱转换成不同的琴键。在美术课上，学生学习颜色和涂抹器，但在大学之前不允许实际绘画。听起来很荒谬？这就是数学的教学方式 -- 我们要求学生花数年时间死记硬背，学习枯燥、脱节的 *基础知识*，我们声称学习这些基础知识以后会有回报，在他们大多数人退出该学科很久之后。\n",
    "\n",
    "Unfortunately, this is where many teaching resources on deep learning begin–-asking learners to follow along with the definition of the Hessian and theorems for the Taylor approximation of your loss functions, without ever giving examples of actual working code. We're not knocking calculus. We love calculus, and Sylvain has even taught it at the college level, but we don't think it's the best place to start when learning deep learning!\n",
    "\n",
    "\n",
    "不幸的是，这是许多关于深度学习的教学资源开始的地方 -- 要求学习者遵循Hessian的定义和损失函数的泰勒近似定理，而不给出实际工作代码的示例。我们不是在敲微积分。我们喜欢微积分，Sylvain甚至在大学里教过微积分，但是我们不认为这是学习深度学习的最佳起点!\n",
    "\n",
    "In deep learning, it really helps if you have the motivation to fix your model to get it to do better. That's when you start learning the relevant theory. But you need to have the model in the first place. We teach almost everything through real examples. As we build out those examples, we go deeper and deeper, and we'll show you how to make your projects better and better. This means that you'll be gradually learning all the theoretical foundations you need, in context, in such a way that you'll see why it matters and how it works.\n",
    "\n",
    "\n",
    "在深度学习中，如果你有动力修复你的模型，让它做得更好，这真的很有帮助。这就是你开始学习相关理论的时候。但是你首先需要有这个模型。我们教所有的东西几乎都通过真实的例子进行。当我们构建这些示例时，我们会越来越深入，并将向你展示如何改进你的项目使之越来越好。这意味着你将在上下文中逐步学习所需的所有理论基础，以便你理解其重要性以及其工作原理。\n",
    "\n",
    "So, here's our commitment to you. Throughout this book, we will follow these principles:\n",
    "\n",
    "\n",
    "这就是我们对你的承诺。在本书中，我们将遵循以下原则:\n",
    "\n",
    "- Teaching the *whole game*. We'll start by showing how to use a complete, working, very usable, state-of-the-art deep learning network to solve real-world problems, using simple, expressive tools. And then we'll gradually dig deeper and deeper into understanding how those tools are made, and how the tools that make those tools are made, and so on…\n",
    "- Always teaching through examples. We'll ensure that there is a context and a purpose that you can understand intuitively, rather than starting with algebraic symbol manipulation.\n",
    "- Simplifying as much as possible. We've spent years building tools and teaching methods that make previously complex topics very simple.\n",
    "- Removing barriers. Deep learning has, until now, been a very exclusive game. We're breaking it open, and ensuring that everyone can play.\n",
    "\n",
    "- 教你 *整个游戏*。我们将首先展示如何使用简单，富于表现力的工具，使用完整，有效，非常有用的最新技术深度学习网络来解决实际问题。然后，我们将逐渐深入了解这些工具的制造方式，以及制造这些工具的工具的制造方式，等等。\n",
    "- 始终通过实例进行教学。我们将确保你可以直观地理解上下文和目的，而不是从代数符号的操作开始。\n",
    "- 尽可能简化.我们花了数年时间构建工具和教学方法，使以前复杂的主题变得非常简单。\n",
    "- 消除障碍。直到现在，深度学习一直是一个非常排外的游戏。我们要打开它，确保每个人都能玩。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hardest part of deep learning is artisanal: how do you know if you've got enough data, whether it is in the right format, if your model is training properly, and, if it's not, what you should do about it? That is why we believe in learning by doing. As with basic data science skills, with deep learning you only get better through practical experience. Trying to spend too much time on the theory can be counterproductive. The key is to just code and try to solve problems: the theory can come later, when you have context and motivation.\n",
    "\n",
    "\n",
    "深度学习最难的部分是工艺部分: 你如何知道你是否有足够的数据，它是否是正确的格式，你的模型是否训练正确，以及，如果不是，你该怎么办？这就是为什么我们信奉边做边学。与基本数据科学技能一样，在深度学习里，你只能通过实践经验获得更好的效果。试图在理论上花太多时间可能会适得其反。关键是编码并尝试解决问题: 当你有背景知识和动力时，理论可以在以后出现。\n",
    "\n",
    "There will be times when the journey will feel hard. Times where you feel stuck. Don't give up! Rewind through the book to find the last bit where you definitely weren't stuck, and then read slowly through from there to find the first thing that isn't clear. Then try some code experiments yourself, and Google around for more tutorials on whatever the issue you're stuck with is—often you'll find some different angle on the material might help it to click. Also, it's expected and normal to not understand everything (especially the code) on first reading. Trying to understand the material serially before proceeding can sometimes be hard. Sometimes things click into place after you get more context from parts down the road, from having a bigger picture. So if you do get stuck on a section, try moving on anyway and make a note to come back to it later.\n",
    "\n",
    "\n",
    "有时候学习过程会很艰难。你感到卡住的时候。不要放弃!往回翻阅这本书，找到你绝对没有被卡住的最后一点，然后从那里慢慢读一遍，找到不清楚的第一件事情。然后自己尝试一些代码实验，并在谷歌上搜索更多关于你所遇到的问题的教程-通常你会发现材料上的一些不同角度可能有助于理解。此外，第一遍阅读时无法理解所有的东西 (尤其是代码) 是意料之中的，很正常。在继续之前试图连续理解材料有时会很困难。有时候，当你从未来的部分以及更广阔的远景中获得更多的背景知识后，就能够灵光一闪地理解。所以，如果你被困在一个章节，试着继续前进，并做个记录，稍后再回来。\n",
    "\n",
    "Remember, you don't need any particular academic background to succeed at deep learning. Many important breakthroughs are made in research and industry by folks without a PhD, such as [\"Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks\"](https://arxiv.org/abs/1511.06434)—one of the most influential papers of the last decade—with over 5,000 citations, which was written by Alec Radford when he was an undergraduate. Even at Tesla, where they're trying to solve the extremely tough challenge of making a self-driving car, CEO [Elon Musk says](https://twitter.com/elonmusk/status/1224089444963311616):\n",
    "\n",
    "\n",
    "记住，你不需要任何特定的学术背景就能在深度学习中取得成功。没有博士学位的人在研究和实业方面取得了许多重要突破，例如 [“深度卷积生成对抗网络的无监督表征学习”](https://arxiv.org/abs/1511.06434) -- 过去十年里最有影响力的论文之一 -- 被引用超过5,000次，这是Alec Radford在本科时写的。即使试图完成制造自动驾驶汽车这一极其艰巨的挑战的特斯拉，其首席执行官 [Elon Musk说](https://twitter.com/elonmusk/status/1224089444963311616):\n",
    "\n",
    "> : A PhD is definitely not required. All that matters is a deep understanding of AI & ability to implement NNs in a way that is actually useful (latter point is what’s truly hard). Don’t care if you even graduated high school.\n",
    "\n",
    ">: 博士学位绝非必要。重要的是对人工智能的深刻理解以及以一种实际上有用的方式实现神经网络的能力 (后一点才是真正困难的)。我们甚至不在乎你是否高中毕业。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you will need to do to succeed however is to apply what you learn in this book to a personal project, and always persevere.\n",
    "\n",
    "然而，要取得成功，你需要做的是将你在这本书里学到的东西应用到个人项目中，并一直坚持下去。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Projects and Your Mindset\n",
    "\n",
    "# 你的项目和你的心态"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether you're excited to identify if plants are diseased from pictures of their leaves, auto-generate knitting patterns, diagnose TB from X-rays, or determine when a raccoon is using your cat door, we will get you using deep learning on your own problems (via pre-trained models from others) as quickly as possible, and then will progressively drill into more details. You'll learn how to use deep learning to solve your own problems at state-of-the-art accuracy within the first 30 minutes of the next chapter! (And feel free to skip straight there now if you're dying to get coding right away.) There is a pernicious myth out there that you need to have computing resources and datasets the size of those at Google to be able to do deep learning, but it's not true.\n",
    "\n",
    "\n",
    "无论你是兴奋地从叶子的图片中识别植物是否有病，自动生成编织图案，从x射线中诊断结核病，还是确定浣熊何时使用你的猫门，我们将尽快让你对自己的问题进行深度学习 (通过他人预先训练的模型)，然后将逐步深入更多细节。在下一章的前30分钟内，你将学习如何使用深度学习以最先进的准确性解决自己的问题!(如果您迫切需要立即进行编码，请随时跳过此处。)要想进行深度学习，你需要拥有谷歌规模的计算资源和数据集，这是一个有害的“所谓的真相”，但这不是事实。\n",
    "\n",
    "So, what sorts of tasks make for good test cases? You could train your model to distinguish between Picasso and Monet paintings or to pick out pictures of your daughter instead of pictures of your son. It helps to focus on your hobbies and passions–-setting yourself four or five little projects rather than striving to solve a big, grand problem tends to work better when you're getting started. Since it is easy to get stuck, trying to be too ambitious too early can often backfire. Then, once you've got the basics mastered, aim to complete something you're really proud of!\n",
    "\n",
    "那么，什么样的任务是好的测试用例呢？你可以训练你的模型来区分毕加索和莫奈的画，或者挑选你女儿的照片而不是你儿子的照片。专注于你的爱好和激情是有帮助的 -- 当你开始的时候，给自己设定四五个小项目，而不是努力解决一个大问题，这样效果会更好。由于很容易卡住，过早尝试过于雄心勃勃往往会适得其反。然后，一旦掌握了基础知识，就可以完成自己真正引以为傲的事情!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> J: Deep learning can be set to work on almost any problem. For instance, my first startup was a company called FastMail, which provided enhanced email services when it launched in 1999 (and still does to this day). In 2002 I set it up to use a primitive form of deep learning, single-layer neural networks, to help categorize emails and stop customers from receiving spam.\n",
    "\n",
    "> J: 深度学习可以设置为处理几乎任何问题。例如，我的第一家创业公司是一家名为FastMail的公司，该公司在1999年推出时提供加强的电子邮件服务 (至今仍然如此)。在2002中，我将它设置为使用一种原始形式的深度学习 -- 单层神经网络，来帮助对电子邮件进行分类，并替客户拦截垃圾邮件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common character traits in the people that do well at deep learning include playfulness and curiosity. The late physicist Richard Feynman is an example of someone who we'd expect to be great at deep learning: his development of an understanding of the movement of subatomic particles came from his amusement at how plates wobble when they spin in the air.\n",
    "\n",
    "擅长深度学习的人有着共同性格特征，包括爱玩和好奇心。一个典型的例子就是已故物理学家理Richard Feynman，他是一个在深度学习领域表现出色的人: 他对亚原子粒子运动理解的开发就源自于他对板块在空中旋转时如何摆动的玩乐。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now focus on what you will learn, starting with the software.\n",
    "\n",
    "现在，从软件开始，我们重点关注你将学到的东西。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Software: PyTorch, fastai, and Jupyter\n",
    "\n",
    "## 软件: PyTorch、fastai、Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(And Why It Doesn't Matter)\n",
    "\n",
    "(以及为什么不重要)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've completed hundreds of machine learning projects using dozens of different packages, and many different programming languages. At fast.ai, we have written courses using most of the main deep learning and machine learning packages used today. After PyTorch came out in 2017 we spent over a thousand hours testing it before deciding that we would use it for future courses, software development, and research. Since that time PyTorch has become the world's fastest-growing deep learning library and is already used for most research papers at top conferences. This is generally a leading indicator of usage in industry, because these are the papers that end up getting used in products and services commercially. We have found that PyTorch is the most flexible and expressive library for deep learning. It does not trade off speed for simplicity, but provides both.\n",
    "\n",
    "\n",
    "我们已经使用数十个不同的包和许多不同的编程语言完成了数百个机器学习项目。在fast.ai，我们使用当今使用的大多数主要深度学习和机器学习包编写了课程。2017年PyTorch问世后，我们花了一千多个小时测试它，然后决定将它用于未来的课程、软件开发和研究。自那时起，PyTorch已经成为世界上发展最快的深度学习库，并且已经被用于顶级会议的大多数研究论文。这通常是产业使用的主要指标，因为这些论文最终被用于商业产品和服务。我们发现PyTorch是深度学习最灵活、最具表现力的库。它不会为了简单而牺牲速度，而是同时具备了两者。\n",
    "\n",
    "PyTorch works best as a low-level foundation library, providing the basic operations for higher-level functionality. The fastai library is the most popular library for adding this higher-level functionality on top of PyTorch. It's also particularly well suited to the purposes of this book, because it is unique in providing a deeply layered software architecture (there's even a [peer-reviewed academic paper](https://arxiv.org/abs/2002.04688) about this layered API). In this book, as we go deeper and deeper into the foundations of deep learning, we will also go deeper and deeper into the layers of fastai. This book covers version 2 of the fastai library, which is a from-scratch rewrite providing many unique features.\n",
    "\n",
    "PyTorch作为一个低级基础库工作得最好，为高级功能提供基本操作。Fastai库是在PyTorch上添加高级功能的最受欢迎的库。它也特别适合本书的目的，因为它在提供深度分层的软件架构方面是独一无二的 (甚至有一篇关于这个分层接口的 [同行评审学术论文](https://arxiv.org/abs/2002.04688)。在这本书里，随着我们越来越深入到深度学习的基础，我们也将越来越深入到fastai的层次。本书涵盖了fastai库的版本2，这是从头开始的重构，提供了许多独有的功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it doesn't really matter what software you learn, because it takes only a few days to learn to switch from one library to another. What really matters is learning the deep learning foundations and techniques properly. Our focus will be on using code that as clearly as possibly expresses the concepts that you need to learn. Where we are teaching high-level concepts, we will use high-level fastai code. Where we are teaching low-level concepts, we will use low-level PyTorch, or even pure Python code.\n",
    "\n",
    "\n",
    "然而，学习什么软件其实并不重要，因为只需要几天就能学会从一个库切换到另一个库。真正重要的是正确学习深度学习的基础和技巧。我们的重点是使用尽可能清楚地表达你需要学习的概念的代码。在我们教授高级概念的地方，我们将使用高级fastai代码。在我们教授低级概念的地方，我们将使用低级PyTorch，甚至是纯Python代码。\n",
    "\n",
    "If it feels like new deep learning libraries are appearing at a rapid pace nowadays, then you need to be prepared for a much faster rate of change in the coming months and years. As more people enter the field, they will bring more skills and ideas, and try more things. You should assume that whatever specific libraries and software you learn today will be obsolete in a year or two. Just think about the number of changes in libraries and technology stacks that occur all the time in the world of web programming—a much more mature and slow-growing area than deep learning. We strongly believe that the focus in learning needs to be on understanding the underlying techniques and how to apply them in practice, and how to quickly build expertise in new tools and techniques as they are released.\n",
    "\n",
    "如果现在感觉新的深度学习库正在快速出现，那么你需要为未来几个月和几年更快的变化做好准备。随着更多的人进入这个领域，他们会带来更多的技巧和想法，并尝试更多的东西。你应该假设你今天学到的任何特定的库和软件将在一两年内过时。只需想一下Web编程世界中无时无刻不在发生的库和技术堆栈变化的数量 -- 这还是比深度学习更为成熟和缓慢发展的领域。我们坚信，学习的重点需要放在理解底层技术和如何在实践中应用它们，以及如何在新工具和技术发布时快速构建专业知识上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the end of the book, you'll understand nearly all the code that's inside fastai (and much of PyTorch too), because in each chapter we'll be digging a level deeper to show you exactly what's going on as we build and train our models. This means that you'll have learned the most important best practices used in modern deep learning—not just how to use them, but how they really work and are implemented. If you want to use those approaches in another framework, you'll have the knowledge you need to do so if needed.\n",
    "\n",
    "\n",
    "在本书的结尾，您将了解fastai (以及PyTorch的大部分) 中的几乎所有代码，因为在每一章中，我们将深入挖掘一个层次，向你确切展示我们在构建和训练模型时发生的情况。这意味着你将学习现代深度学习中使用的最重要的最佳实践 -- 不仅仅是如何使用它们，而且还要了解它们的实际工作方式和实施方式。如果你想在另一个框架中使用这些方法，则将拥有必要的知识。\n",
    "\n",
    "Since the most important thing for learning deep learning is writing code and experimenting, it's important that you have a great platform for experimenting with code. The most popular programming experimentation platform is called Jupyter. This is what we will be using throughout this book. We will show you how you can use Jupyter to train and experiment with models and introspect every stage of the data pre-processing and model development pipeline. [Jupyter Notebook](https://jupyter.org/) is the most popular tool for doing data science in Python, for good reason. It is powerful, flexible, and easy to use. We think you will love it!\n",
    "\n",
    "因为学习深度学习最重要的事情是编写代码和进行实验，所以有一个很好的平台来实验代码是很重要的。最流行的编程实验平台叫做Jupyter。这就是我们在本书中将使用的内容。我们将向您展示如何使用Jupyter来训练和实验模型，并回顾数据预处理和模型开发流程的每个阶段。[Jupyter笔记本](https://jupyter.org/) 是用Python进行数据科学的最受欢迎的工具，这是有充分理由的。它功能强大，灵活，且易于使用。我们认为你会喜欢它的!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see it in practice and train our first model.\n",
    "\n",
    "让我们在实践中看看如何使用它，并训练我们的第一个模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your First Model\n",
    "\n",
    "## 你的第一个模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said before, we will teach you how to do things before we explain why they work. Following this top-down approach, we will begin by actually training an image classifier to recognize dogs and cats with almost 100% accuracy. To train this model and run our experiments, you will need to do some initial setup. Don't worry, it's not as hard as it looks.\n",
    "\n",
    "正如我们之前所说的，在我们解释事情为什么起作用之前，我们会教你如何去做这些事情。遵循这种自上而下的方法，我们将从实际训练图像分类器开始，以几乎100% 的准确率识别狗和猫。为了训练这个模型并运行我们的实验，你需要做一些初始设置。别担心，这并不像看起来那么难。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> s: Do not skip the setup part even if it looks intimidating at first, especially if you have little or no experience using things like a terminal or the command line. Most of that is actually not necessary and you will find that the easiest servers can be set up with just your usual web browser. It is crucial that you run your own experiments in parallel with this book in order to learn.\n",
    "\n",
    "> S: 不要跳过设置部分，即使一开始看起来很吓人，尤其是如果你只有很少或完全没有使用终端或命令行之类的东西的经验。其中大部分实际上是不必要的，你会发现最简单的服务器可以用你通常的网络浏览器来设置。为了达到学习的目的，与本书并行进行你自己的实验是至关重要的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a GPU Deep Learning Server\n",
    "\n",
    "# 获取显卡深度学习服务器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do nearly everything in this book, you'll need access to a computer with an NVIDIA GPU (unfortunately other brands of GPU are not fully supported by the main deep learning libraries). However, we don't recommend you buy one; in fact, even if you already have one, we don't suggest you use it just yet! Setting up a computer takes time and energy, and you want all your energy to focus on deep learning right now. Therefore, we instead suggest you rent access to a computer that already has everything you need preinstalled and ready to go. Costs can be as little as US$0.25 per hour while you're using it, and some options are even free.\n",
    "\n",
    "要完成本书中的全部内容，您需要访问带有英伟达显卡的计算机 (不幸的是，主要的深度学习库未完全支持其他品牌的显卡)。然而，我们不建议你买一个; 事实上，即使你已经有了，我们也不建议你现在就使用它!安装电脑需要时间和精力，你要把现在所有的精力都集中在深度学习上。因此，我们建议您租用一台已经预装并准备就绪的计算机。当你使用它时，成本可以低至每小时0.25美元，有些项目甚至是免费的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> jargon: Graphics Processing Unit (GPU): Also known as a _graphics card_. A special kind of processor in your computer that can handle thousands of single tasks at the same time, especially designed for displaying 3D environments on a computer for playing games. These same basic tasks are very similar to what neural networks do, such that GPUs can run neural networks hundreds of times faster than regular CPUs. All modern computers contain a GPU, but few contain the right kind of GPU necessary for deep learning.\n",
    "\n",
    "> 行话: 图形处理器 (GPU): 也称为“显卡”。是计算机中的一种特殊处理器，可以同时处理数千个单个任务，专为在计算机上玩游戏时显示3D环境而设计。这些相同的基本任务与神经网络的工作非常相似，因此显卡运行神经网络的速度可以比普通处理器快数百倍。所有现代计算机都包含显卡，但几乎没有包含深度学习所需的正确类型的显卡。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best choice of GPU servers to use with this book will change over time, as companies come and go and prices change. We maintain a list of our recommended options on the [book's website](https://book.fast.ai/), so go there now and follow the instructions to get connected to a GPU deep learning server. Don't worry, it only takes about two minutes to get set up on most platforms, and many don't even require any payment, or even a credit card, to get started.\n",
    "\n",
    "\n",
    "随着公司变动和价格的变化，本书中使用的显卡服务器的最佳选择将会随着时间的推移而改变。我们在 [图书网站](https://book.fast.ai/) 上保留了我们推荐的选项列表，因此，请立即前往该网址并按照说明连接到显卡深度学习服务器。别担心，在大多数平台上安装只需要大约两分钟，许多甚至不需要付费或者信用卡就可以开始。\n",
    "\n",
    "> A: My two cents: heed this advice! If you like computers you will be tempted to set up your own box. Beware! It is feasible but surprisingly involved and distracting. There is a good reason this book is not titled, _Everything You Ever Wanted to Know About Ubuntu System Administration, NVIDIA Driver Installation, apt-get, conda, pip, and Jupyter Notebook Configuration_. That would be a book of its own. Having designed and deployed our production machine learning infrastructure at work, I can testify it has its satisfactions, but it is as unrelated to modeling as maintaining an airplane is to flying one.\n",
    "\n",
    "\n",
    "> A: 个人浅见: 注意这个建议!如果你喜欢电脑，你会很想去设置自己的沙盒环境。当心!这是可行的，但它会出人意料地复杂又分散注意力。这本书没有标题的理由很充分，《所有你想知道的关于Ubuntu系统管理，英伟达驱动程序安装，apt-get，conda，pip，和Jupyter笔记本配置》。这本身就是一本书。在工作中设计并部署了我们的生产机器学习基础架构之后，我可以证明它具有令人满意的功能，但是它与建模无关，就像维护飞机就是去驾驶飞机一样。\n",
    "\n",
    "Each option shown on the website includes a tutorial; after completing the tutorial, you will end up with a screen looking like <<notebook_init>>.\n",
    "\n",
    "网站上显示的每个选项都包括一个教程; 完成教程后，你会得到一个看起来像 <<notebook_init>> 的屏幕。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Initial view of Jupyter Notebook\" width=\"658\" caption=\"Initial view of Jupyter Notebook\" id=\"notebook_init\" src=\"images/att_00057.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now ready to run your first Jupyter notebook!\n",
    "\n",
    "现在可以运行你的第一个Jupyter笔记本了!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> jargon: Jupyter Notebook: A piece of software that allows you to include formatted text, code, images, videos, and much more, all within a single interactive document. Jupyter received the highest honor for software, the ACM Software System Award, thanks to its wide use and enormous impact in many academic fields and in industry. Jupyter Notebook is the software most widely used by data scientists for developing and interacting with deep learning models.\n",
    "\n",
    "> 行话: Jupyter笔记本: 一款软件，可以使你在单个交互式文档中包含格式化文本、代码、图像、视频等。Jupyter获得了软件的最高荣誉，国际计算机协会软件系统奖，这要归功于它在许多学术领域和产业中的广泛应用和巨大影响。Jupyter笔记本是数据科学家最广泛用于开发深度学习模型并与之交互的软件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Your First Notebook\n",
    "\n",
    "# 运行你的第一个笔记本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebooks are labeled by chapter and then by notebook number, so that they are in the same order as they are presented in this book. So, the very first notebook you will see listed is the notebook that you need to use now. You will be using this notebook to train a model that can recognize dog and cat photos. To do this, you'll be downloading a _dataset_ of dog and cat photos, and using that to _train a model_. A dataset is simply a bunch of data—it could be images, emails, financial indicators, sounds, or anything else. There are many datasets made freely available that are suitable for training models. Many of these datasets are created by academics to help advance research, many are made available for competitions (there are competitions where data scientists can compete to see who has the most accurate model!), and some are by-products of other processes (such as financial filings).\n",
    "\n",
    "笔记本按章节标记，然后按笔记本编号标记，因此它们的顺序与本书中的顺序相同。所以，你会看到列出的第一个笔记本是你现在需要使用的笔记本。你将使用这个笔记本来训练一个能够识别狗和猫照片的模型。为此，你要下载狗和猫照片的 _数据集_ ，并使用它来 _训练模型_ 。数据集只是一堆数据 -- 它可以是图像、电子邮件、财务指标、声音或其他任何东西。有许多数据集免费提供，适合用来训练模型。其中许多数据集是由学者创建的，以帮助推进研究，许多数据集可用于竞赛 (有些竞赛中，数据科学家可以比赛，看看谁拥有最准确的模型!)，有些是其他流程 (如财务申报) 的副产品。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> note: Full and Stripped Notebooks: There are two folders containing different versions of the notebooks. The _full_ folder contains the exact notebooks used to create the book you're reading now, with all the prose and outputs. The _stripped_ version has the same headings and code cells, but all outputs and prose have been removed. After reading a section of the book, we recommend working through the stripped notebooks, with the book closed, and seeing if you can figure out what each cell will show before you execute it. Also try to recall what the code is demonstrating.\n",
    "\n",
    "> 注: 完整和精简笔记本: 有两个文件夹包含不同版本的笔记本。“完整”文件夹包含用于创建你现在正在阅读的书籍的精确笔记本，以及所有套话和输出。“精简”版本具有相同的标题和代码单元格，但所有输出和套话都已被删除。读完这本书的一部分后，我们建议在书合上的情况下，浏览一下精简笔记本，看看你能否在执行之前弄清楚每个单元格会显示什么。也试着回忆一下代码的演示内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To open a notebook, just click on it. The notebook will open, and it will look something like <<jupyter>> (note that there may be slight differences in details across different platforms; you can ignore those differences).\n",
    "\n",
    "点击即可打开笔记本。笔记本打开后看起来像 <<jupyter>> (请注意，不同平台的细节可能会有细微的差异; 你可以忽略这些差异)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"An example of notebook\" width=\"700\" caption=\"A Jupyter notebook\" src=\"images/0_jupyter.png\" id=\"jupyter\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A notebook consists of _cells_. There are two main types of cell:\n",
    "\n",
    "\n",
    "一个笔记本由 _单元格_ 组成。单元格有两种主要类型:\n",
    "\n",
    "- Cells containing formatted text, images, and so forth. These use a format called *markdown*, which you will learn about soon.\n",
    "- Cells containing code that can be executed, and outputs will appear immediately underneath (which could be plain text, tables, images, animations, sounds, or even interactive applications).\n",
    "\n",
    "\n",
    "- 包含格式化文本、图像等的单元格。这些使用一种名为 *markdown* 的格式，你很快就会了解到。\n",
    "- 包含可执行代码的单元格，输出将立即出现在下方 (可以是纯文本、表格、图像、动画、声音，甚至是交互式应用程序)。\n",
    "\n",
    "Jupyter notebooks can be in one of two modes: edit mode or command mode. In edit mode typing on your keyboard enters the letters into the cell in the usual way. However, in command mode, you will not see any flashing cursor, and the keys on your keyboard will each have a special function.\n",
    "\n",
    "\n",
    "Jupyter笔记本可以处于两种模式之一: 编辑模式或命令模式。在编辑模式下，在键盘上键入以常用的方式将字母输入单元格。但是，在命令模式下，你不会看到任何闪烁的光标，键盘上的按键将每个都有一个特殊的功能。\n",
    "\n",
    "Before continuing, press the Escape key on your keyboard to switch to command mode (if you are already in command mode, this does nothing, so press it now just in case). To see a complete list of all of the functions available, press H; press Escape to remove this help screen. Notice that in command mode, unlike most programs, commands do not require you to hold down Control, Alt, or similar—you simply press the required letter key.\n",
    "\n",
    "\n",
    "在继续之前，按键盘上的Escape键切换到命令模式 (如果您已经处于命令模式，将会不起作用，所以现在按下它以防万一)。要查看所有可用功能的完整列表，请按H; 按Escape清除此帮助屏幕。请注意，在命令模式下，与大多数程序不同，命令不需要你按住Control、Alt或类似的键 -- 你只需按下所需的字母键。\n",
    "\n",
    "You can make a copy of a cell by pressing C (the cell needs to be selected first, indicated with an outline around it; if it is not already selected, click on it once). Then press V to paste a copy of it.\n",
    "\n",
    "您可以通过按C键复制一个单元格 (首先需要选择该单元格，并在其周围注明轮廓; 如果还没有选择，请单击一次)。然后按V粘贴它的副本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on the cell that begins with the line \"# CLICK ME\" to select it. The first character in that line indicates that what follows is a comment in Python, so it is ignored when executing the cell. The rest of the cell is, believe it or not, a complete system for creating and training a state-of-the-art model for recognizing cats versus dogs. So, let's train it now! To do so, just press Shift-Enter on your keyboard, or press the Play button on the toolbar. Then wait a few minutes while the following things happen:\n",
    "\n",
    "\n",
    "单击以 “# 单击我” 行开头的单元格以选择它。该行中的第一个字符表示后面的是Python中的注释，因此在执行单元格时会被忽略。信不信由你，单元格的其余部分是一个完整的系统，用于创建和训练一个最先进的模型来识别猫和狗。所以，让我们现在训练它!为此，只需按键盘上的Shift-Enter键，或按工具栏上的 “播放” 按钮。然后等待几分钟，就会发生以下情况:\n",
    "\n",
    "1. A dataset called the [Oxford-IIIT Pet Dataset](http://www.robots.ox.ac.uk/~vgg/data/pets/) that contains 7,349 images of cats and dogs from 37 different breeds will be downloaded from the fast.ai datasets collection to the GPU server you are using, and will then be extracted.\n",
    "2. A *pretrained model* that has already been trained on 1.3 million images, using a competition-winning model will be downloaded from the internet.\n",
    "3. The pretrained model will be *fine-tuned* using the latest advances in transfer learning, to create a model that is specially customized for recognizing dogs and cats.\n",
    "\n",
    "\n",
    "1. 一个名为 [Oxford-IIIT 宠物数据集](http://www.robots.ox.ac.uk/~vgg/data/pets/) 的数据集，其中包含来自37个不同品种的猫和狗的7,349张图像将从fast.ai数据集集合下载到您正在使用的显卡服务器，然后将被提取出来。\n",
    "2. 一个将从互联网上下载的 *预训练模型*，该模型已经在130万张图像上训练，使用的是在比赛中获胜的模型。\n",
    "3. 预训练的模型将使用迁移学习的最新进展进行 *精调*，以创建专门为识别狗和猫而定制的模型。\n",
    "\n",
    "The first two steps only need to be run once on your GPU server. If you run the cell again, it will use the dataset and model that have already been downloaded, rather than downloading them again. Let's take a look at the contents of the cell, and the results (<<first_training>>):\n",
    "\n",
    "前两个步骤只需在您的显卡服务器上运行一次。如果你再次运行该单元格，它将使用已经下载的数据集和模型，而不是再次下载它们。让我们看看单元格的内容和结果 (<<first_training>>):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#id first_training\n",
    "#caption Results from the first training\n",
    "# CLICK ME\n",
    "from fastai.vision.all import *\n",
    "path = untar_data(URLs.PETS)/'images'\n",
    "\n",
    "def is_cat(x): return x[0].isupper()\n",
    "dls = ImageDataLoaders.from_name_func(\n",
    "    path, get_image_files(path), valid_pct=0.2, seed=42,\n",
    "    label_func=is_cat, item_tfms=Resize(224))\n",
    "\n",
    "learn = cnn_learner(dls, resnet34, metrics=error_rate)\n",
    "learn.fine_tune(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will probably not see exactly the same results that are in the book. There are a lot of sources of small random variation involved in training models. We generally see an error rate of well less than 0.02 in this example, however.\n",
    "\n",
    "你可能不会得到与书中完全相同的结果。训练模型中涉及大量小的随机变化。然而，在这个例子中，我们通常看到错误率远小于0.02。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> important: Trianing Time: Depending on your network speed, it might take a few minutes to download the pretrained model and dataset. Running `fine_tune` might take a minute or so. Often models in this book take a few minutes to train, as will your own models, so it's a good idea to come up with good techniques to make the most of this time. For instance, keep reading the next section while your model trains, or open up another notebook and use it for some coding experiments.\n",
    "\n",
    "> 重要提示: 训练时间: 根据你的网络速度，下载预训练的模型和数据集可能需要几分钟时间。运行 `fine_tune` 可能需要一分钟左右的时间。这本书里的模型通常需要几分钟来训练，你自己的模型也一样，所以想出好方法来充分利用这段时间是个好主意。例如，在模型训练时继续阅读下一节，或者打开另一个笔记本并将其用于一些编码实验。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sidebar: This Book Was Written in Jupyter Notebooks\n",
    "\n",
    "# 侧边栏: 这本书是用Jupyter笔记本写的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrote this book using Jupyter notebooks, so for nearly every chart, table, and calculation in this book, we'll be showing you the exact code required to replicate it yourself. That's why very often in this book, you will see some code immediately followed by a table, a picture or just some text. If you go on the [book's website](https://book.fast.ai) you will find all the code, and you can try running and modifying every example yourself.\n",
    "\n",
    "我们使用Jupyter笔记本编写了这本书，因此对于本书中的几乎每个图表、表格和计算，我们将向你展示自己复制它所需的确切代码。这就是为什么在这本书里，你会经常看到一些代码，后面跟着一张表格、一张图片或者只是一些文本。如果你去 [图书网站](https://book.fast.ai) 就可以找到所有代码，你可以尝试自己运行和修改每个示例。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just saw how a cell that outputs a table looks inside the book. Here is an example of a cell that outputs text:\n",
    "\n",
    "你刚刚了解到了输出表格的单元格在书中看起来是怎样的。下面是一个输出文本的单元格示例:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter will always print or show the result of the last line (if there is one). For instance, here is an example of a cell that outputs an image:\n",
    "\n",
    "Jupyter将始终打印或显示最后一行的结果 (如果有)。例如，这是一个输出图像的单元格示例:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = PILImage.create('images/chapter1_cat_example.jpg')\n",
    "img.to_thumb(192)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End sidebar\n",
    "\n",
    "# 结束侧边栏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how do we know if this model is any good? In the last column of the table you can see the error rate, which is the proportion of images that were incorrectly identified. The error rate serves as our metric—our measure of model quality, chosen to be intuitive and comprehensible. As you can see, the model is nearly perfect, even though the training time was only a few seconds (not including the one-time downloading of the dataset and the pretrained model). In fact, the accuracy you've achieved already is far better than anybody had ever achieved just 10 years ago!\n",
    "\n",
    "\n",
    "那么，我们如何知道这个模型好不好呢？在表的最后一列中，您可以看到错误率，即被错误识别的图像的比例。错误率作为我们的衡量标准 -- 我们选择直观易懂的标准来衡量模型的质量。如你所见，该模型近乎完美，尽管训练时间只有几秒钟 (不包括数据集和预训练模型的一次性下载)。事实上，你所达到的准确性远比10年前的任何人都高!\n",
    "\n",
    "Finally, let's check that this model actually works. Go and get a photo of a dog, or a cat; if you don't have one handy, just search Google Images and download an image that you find there. Now execute the cell with `uploader` defined. It will output a button you can click, so you can select the image you want to classify:\n",
    "\n",
    "最后，让我们检查一下这个模型是否真的有效。去拍一张狗或猫的照片; 如果你手边没有，只需搜索谷歌图片，下载你在那里面找到的图片。现在执行定义了`uploader`的单元格。它会输出一个你可以点击的按钮，所以你可以选择你想要分类的图像:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_output\n",
    "uploader = widgets.FileUpload()\n",
    "uploader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"An upload button\" width=\"159\" id=\"upload\" src=\"images/att_00008.png\">\n",
    "\n",
    "<Img alt = \"a upload button\" width = \"159\" id = \"upload\" src = \"images/att_00008.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can pass the uploaded file to the model. Make sure that it is a clear photo of a single dog or a cat, and not a line drawing, cartoon, or similar. The notebook will tell you whether it thinks it is a dog or a cat, and how confident it is. Hopefully, you'll find that your model did a great job:\n",
    "\n",
    "现在，您可以将上传的文件传递给模型。确保它是一只狗或一只猫的清晰照片，而不是线条画、卡通或类似的照片。笔记本会告诉你它认为这是狗还是猫，以及其准确性有多高。希望你会发现你的模型做得很好:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "# For the book, we can't actually click an upload button, so we fake it\n",
    "uploader = SimpleNamespace(data = ['images/chapter1_cat_example.jpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = PILImage.create(uploader.data[0])\n",
    "is_cat,_,probs = learn.predict(img)\n",
    "print(f\"Is this a cat?: {is_cat}.\")\n",
    "print(f\"Probability it's a cat: {probs[1].item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on your first classifier!\n",
    "\n",
    "\n",
    "祝贺你有了第一个分类器!\n",
    "\n",
    "But what does this mean? What did you actually do? In order to explain this, let's zoom out again to take in the big picture. \n",
    "\n",
    "但这意味着什么呢？你到底做了什么？为了解释这一点，让我们再次推远一点以看宏观的局面。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Is Machine Learning?\n",
    "\n",
    "# 什么是机器学习？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your classifier is a deep learning model. As was already mentioned, deep learning models use neural networks, which originally date from the 1950s and have become powerful very recently thanks to recent advancements.\n",
    "\n",
    "\n",
    "你的分类器是一个深度学习模型。正如已经提到的，深度学习模型使用神经网络，它最初可以追溯到20世纪50年代，得益于最近的进步，它已经变得非常强大。\n",
    "\n",
    "Another key piece of context is that deep learning is just a modern area in the more general discipline of *machine learning*. To understand the essence of what you did when you trained your own classification model, you don't need to understand deep learning. It is enough to see how your model and your training process are examples of the concepts that apply to machine learning in general.\n",
    "\n",
    "\n",
    "另一个关键背景是，深度学习只是 *机器学习* 中更普通的学科中的一个现代领域。要了解你在训练自己的分类模型时所做工作的本质，你无需了解深度学习。一般来说，看你的模型和训练过程是如何成为适用于机器学习的概念的示例就足够了。\n",
    "\n",
    "So in this section, we will describe what machine learning is. We will look at the key concepts, and show how they can be traced back to the original essay that introduced them.\n",
    "\n",
    "\n",
    "所以在这一节中，我们将解释什么是机器学习。我们将研究这些关键概念，并展示如何追溯到介绍它们的原始文章。\n",
    "\n",
    "*Machine learning* is, like regular programming, a way to get computers to complete a specific task. But how would we use regular programming to do what we just did in the last section: recognize dogs versus cats in photos? We would have to write down for the computer the exact steps necessary to complete the task.\n",
    "\n",
    "\n",
    "*机器学习*，与常规编程一样，是一种让计算机完成特定任务的方法。但是我们如何使用常规编程来做我们刚刚在上一节中做的事情: 识别照片中的狗和猫？我们必须为计算机写下完成任务所需的确切步骤。\n",
    "\n",
    "Normally, it's easy enough for us to write down the steps to complete a task when we're writing a program. We just think about the steps we'd take if we had to do the task by hand, and then we translate them into code. For instance, we can write a function that sorts a list. In general, we'd write a function that looks something like <<basic_program>> (where *inputs* might be an unsorted list, and *results* a sorted list).\n",
    "\n",
    "通常，当编写程序时，写下完成任务的步骤是很容易的。只用考虑如果必须手工完成任务，我们会采取的步骤，然后把它们翻译成代码。例如，我们可以编写一个对列表进行排序的函数。一般来说，我们会编写一个类似 <<basic_program>> 的函数 (其中 *输入* 可能是未排序的列表，*结果* 是排序的列表)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "#hide_input\n",
    "#caption A traditional program\n",
    "#id basic_program\n",
    "#alt Pipeline inputs, program, results\n",
    "gv('''program[shape=box3d width=1 height=0.7]\n",
    "inputs->program->results''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But for recognizing objects in a photo that's a bit tricky; what *are* the steps we take when we recognize an object in a picture? We really don't know, since it all happens in our brain without us being consciously aware of it!\n",
    "\n",
    "\n",
    "但是对于识别照片中的物体来说，这有点棘手; 当我们识别图片中的物体时，我们采取的步骤是什么？我们真的不知道，因为这一切都发生在我们的大脑中，我们没有意识到!\n",
    "\n",
    "Right back at the dawn of computing, in 1949, an IBM researcher named Arthur Samuel started working on a different way to get computers to complete tasks, which he called *machine learning*. In his classic 1962 essay \"Artificial Intelligence: A Frontier of Automation\", he wrote:\n",
    "\n",
    "早在1949年，计算机技术诞生之初，一位名叫Arthur Samuel的IBM研究员就开始研究让计算机完成任务的不同方法，他称之为 *机器学习*。在其经典的1962年的文章 《人工智能: 自动化的前沿》 中，他写道:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> : Programming a computer for such computations is, at best, a difficult task, not primarily because of any inherent complexity in the computer itself but, rather, because of the need to spell out every minute step of the process in the most exasperating detail. Computers, as any programmer will tell you, are giant morons, not giant brains.\n",
    "\n",
    ">: 为这样的计算编程计算机充其量是一项困难的任务，主要不是因为计算机本身的任何固有复杂性，而是，因为需要以最令人恼火的细节阐明流程的每一分钟。就像任何程序员都会告诉你的那样，计算机是大白痴，而不是大脑。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "His basic idea was this: instead of telling the computer the exact steps required to solve a problem, show it examples of the problem to solve, and let it figure out how to solve it itself. This turned out to be very effective: by 1961 his checkers-playing program had learned so much that it beat the Connecticut state champion! Here's how he described his idea (from the same essay as above):\n",
    "\n",
    "他的基本想法是这样的: 与其告诉计算机解决问题所需的具体步骤，不如向计算机展示要解决的问题的例子，让它自己解决问题。事实证明这是非常有效的: 到1961年，他的跳棋程序学到了很多东西，以至于击败了康涅狄格州冠军!以下是他如何描述自己的想法 (来自上述同一篇文章):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> : Suppose we arrange for some automatic means of testing the effectiveness of any current weight assignment in terms of actual performance and provide a mechanism for altering the weight assignment so as to maximize the performance. We need not go into the details of such a procedure to see that it could be made entirely automatic and to see that a machine so programmed would \"learn\" from its experience.\n",
    "\n",
    ">: 假设我们安排了一些自动方法来测试任一当前权重分配在实际性能方面的有效性，并提供了一种改变权重分配以最大化其性能的机制。我们不需要深入了解这种程序的细节，就可以看到它可以完全自动化，并且看到如此编程的机器可以从其经验中 “学习”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of powerful concepts embedded in this short statement: \n",
    "\n",
    "\n",
    "在这个简短的声明中嵌入了许多强大的概念:\n",
    "\n",
    "- The idea of a \"weight assignment\" \n",
    "- The fact that every weight assignment has some \"actual performance\"\n",
    "- The requirement that there be an \"automatic means\" of testing that performance,  \n",
    "- The need for a \"mechanism\" (i.e., another automatic process) for improving the performance by changing the weight assignments\n",
    "\n",
    "\n",
    "- “权重分配” 的想法\n",
    "- 每个权重分配都有一些 “实际性能” 的事实\n",
    "- 要求有一种 “自动手段” 来测试该性能，\n",
    "- 需要一个 “机制” (即另一个自动过程)，通过改变权重分配来提高性能\n",
    "\n",
    "Let us take these concepts one by one, in order to understand how they fit together in practice. First, we need to understand what Samuel means by a *weight assignment*.\n",
    "\n",
    "\n",
    "让我们一个接一个地理解这些概念，以便理解它们在实践中是如何结合在一起的。首先，我们需要理解Samuel的 *权重分配* 是什么意思。\n",
    "\n",
    "Weights are just variables, and a weight assignment is a particular choice of values for those variables. The program's inputs are values that it processes in order to produce its results—for instance, taking image pixels as inputs, and returning the classification \"dog\" as a result. The program's weight assignments are other values that define how the program will operate.\n",
    "\n",
    "\n",
    "权重只是变量，权重分配是这些变量的特定值选择。程序的输入是它为了产生结果而处理的值--例如，以图像像素作为输入，并返回分类 “狗” 作为结果。程序的权重分配是定义程序将如何运行的其他值。\n",
    "\n",
    "Since they will affect the program they are in a sense another kind of input, so we will update our basic picture in <<basic_program>> and replace it with <<weight_assignment>> in order to take this into account.\n",
    "\n",
    "因为会影响程序，所以它们在某种意义上是另一种输入，因此，我们将在 <<basic_program>> 中更新基本图片，并用 <<weight_assignment>> 替换它，就是为了考虑到这一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "#hide_input\n",
    "#caption A program using weight assignment\n",
    "#id weight_assignment\n",
    "gv('''model[shape=box3d width=1 height=0.7]\n",
    "inputs->model->results; weights->model''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've changed the name of our box from *program* to *model*. This is to follow modern terminology and to reflect that the *model* is a special kind of program: it's one that can do *many different things*, depending on the *weights*. It can be implemented in many different ways. For instance, in Samuel's checkers program, different values of the weights would result in different checkers-playing strategies. \n",
    "\n",
    "\n",
    "我们已经将这玩意儿\n",
    "的名称从 *程序* 更改为 *模型* 。这是为了遵循现代术语，并反映出 *模型* 是一种特殊的程序: 它可以做 *许多不同的事情*，这取决于 *权重*。它可以以许多不同的方式实现。例如，在Samuel的跳棋程序中，不同的权重值会导致不同的跳棋游戏策略。\n",
    "\n",
    "(By the way, what Samuel called \"weights\" are most generally refered to as model *parameters* these days, in case you have encountered that term. The term *weights* is reserved for a particular type of model parameter.)\n",
    "\n",
    "\n",
    "(顺便说一句，如果你遇到这个术语，Samuel所说的 “权重” 现在最普遍地被称为模型 *参数* 。术语 *权重* 是为特定类型的模型参数保留的。)\n",
    "\n",
    "Next, Samuel said we need an *automatic means of testing the effectiveness of any current weight assignment in terms of actual performance*. In the case of his checkers program, the \"actual performance\" of a model would be how well it plays. And you could automatically test the performance of two models by setting them to play against each other, and seeing which one usually wins.\n",
    "\n",
    "\n",
    "接下来，Samuel说我们需要一个 *用来测试任何当前权重分配在实际性能方面的有效性的自动手段*。就他的跳棋程序而言，模型的 “实际性能” 将是它的游戏玩得如何。你可以自动测试两个模型的性能，方法是让它们互相对抗，看看哪一个通常会赢。\n",
    "\n",
    "Finally, he says we need *a mechanism for altering the weight assignment so as to maximize the performance*. For instance, we could look at the difference in weights between the winning model and the losing model, and adjust the weights a little further in the winning direction.\n",
    "\n",
    "\n",
    "最后，他说我们需要 *一种改变权重分配以最大化其性能的机制*。例如，我们可以查看获胜模型和失败模型之间的权重差异，并往获胜方向进一步调整权重。\n",
    "\n",
    "We can now see why he said that such a procedure *could be made entirely automatic and... a machine so programmed would \"learn\" from its experience*. Learning would become entirely automatic when the adjustment of the weights was also automatic—when instead of us improving a model by adjusting its weights manually, we relied on an automated mechanism that produced adjustments based on performance.\n",
    "\n",
    "\n",
    "我们现在可以明白为什么他说这样的程序 *可以完全自动进行，并且如此编程的机器将从其经验中 “学习“*。当权重的调整也自动时，学习将变得完全自动 -- 不是通过手动调整其权重来改进模型，而是依靠一种基于性能产生调整的自动化机制。\n",
    "\n",
    "<<training_loop>> shows the full picture of Samuel's idea of training a machine learning model.\n",
    "\n",
    "<<Training_loop>> 展示了Samuel训练机器学习模型的想法的全貌。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "#hide_input\n",
    "#caption Training a machine learning model\n",
    "#id training_loop\n",
    "#alt The basic training loop\n",
    "gv('''ordering=in\n",
    "model[shape=box3d width=1 height=0.7]\n",
    "inputs->model->results; weights->model; results->performance\n",
    "performance->weights[constraint=false label=update]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the distinction between the model's *results*  (e.g., the moves in a checkers game) and its *performance* (e.g., whether it wins the game, or how quickly it wins). \n",
    "\n",
    "\n",
    "注意模型的 *结果* (例如，跳棋游戏中的动作) 及其 *性能* (例如，是否赢得了游戏，或者多久赢得游戏)。\n",
    "\n",
    "Also note that once the model is trained—that is, once we've chosen our final, best, favorite weight assignment—then we can think of the weights as being *part of the model*, since we're not varying them any more.\n",
    "\n",
    "\n",
    "还要注意，一旦模型被训练 -- 也就是说，一旦选择了最终的，最好的，最喜欢的权重分配 -- 然后我们可以认为权重是 *模型的一部分*，因为我们不再更改它们了。\n",
    "\n",
    "Therefore, actually *using* a model after it's trained looks like <<using_model>>.\n",
    "\n",
    "因此，实际上 *使用* 训练后的模型看起来像 <<using_model>>。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "#hide_input\n",
    "#caption Using a trained model as a program\n",
    "#id using_model\n",
    "gv('''model[shape=box3d width=1 height=0.7]\n",
    "inputs->model->results''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks identical to our original diagram in <<basic_program>>, just with the word *program* replaced with *model*. This is an important insight: *a trained model can be treated just like a regular computer program*.\n",
    "\n",
    "这看起来与我们在 <<basic_program>> 中的原始图表相同，只是将单词 *程序* 替换为 *模型*。这是一个重要的见解: *一个经过训练的模型可以像一个常规的计算机程序一样被对待*。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> jargon: Machine Learning: The training of programs developed by allowing a computer to learn from its experience, rather than through manually coding the individual steps.\n",
    "\n",
    "> 行话: 机器学习: 通过允许计算机从其经验中学习而不是通过手动编码单个步骤而开发的程序训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Is a Neural Network?\n",
    "\n",
    "# 什么是神经网络？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not too hard to imagine what the model might look like for a checkers program. There might be a range of checkers strategies encoded, and some kind of search mechanism, and then the weights could vary how strategies are selected, what parts of the board are focused on during a search, and so forth. But it's not at all obvious what the model might look like for an image recognition program, or for understanding text, or for many other interesting problems we might imagine.\n",
    "\n",
    "\n",
    "不难想象跳棋程序的模型会是什么样子。可能会有一系列的跳棋策略编码，以及某种搜索机制，然后权重可能会改变策略的选择方式，在搜索过程中哪些部分被重点关注，等等。但是对于图像识别程序，或者对于理解文本，或者对于我们可能想象的许多其他有趣的问题，模型可能会是什么样子并不明显。\n",
    "\n",
    "What we would like is some kind of function that is so flexible that it could be used to solve any given problem, just by varying its weights. Amazingly enough, this function actually exists! It's the neural network, which we already discussed. That is, if you regard a neural network as a mathematical function, it turns out to be a function which is extremely flexible depending on its weights. A mathematical proof called the *universal approximation theorem* shows that this function can solve any problem to any level of accuracy, in theory. The fact that neural networks are so flexible means that, in practice, they are often a suitable kind of model, and you can focus your effort on the process of training them—that is, of finding good weight assignments.\n",
    "\n",
    "\n",
    "我们想要的是某种非常灵活的函数，只需改变它的权重，就可以用来解决任何给定的问题。令人惊讶的是，这个功能确实存在!这就是我们已经讨论过的神经网络。也就是说，如果你把神经网络看作一个数学函数，它会是一个非常灵活的函数，这取决于它的权重。一个被称为 *万能逼近定理* 的数学证明表明，理论上，这个函数可以解决任何精度水平的任何问题。神经网络如此灵活的事实意味着，在实践中，它们通常是一种合适的模型，你可以把精力集中在训练它们的过程上 -- 也就是说，寻找好的权重分配。\n",
    "\n",
    "But what about that process?  One could imagine that you might need to find a new \"mechanism\" for automatically updating weight for every problem. This would be laborious. What we'd like here as well is a completely general way to update the weights of a neural network, to make it improve at any given task. Conveniently, this also exists!\n",
    "\n",
    "\n",
    "但是这个过程是怎样的呢？人们可以想象，你可能需要找到一个新的 “机制” 来自动更新每个问题的权重。这会很费力。我们在这里也想要一种完全通用的方法来更新神经网络的权重，以使其在任何给定的任务中都得到改善。很方便的是，这也已经存在了!\n",
    "\n",
    "This is called *stochastic gradient descent* (SGD). We'll see how neural networks and SGD work in detail in <<chapter_mnist_basics>>, as well as explaining the universal approximation theorem. For now, however, we will instead use Samuel's own words: *We need not go into the details of such a procedure to see that it could be made entirely automatic and to see that a machine so programmed would \"learn\" from its experience.*\n",
    "\n",
    "这被称为 *随机梯度下降* (SGD)。我们将在 <<chapter_mnist_basics>> 中详细了解神经网络和随机梯度下降是如何工作的，并解释万能逼近定理。然而，现在我们将使用Samuel自己的话: *我们不需要深入研究这一过程的细节，就可以看到它可以完全自动化，并且可以看到如此编程的机器可以从其经验中 “学习”。*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> J: Don't worry, neither SGD nor neural nets are mathematically complex. Both nearly entirely rely on addition and multiplication to do their work (but they do a _lot_ of addition and multiplication!). The main reaction we hear from students when they see the details is: \"Is that all it is?\"\n",
    "\n",
    "> J: 别担心，随机梯度下降和神经网络在数学上都不复杂。两者几乎完全依赖加法和乘法来完成他们的工作 (但是他们做了 _很多_ 加法和乘法!)。当学生看到细节时，我们听到的主要反应是: “这就是全部吗？\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, to recap, a neural network is a particular kind of machine learning model, which fits right in to Samuel's original conception. Neural networks are special because they are highly flexible, which means they can solve an unusually wide range of problems just by finding the right weights. This is powerful, because stochastic gradient descent provides us a way to find those weight values automatically.\n",
    "\n",
    "\n",
    "换句话说，概括一下，神经网络是一种特殊的机器学习模型，它正好符合Samuel的原始概念。神经网络很特别，因为它们高度灵活，这意味着它们可以通过找到合适的权重来解决异常广泛的问题。这很强大，因为随机梯度下降为我们提供了一种自动找到这些权重值的方法。\n",
    "\n",
    "Having zoomed out, let's now zoom back in and revisit our image classification problem using Samuel's framework.\n",
    "\n",
    "\n",
    "推远完成后，现在让我们推近回来并使用Samuel的框架重新审视我们的图像分类问题。\n",
    "\n",
    "Our inputs are the images. Our weights are the weights in the neural net. Our model is a neural net. Our results are the values that are calculated by the neural net, like \"dog\" or \"cat.\"\n",
    "\n",
    "\n",
    "我们的输入是图像。我们的权重是神经网络中的权重。我们的模型是一个神经网络。我们的结果是由神经网络计算的值，如 “狗” 或者 “猫”。\n",
    "\n",
    "What about the next piece, an *automatic means of testing the effectiveness of any current weight assignment in terms of actual performance*? Determining \"actual performance\" is easy enough: we can simply define our model's performance as its accuracy at predicting the correct answers.\n",
    "\n",
    "\n",
    "下一部分是什么，一种 *用来测试任何当前权重分配在实际性能方面的有效性的自动手段*？确定 “实际性能” 很容易: 我们可以简单地将模型的性能定义为预测正确答案的准确性。\n",
    "\n",
    "Putting this all together, and assuming that SGD is our mechanism for updating the weight assignments, we can see how our image classifier is a machine learning model, much like Samuel envisioned.\n",
    "\n",
    "把这些放在一起，假设随机梯度下降是我们更新权重分配的机制，我们可以看到我们的图像分类器是如何成为机器学习模型的，正像Samuel所设想的那样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Bit of Deep Learning Jargon\n",
    "\n",
    "# 一点深度学习行话"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samuel was working in the 1960s, and since then terminology has changed. Here is the modern deep learning terminology for all the pieces we have discussed:\n",
    "\n",
    "\n",
    "Samuel在1960年代工作，从那以后术语发生了变化。以下是我们讨论过的所有现代深度学习术语:\n",
    "\n",
    "- The functional form of the *model* is called its *architecture* (but be careful—sometimes people use *model* as a synonym of *architecture*, so this can get confusing).\n",
    "- The *weights* are called *parameters*.\n",
    "- The *predictions* are calculated from the *independent variable*, which is the *data* not including the *labels*.\n",
    "- The *results* of the model are called *predictions*.\n",
    "- The measure of *performance* is called the *loss*.\n",
    "- The loss depends not only on the predictions, but also the correct *labels* (also known as *targets* or the *dependent variable*); e.g., \"dog\" or \"cat.\"\n",
    "\n",
    "\n",
    "- *模型* 的功能形式被称为它的 *架构* (但是要小心--有时候人们用 *模型* 作为 *架构* 的同义词，所以这可能会让人困惑)。\n",
    "- *权重* 被称为 *参数*。\n",
    "- *预测* 由 *自变量* 计算，这是 *数据*，不包括 *标签*。\n",
    "- 模型的 *结果* 被称为 *预测*。\n",
    "- *性能* 的衡量被称为 *损失*。\n",
    "- 损失不仅取决于预测，还取决于正确的 *标签* (也称为*目标*或*因变量*); 例如，“狗” 或 “猫”。\n",
    "\n",
    "After making these changes, our diagram in <<training_loop>> looks like <<detailed_loop>>.\n",
    "\n",
    "进行这些更改后，我们在 <<training_loop>> 中的图表看起来像 <<detailed_loop>>。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "#hide_input\n",
    "#caption Detailed training loop\n",
    "#id detailed_loop\n",
    "gv('''ordering=in\n",
    "model[shape=box3d width=1 height=0.7 label=architecture]\n",
    "inputs->model->predictions; parameters->model; labels->loss; predictions->loss\n",
    "loss->parameters[constraint=false label=update]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations Inherent To Machine Learning\n",
    "\n",
    "\n",
    "# 机器学习固有的限制\n",
    "\n",
    "From this picture we can now see some fundamental things about training a deep learning model:\n",
    "\n",
    "\n",
    "从这张图片中，我们现在可以看到一些关于训练深度学习模型的基本内容:\n",
    "\n",
    "- A model cannot be created without data.\n",
    "- A model can only learn to operate on the patterns seen in the input data used to train it.\n",
    "- This learning approach only creates *predictions*, not recommended *actions*.\n",
    "- It's not enough to just have examples of input data; we need *labels* for that data too (e.g., pictures of dogs and cats aren't enough to train a model; we need a label for each one, saying which ones are dogs, and which are cats).\n",
    "\n",
    "\n",
    "- 没有数据无法创建模型。\n",
    "- 模型只能学习对用于训练它的输入数据中看到的模式进行操作。\n",
    "- 这种学习方法只创建 *预测*，而无建议 *行动*。\n",
    "- 仅仅有输入数据的例子是不够的; 我们也需要 *标签* 用于这些数据 (例如，狗和猫的照片不足以训练一个模型; 我们需要为每张图片加标签，说明哪些是狗，哪些是猫)。\n",
    "\n",
    "Generally speaking, we've seen that most organizations that say they don't have enough data, actually mean they don't have enough *labeled* data. If any organization is interested in doing something in practice with a model, then presumably they have some inputs they plan to run their model against. And presumably they've been doing that some other way for a while (e.g., manually, or with some heuristic program), so they have data from those processes! For instance, a radiology practice will almost certainly have an archive of medical scans (since they need to be able to check how their patients are progressing over time), but those scans may not have structured labels containing a list of diagnoses or interventions (since radiologists generally create free-text natural language reports, not structured data). We'll be discussing labeling approaches a lot in this book, because it's such an important issue in practice.\n",
    "\n",
    "\n",
    "一般来说，我们看到大多数组织说他们没有足够的数据，实际上意味着他们没有足够的 *标签* 数据。如果任何组织有兴趣在实践中用模型做一些事情，那么大概他们有一些计划要对模型进行操作的输入。大概他们已经用其他方式做了一段时间了 (例如，手动，或者用一些启发式程序)，所以他们有来自这些过程的数据!例如，放射学实践几乎肯定会有医疗扫描档案 (因为他们需要能够检查他们的病人随着时间的推移是如何发展的)，但是这些扫描可能没有包含诊断或干预列表的结构化标签 (因为放射科医生通常创建自由文本自然语言报告，而不是结构化数据)。我们将在这本书里讨论很多标签方法，因为这在实践中是一个非常重要的问题。\n",
    "\n",
    "Since these kinds of machine learning models can only make *predictions* (i.e., attempt to replicate labels), this can result in a significant gap between organizational goals and model capabilities. For instance, in this book you'll learn how to create a *recommendation system* that can predict what products a user might purchase. This is often used in e-commerce, such as to customize products shown on a home page by showing the highest-ranked items. But such a model is generally created by looking at a user and their buying history (*inputs*) and what they went on to buy or look at (*labels*), which means that the model is likely to tell you about products the user already has or already knows about, rather than new products that they are most likely to be interested in hearing about. That's very different to what, say, an expert at your local bookseller might do, where they ask questions to figure out your taste, and then tell you about authors or series that you've never heard of before.\n",
    "\n",
    "由于这些机器学习模型只能进行 *预测* (即尝试复制标签)，这可能会导致组织目标和模型能力之间的巨大差距。例如，在本书中，你将学习如何创建一个 *推荐系统* 来预测用户可能购买的产品。这通常用于电子商务，例如通过显示排名最高的项目来定制主页上显示的产品。但是这种模型通常是通过查看用户及其购买历史 (*输入*) 以及他们继续购买或查看的内容 (*标签*) 来创建的，这意味着模型可能会告诉你用户已经拥有或已经知道的产品，而不是他们最有可能听到而感兴趣的新产品。这与你当地书商的专家可能会做的非常不同，他们会通过问问题来了解你的品味，然后告诉你你以前从未听说过的作者或系列。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another critical insight comes from considering how a model interacts with its environment. This can create *feedback loops*, as described here:\n",
    "\n",
    "\n",
    "另一个重要的见解来自考虑模型如何与其环境交互。这可以创建 *反馈循环*，如下所述:\n",
    "\n",
    "- A *predictive policing* model is created based on where arrests have been made in the past. In practice, this is not actually predicting crime, but rather predicting arrests, and is therefore partially simply reflecting biases in existing policing processes.\n",
    "- Law enforcement officers then might use that model to decide where to focus their police activity, resulting in increased arrests in those areas.\n",
    "- Data on these additional arrests would then be fed back in to retrain future versions of the model.\n",
    "\n",
    "\n",
    "- 基于过去逮捕的地点创建了一个 *预测警务* 模型。在实践中，这实际上不是预测犯罪，而是预测逮捕，因此部分反映了现有警务过程中的偏见。\n",
    "- 然后，执法人员可以使用该模型来决定将其警察活动重点放在哪里，从而导致这些地区的逮捕人数增加。\n",
    "- 这些额外逮捕的数据将被反馈回来，以重新训练模型的未来版本。\n",
    "\n",
    "This is a *positive feedback loop*, where the more the model is used, the more biased the data becomes, making the model even more biased, and so forth.\n",
    "\n",
    "\n",
    "这是一个 *正反馈循环*，模型使用得越多，数据的偏差就越大，使模型的偏差就越大，依此类推。\n",
    "\n",
    "Feedback loops can also create problems in commercial settings. For instance, a video recommendation system might be biased toward recommending content consumed by the biggest watchers of video (e.g., conspiracy theorists and extremists tend to watch more online video content than the average), resulting in those users increasing their video consumption, resulting in more of those kinds of videos being recommended. We'll consider this topic more in detail in <<chapter_ethics>>.\n",
    "\n",
    "反馈循环也会在商业环境中产生问题。例如，视频推荐系统可能偏向于推荐最大的视频观看者所消费的内容 (例如，阴谋论者和极端分子倾向于观看比普通人更多的在线视频内容)，导致这些用户的视频消费量增加，因此推荐了更多此类视频。我们将在 <<chapter_ethics>> 中更详细地研究这个主题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have seen the base of the theory, let's go back to our code example and see in detail how the code corresponds to the process we just described.\n",
    "\n",
    "既然你已经看到了理论的基础，让我们回到代码示例，详细看看代码是如何对应我们刚才描述的过程的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Our Image Recognizer Works\n",
    "\n",
    "# 我们的图像识别器如何工作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see just how our image recognizer code maps to these ideas. We'll put each line into a separate cell, and look at what each one is doing (we won't explain every detail of every parameter yet, but will give a description of the important bits; full details will come later in the book).\n",
    "\n",
    "让我们看看我们的图像识别器代码是如何映射到这些想法的。我们将把每一行放入一个单独的单元格中，看看每一行都在做什么 (我们还不会解释每个参数的每个细节，但会给出重要部分的描述; 完整的细节将在本书后面的内容中介绍)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line imports all of the fastai.vision library.\n",
    "\n",
    "\n",
    "第一行导入所有fastai.vision库。\n",
    "\n",
    "```python\n",
    "from fastai.vision.all import *\n",
    "```\n",
    "\n",
    "This gives us all of the functions and classes we will need to create a wide variety of computer vision models.\n",
    "\n",
    "这为我们提供了创建各种计算机视觉模型所需的所有功能和分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> J: A lot of Python coders recommend avoiding importing a whole library like this (using the `import *` syntax), because in large software projects it can cause problems. However, for interactive work such as in a Jupyter notebook, it works great. The fastai library is specially designed to support this kind of interactive use, and it will only import the necessary pieces into your environment.\n",
    "\n",
    "> J: 许多Python程序员建议避免像这样导入整个库 (使用 `import *` 语法)，因为在大型软件项目中会引起问题。但是，对于像Jupyter笔记本这样的交互式工作，它效果很好。Fastai库是专门为支持这种交互式使用而设计的，它只会将必要的部分导入到你的环境中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second line downloads a standard dataset from the [fast.ai datasets collection](https://course.fast.ai/datasets) (if not previously downloaded) to your server, extracts it (if not previously extracted), and returns a `Path` object with the extracted location:\n",
    "\n",
    "\n",
    "第二行从 [fast.ai数据集集合]( https://course.fast.ai/datasets ) 下载标准数据集 (如果之前没有下载) 到你的服务器，提取它 (如果之前没有提取)，并返回一个具有提取位置的 `path` 对象:\n",
    "\n",
    "```python\n",
    "path = untar_data(URLs.PETS)/'images'\n",
    "```\n",
    "\n",
    "> S: Throughout my time studying at fast.ai, and even still today, I've learned a lot about productive coding practices. The fastai library and fast.ai notebooks are full of great little tips that have helped make me a better programmer. For instance, notice that the fastai library doesn't just return a string containing the path to the dataset, but a `Path` object. This is a really useful class from the Python 3 standard library that makes accessing files and directories much easier. If you haven't come across it before, be sure to check out its documentation or a tutorial and try it out. Note that the https://book.fast.ai[website] contains links to recommended tutorials for each chapter. I'll keep letting you know about little coding tips I've found useful as we come across them.\n",
    "\n",
    "> S: 在我在fast.ai学习的这段时间里，直到今天，我已经学到了很多关于生产编码实践的知识。fastai库和fast.ai笔记本中充满了许多很棒的小技巧，这些技巧帮助我成为了一个更好的程序员。例如，请注意，fastai库不仅返回包含数据集路径的字符串，还返回`Path`对象。这是Python 3标准库中的一个非常有用的分类，它使访问文件和目录变得更加容易。如果你以前从未接触过它，请务必查看它的文档或教程进行尝试。请注意， https://book.fast.ai [网站] 包含每个章节的推荐教程的链接。在遇到的时候，我会不断告诉你一些我发现有用的编程小技巧。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the third line we define a function, `is_cat`, labels cats based on a filename rule provided by the dataset creators:\n",
    "\n",
    "在第三行中，我们定义了一个函数， `is_cat`，根据数据集创建者提供的文件名规则添加猫的标签:\n",
    "\n",
    "```python\n",
    "def is_cat(x): return x[0].isupper()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use that function in the fourth line, which tells fastai what kind of dataset we have, and how it is structured:\n",
    "\n",
    "\n",
    "我们在第四行中使用该函数，它告诉fastai我们有什么样的数据集，以及它的结构方式:\n",
    "\n",
    "```python\n",
    "dls = ImageDataLoaders.from_name_func(\n",
    "    path, get_image_files(path), valid_pct=0.2, seed=42,\n",
    "    label_func=is_cat, item_tfms=Resize(224))\n",
    "```\n",
    "\n",
    "\n",
    "There are various different classes for different kinds of deep learning datasets and problems—here we're using `ImageDataLoaders`. The first part of the class name will generally be the type of data you have, such as image, or text.\n",
    "\n",
    "\n",
    "对于不同类型的深度学习数据集和问题，有各种不同的类--这里我们使用的是 `imagedataloaders`。类名的第一部分通常是你拥有的数据类型，如图像或文本。\n",
    "\n",
    "The other important piece of information that we have to tell fastai is how to get the labels from the dataset. Computer vision datasets are normally structured in such a way that the label for an image is part of the filename, or path—most commonly the parent folder name. fastai comes with a number of standardized labeling methods, and ways to write your own. Here we're telling fastai to use the `is_cat` function we just defined.\n",
    "\n",
    "\n",
    "我们必须告诉fastai的另一个重要信息是如何从数据集获取标签。计算机视觉数据集的结构通常是这样的: 图像的标签是文件名或路径的一部分--最常见的是父文件夹名称。Fastai提供了许多标准化的标签方法，以及你自己编写的方法。这里我们告诉fastai使用我们刚刚定义的 `is_cat` 函数。\n",
    "\n",
    "Finally, we define the `Transform`s that we need. A `Transform` contains code that is applied automatically during training; fastai includes many predefined `Transform`s, and adding new ones is as simple as creating a Python function. There are two kinds: `item_tfms` are applied to each item (in this case, each item is resized to a 224-pixel square), while `batch_tfms` are applied to a *batch* of items at a time using the GPU, so they're particularly fast (we'll see many examples of these throughout this book).\n",
    "\n",
    "\n",
    "最后，我们定义我们需要的 `Transform`。`Transform` 包含在训练期间自动应用的代码; fastai包含许多预定义的 `Transform`，添加新的 `Transform` 就像创建Python函数一样简单。有两种类型: `item_tfms` 应用于每个项目 (在这种情况下，每个项目被调整为224像素的正方形)，而`batch_tfms`在使用显卡时应用于 *批* 的项目，所以它们特别快 (我们将在本书中看到许多这样的例子)。\n",
    "\n",
    "Why 224 pixels? This is the standard size for historical reasons (old pretrained models require this size exactly), but you can pass pretty much anything. If you increase the size, you'll often get a model with better results (since it will be able to focus on more details), but at the price of speed and memory consumption; the opposite is true if you decrease the size. \n",
    "\n",
    "为什么是224像素？出于历史原因，这是标准尺寸 (旧的预训练模型恰好需要这个尺寸)，但是你几乎可以传递几乎所有内容。如果你增加尺寸，你通常会得到一个效果更好的模型 (因为它能够专注于更多细节)，但是要以速度和内存消耗为代价; 如果减小尺寸，则情况正好相反。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Classification and Regression: _classification_ and _regression_ have very specific meanings in machine learning. These are the two main types of model that we will be investigating in this book. A classification model is one which attempts to predict a class, or category. That is, it's predicting from a number of discrete possibilities, such as \"dog\" or \"cat.\" A regression model is one which attempts to predict one or more numeric quantities, such as a temperature or a location. Sometimes people use the word _regression_ to refer to a particular kind of model called a _linear regression model_; this is a bad practice, and we won't be using that terminology in this book!\n",
    "\n",
    "> 注意: 分类和回归: _分类_ 和 _回归_ 在机器学习中有非常具体的含义。这是我们将在本书中研究的两种主要类型的模型。分类模型是试图预测一个类或类别的模型。也就是说，它是从许多离散的可能性中预测的，例如 “狗” 或 “猫”。回归模型是试图预测一个或多个数字量的模型，例如温度或位置。有时人们用 _回归_ 这个词来指代一种被称为 _线性回归模型_ 的特定模型; 这是一种不好的做法，我们不会在这本书里使用这个术语!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pet dataset contains 7,390 pictures of dogs and cats, consisting of 37 different breeds. Each image is labeled using its filename: for instance the file *great\\_pyrenees\\_173.jpg* is the 173rd example of an image of a Great Pyrenees breed dog in the dataset. The filenames start with an uppercase letter if the image is a cat, and a lowercase letter otherwise. We have to tell fastai how to get labels from the filenames, which we do by calling `from_name_func` (which means that filenames can be extracted using a function applied to the filename), and passing `x[0].isupper()`, which evaluates to `True` if the first letter is uppercase (i.e., it's a cat).\n",
    "\n",
    "\n",
    "宠物数据集包含7,390张狗和猫的图片，由37个不同的品种组成。每个图像都使用其文件名进行标记: 例如文件 *great\\_pyrenees\\_173.jpg* 是数据集中一只大比利牛斯犬的图像的第173示例。如果图像是cat，则文件名以大写字母开头，否则为小写字母。我们必须告诉fastai如何从文件名中获取标签，我们通过调用 `from_name_func` (这意味着可以使用应用于文件名的函数提取文件名)，并传递 `x[0].isupper()`。如果第一个字母是大写字母，则计算结果为`True`(即，它是一只猫)。\n",
    "\n",
    "The most important parameter to mention here is `valid_pct=0.2`. This tells fastai to hold out 20% of the data and *not use it for training the model at all*. This 20% of the data is called the *validation set*; the remaining 80% is called the *training set*. The validation set is used to measure the accuracy of the model. By default, the 20% that is held out is selected randomly. The parameter `seed=42` sets the *random seed* to the same value every time we run this code, which means we get the same validation set every time we run it—this way, if we change our model and retrain it, we know that any differences are due to the changes to the model, not due to having a different random validation set.\n",
    "\n",
    "\n",
    "这里要提到的最重要的参数是 `valid_pct = 0.2`。这告诉fastai保留20% 的数据，并且 *完全不用它来训练模型* 。这20% 的数据称为 *验证集* ; 剩下的80% 称为 *训练集* 。验证集用于测量模型的准确性。默认情况下，将随机选择保持的20%。参数 `seed = 42` 在我们每次运行此代码时将 *随机种子* 设置为相同的值，这意味着我们每次运行它时都会得到相同的验证集 -- 这样，如果改变模型并重新训练它，我们知道，任何差异都归因于模型的更改，而不是归因于具有不同的随机验证集。\n",
    "\n",
    "fastai will *always* show you your model's accuracy using *only* the validation set, *never* the training set. This is absolutely critical, because if you train a large enough model for a long enough time, it will eventually memorize the label of every item in your dataset! The result will not actually be a useful model, because what we care about is how well our model works on *previously unseen images*. That is always our goal when creating a model: for it to be useful on data that the model only sees in the future, after it has been trained.\n",
    "\n",
    "\n",
    "fastai会*仅*使用验证集，*从不*使用训练集，来向你*始终*显示模型的准确性。这绝对是至关重要的，因为如果你训练足够大的模型足够长的时间，它将最终记住数据集中每个项目的标签！结果实际上这将不是一个有用的模型，因为我们关心的是我们的模型在*以前没见过的图像*上的工作情况。创建模型时，这一直是我们的目标: 在模型经过训练后，它对模型只能在未来见到的数据有用。\n",
    "\n",
    "Even when your model has not fully memorized all your data, earlier on in training it may have memorized certain parts of it. As a result, the longer you train for, the better your accuracy will get on the training set; the validation set accuracy will also improve for a while, but eventually it will start getting worse as the model starts to memorize the training set, rather than finding generalizable underlying patterns in the data. When this happens, we say that the model is *overfitting*.\n",
    "\n",
    "\n",
    "即使你的模型没有完全记住你所有的数据，在训练的早期，它可能已经记住了其中的某些部分。因此，你训练的时间越长，你在训练集中的准确性就越好; 验证集的准确性也会在一段时间内有所改善，但最终会因为模型开始存储训练集而不是在数据中找到可概括的基本模式而变得越来越差。当这种情况发生时，我们说模型是 *过拟合*。\n",
    "\n",
    "<<img_overfit>> shows what happens when you overfit, using a simplified example where we have just one parameter, and some randomly generated data based on the function `x**2`. As you can see, although the predictions in the overfit model are accurate for data near the observed data points, they are way off when outside of that range.\n",
    "\n",
    "<<Img_overfit>> 使用一个简化的示例显示了过拟合时会发生什么，其中我们只有一个参数，以及一些基于函数 `x**2` 随机生成的数据。正如你看到的，尽管过拟合模型中的预测对于观察到的数据点附近的数据是准确的，但当超出该范围时，它们的预测值将大相径庭。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/att_00000.png\" alt=\"Example of overfitting\" caption=\"Example of overfitting\" id=\"img_overfit\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overfitting is the single most important and challenging issue** when training for all machine learning practitioners, and all algorithms. As you will see, it is very easy to create a model that does a great job at making predictions on the exact data it has been trained on, but it is much harder to make accurate predictions on data the model has never seen before. And of course, this is the data that will actually matter in practice. For instance, if you create a handwritten digit classifier (as we will very soon!) and use it to recognize numbers written on checks, then you are never going to see any of the numbers that the model was trained on—check will have slightly different variations of writing to deal with. You will learn many methods to avoid overfitting in this book. However, you should only use those methods after you have confirmed that overfitting is actually occurring (i.e., you have actually observed the validation accuracy getting worse during training). We often see practitioners using over-fitting avoidance techniques even when they have enough data that they didn't need to do so, ending up with a model that may be less accurate than what they could have achieved.\n",
    "\n",
    "**过拟合是所有机器学习从业者和所有算法培训时最重要和最具挑战性的问题**。正如你将看到的，创建一个模型很容易，在对它所训练的精确数据进行预测方面做得很好，但是要对模型从未见过的数据进行准确预测要困难得多。当然，这是在实践中实际上很重要的数据。例如，如果你创建一个手写数字分类器 (我们很快就会这么做!) 用它来识别写在支票上的数字，那么你永远也不会看到经过模型训练的任何数字 -- 支票的书写方式略有不同。你将在本书中学习许多避免过拟合的方法。但是，你应该仅在确认过拟合实际发生后才使用这些方法 (即，你在训练期间实际观察到验证精度越来越差)。我们经常看到从业者使用过拟合回避技术，即使他们有足够的数据不需要这样做，最终得到的模型可能不如他们所能达到的精确。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> important: Validation Set: When you train a model, you must _always_ have both a training set and a validation set, and must measure the accuracy of your model only on the validation set. If you train for too long, with not enough data, you will see the accuracy of your model start to get worse; this is called _overfitting_. fastai defaults `valid_pct` to `0.2`, so even if you forget, fastai will create a validation set for you!\n",
    "\n",
    "> 重要提示: 验证集: 当你训练一个模型时，你必须 _始终_ 同时拥有训练集和验证集，并且必须仅在验证集上测试模型的准确性。如果你训练太久，没有足够的数据，你会看到你的模型的准确性开始变得更差; 这叫做“过拟合”。Fastai将 `valid_pct` 默认为 `0.2`，所以即使你忘记了，fastai也会为你创建验证集!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fifth line of the code training our image recognizer tells fastai to create a *convolutional neural network* (CNN) and specifies what *architecture* to use (i.e. what kind of model to create), what data we want to train it on, and what *metric* to use:\n",
    "\n",
    "\n",
    "代码的第五行训练我们的图像识别器告诉fastai创建一个 *卷积神经网络* (CNN)，并指定使用什么 *架构* (即创建什么样的模型)，我们要在什么数据上训练它，以及要使用什么 *评估指标*:\n",
    "\n",
    "```python\n",
    "learn = cnn_learner(dls, resnet34, metrics=error_rate)\n",
    "```\n",
    "\n",
    "Why a CNN? It's the current state-of-the-art approach to creating computer vision models. We'll be learning all about how CNNs work in this book. Their structure is inspired by how the human vision system works.\n",
    "\n",
    "\n",
    "为什么是卷积神经网络？这是当前创建计算机视觉模型的最先进的方法。我们将在这本书里了解卷积神经网络是如何工作的。它们的结构受到人类视觉系统工作原理的启发。\n",
    "\n",
    "There are many different architectures in fastai, which we will introduce in this book (as well as discussing how to create your own). Most of the time, however, picking an architecture isn't a very important part of the deep learning process. It's something that academics love to talk about, but in practice it is unlikely to be something you need to spend much time on. There are some standard architectures that work most of the time, and in this case we're using one called _ResNet_ that we'll be talking a lot about during the book; it is both fast and accurate for many datasets and problems. The `34` in `resnet34` refers to the number of layers in this variant of the architecture (other options are `18`, `50`, `101`, and `152`). Models using architectures with more layers take longer to train, and are more prone to overfitting (i.e. you can't train them for as many epochs before the accuracy on the validation set starts getting worse). On the other hand, when using more data, they can be quite a bit more accurate.\n",
    "\n",
    "\n",
    "Fastai中有许多不同的架构，我们将在本书中介绍 (以及讨论如何创建你自己的架构)。然而，大多数时候，选择架构并不是深度学习过程中很重要的部分。这是学者们喜欢谈论的事情，但实际上，你不太可能需要花很多时间去做的这件事。有一些标准体系结构大部分时间都在工作，在这种情况下，我们使用了一个叫做 _ResNet_ 的体系结构，我们将在书中讨论很多; 对于许多数据集和问题，它既快速又准确。“Resnet34” 中的 “34” 是指该体系结构变体中的层数 (其他选项为 “18” 、 “50” 、 “101” 、和 '152')。使用具有更多层的架构的模型需要更长的时间来训练，并且更容易过拟合 (即在验证集的准确性开始变得更差之前，你不能训练它们很多次)。另一方面，当使用更多的数据时，它们可能会更加准确。\n",
    "\n",
    "What is a metric? A *metric* is a function that measures the quality of the model's predictions using the validation set, and will be printed at the end of each *epoch*. In this case, we're using `error_rate`, which is a function provided by fastai that does just what it says: tells you what percentage of images in the validation set are being classified incorrectly. Another common metric for classification is `accuracy` (which is just `1.0 - error_rate`). fastai provides many more, which will be discussed throughout this book.\n",
    "\n",
    "\n",
    "什么是评估指标？*评估指标* 是一个使用验证集测量模型预测质量的函数，将在每次*迭代*的末尾打印。在这种情况下，我们使用 `error_rate`，这是fastai提供的一个函数，它只执行其命令: 告诉你验证集中有多少百分比的图像被错误地分类。另一个常见的分类指标是 `accuracy` (即 `1.0-error_rate`)。Fastai提供了更多内容，将在本书中讨论。\n",
    "\n",
    "The concept of a metric may remind you of *loss*, but there is an important distinction. The entire purpose of loss is to define a \"measure of performance\" that the training system can use to update weights automatically. In other words, a good choice for loss is a choice that is easy for stochastic gradient descent to use. But a metric is defined for human consumption, so a good metric is one that is easy for you to understand, and that hews as closely as possible to what you want the model to do. At times, you might decide that the loss function is a suitable metric, but that is not necessarily the case.\n",
    "\n",
    "评估指标的概念可能会让你想到 *损失*，但有一个重要的区别。损失的全部目的是定义一个 “性能衡量”，训练系统可以使用它来自动更新权重。换句话说，对于损失而言，一个好的选择是易于使用随机梯度下降法的选择。但是评估指标是为人类消费而定义的，所以一个好的评估指标对你来说很容易理解，并且尽可能接近你想要模型做的事情。有时，你可能会认为损失函数是一个合适的指标，但情况并非如此。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cnn_learner` also has a parameter `pretrained`, which defaults to `True` (so it's used in this case, even though we haven't specified it), which sets the weights in your model to values that have already been trained by experts to recognize a thousand different categories across 1.3 million photos (using the famous [*ImageNet* dataset](http://www.image-net.org/)). A model that has weights that have already been trained on some other dataset is called a *pretrained model*. You should nearly always use a pretrained model, because it means that your model, before you've even shown it any of your data, is already very capable. And, as you'll see, in a deep learning model many of these capabilities are things you'll need, almost regardless of the details of your project. For instance, parts of pretrained models will handle edge, gradient, and color detection, which are needed for many tasks.\n",
    "\n",
    "\n",
    "`cnn_learner` 还有一个参数 `pretrained`，默认为 `True` (因此即使在我们未指定它的情况下也使用了它)，它将模型中的权重设置为已经由专家训练的值，以识别130万张照片中的一千个不同类别 (使用著名的 [*ImageNet* 数据集](http://www.image-net.org/))。具有已经在其他数据集上训练过的权重的模型称为 *预训练模型*。你基本应该始终使用经过预训练的模型，因为这意味着在甚至没有显示任何数据之前，你的模型已经非常有能力。正如你将看到的，在深度学习模型中，几乎所有这些功能都是你所需要的，而与项目的细节无关。例如，预训练模型的某些部分将处理许多任务所需的边缘，渐变和颜色检测。\n",
    "\n",
    "When using a pretrained model, `cnn_learner` will remove the last layer, since that is always specifically customized to the original training task (i.e. ImageNet dataset classification), and replace it with one or more new layers with randomized weights, of an appropriate size for the dataset you are working with. This last part of the model is known as the *head*.\n",
    "\n",
    "\n",
    "当使用预训练模型时，`cnn_learner` 将移除最后一层，因为它总是针对原始训练任务进行专门定制的 (即ImageNet数据集分类)，并将其替换为一个或多个具有随机权重的层，这些层的大小适合您正在使用的数据集。模型的这最后一部分被称为 *模型头*。\n",
    "\n",
    "Using pretrained models is the *most* important method we have to allow us to train more accurate models, more quickly, with less data, and less time and money. You might think that would mean that using pretrained models would be the most studied area in academic deep learning... but you'd be very, very wrong! The importance of pretrained models is generally not recognized or discussed in most courses, books, or software library features, and is rarely considered in academic papers. As we write this at the start of 2020, things are just starting to change, but it's likely to take a while. So be careful: most people you speak to will probably greatly underestimate what you can do in deep learning with few resources, because they probably won't deeply understand how to use pretrained models.\n",
    "\n",
    "\n",
    "使用预训练模型是让我们用更少的数据、更少的时间和金钱来训练更精确、更快的模型的最重要的方法。你可能会认为这意味着使用预训练模型将是学术深度学习中研究最多的领域。但是你会大错特错了!在大多数课程，书籍或软件库功能中，通常都不会承认或讨论预训练模型的重要性，因此学术论文很少涉及。当我们在2020年初写这篇文章时，刚刚开始发生变化，但这可能还需要一段时间。所以要小心:与你交谈的大多数人可能会大大低估你只需很少的资源就能在深度学习中完成的工作，因为他们可能不会深刻理解如何使用预训练的模型。\n",
    "\n",
    "Using a pretrained model for a task different to what it was originally trained for is known as *transfer learning*. Unfortunately, because transfer learning is so under-studied, few domains have pretrained models available. For instance, there are currently few pretrained models available in medicine, making transfer learning challenging to use in that domain. In addition, it is not yet well understood how to use transfer learning for tasks such as time series analysis.\n",
    "\n",
    "使用预训练模型进行不同于最初训练的任务，被称为 *迁移学习*。不幸的是，由于对迁移学习的研究不足，很少有领域有预训练模型可用。例如，目前医学上几乎没有预训练模型，这使得迁移学习在该领域的使用具有挑战性。此外，尚未充分了解如何将迁移学习用于诸如时间序列分析之类的任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> jargon: Transfer learning: Using a pretrained model for a task different to what it was originally trained for.\n",
    "\n",
    "> 行话: 迁移学习: 使用预训练模型来完成与最初训练不同的任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sixth line of our code tells fastai how to *fit* the model:\n",
    "\n",
    "\n",
    "我们代码的第六行告诉fastai如何 *拟合* 模型:\n",
    "\n",
    "```python\n",
    "learn.fine_tune(1)\n",
    "```\n",
    "\n",
    "As we've discussed, the architecture only describes a *template* for a mathematical function; it doesn't actually do anything until we provide values for the millions of parameters it contains.\n",
    "\n",
    "\n",
    "正如我们已经讨论过的，该架构只描述了数学函数的 *模板*; 在我们为它包含的数百万个参数提供值之前，它实际上什么也做不了。\n",
    "\n",
    "This is the key to deep learning—determining how to fit the parameters of a model to get it to solve your problem. In order to fit a model, we have to provide at least one piece of information: how many times to look at each image (known as number of *epochs*). The number of epochs you select will largely depend on how much time you have available, and how long you find it takes in practice to fit your model. If you select a number that is too small, you can always train for more epochs later.\n",
    "\n",
    "\n",
    "这是深度学习的关键 -- 确定如何拟合模型的参数来解决您的问题。为了拟合一个模型，我们必须提供至少一条信息: 看每张图像多少次 (称为 *迭代*)。选择的迭代数量将在很大程度上取决于你有多少可用时间，以及你在实践中发现适合你的模型需要多长时间。如果你选择的数字太小，你可以随时训练更多的迭代。\n",
    "\n",
    "But why is the method called `fine_tune`, and not `fit`? fastai actually *does* have a method called `fit`, which does indeed fit a model (i.e. look at images in the training set multiple times, each time updating the parameters to make the predictions closer and closer to the target labels). But in this case, we've started with a pretrained model, and we don't want to throw away all those capabilities that it already has. As you'll learn in this book, there are some important tricks to adapt a pretrained model for a new dataset—a process called *fine-tuning*.\n",
    "\n",
    "但是为什么这个方法叫做 `fine_tune`，而不是 `fit`？Fastai实际上 *的确* 有一种叫做`fit`的方法，它确实拟合一个模型 (即多次查看训练集中的图像，每次更新参数以使预测越来越接近目标标签)。但是在这种情况下，我们从预训练模型开始，并不想抛弃它已经拥有的所有功能。正如你将在本书中了解到的，有一些重要的技巧可以为一个新的数据集调整一个预训练模型 -- 一个叫做 *精调* 的过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> jargon: Fine-tuning: A transfer learning technique where the parameters of a pretrained model are updated by training for additional epochs using a different task to that used for pretraining.\n",
    "\n",
    "> 行话: 精调: 一种迁移学习技术，其中预训练模型的参数通过使用与预训练不同的任务对其他迭代进行训练来更新。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you use the `fine_tune` method, fastai will use these tricks for you. There are a few parameters you can set (which we'll discuss later), but in the default form shown here, it does two steps:\n",
    "\n",
    "\n",
    "当使用 `fine_tune` 方法时，fastai将为你使用这些技巧。你可以设置几个参数 (我们稍后将讨论)，但是在这里显示的默认形式中，它执行两个步骤:\n",
    "\n",
    "1. Use one epoch to fit just those parts of the model necessary to get the new random head to work correctly with your dataset.\n",
    "1. Use the number of epochs requested when calling the method to fit the entire model, updating the weights of the later layers (especially the head) faster than the earlier layers (which, as we'll see, generally don't require many changes from the pretrained weights).\n",
    "\n",
    "\n",
    "1. 使用一个迭代只适合模型的那些部分，以使新的随机模型头正确地与你的数据集一起工作。\n",
    "1. 调用该方法以拟合整个模型时，使用请求的迭代数，比起先前的层，会更快地更新后续层（特别是模型头）的权重 (正如我们将看到的那样，通常不需要对预训练的权重进行很多更改)。\n",
    "\n",
    "The *head* of a model is the part that is newly added to be specific to the new dataset. An *epoch* is one complete pass through the dataset. After calling `fit`, the results after each epoch are printed, showing the epoch number, the training and validation set losses (the \"measure of performance\" used for training the model), and any *metrics* you've requested (error rate, in this case).\n",
    "\n",
    "模型的 *模型头* 是新添加的特定于新数据集的部分。*迭代* 是通过数据集的一次完整传递。调用 `fit`后，打印每次迭代后的结果，显示迭代编号、训练和验证集损失 (用于训练模型的 “性能衡量”)，以及你要求的任何 *评估指标* (在本例中为错误率)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, with all this code our model learned to recognize cats and dogs just from labeled examples. But how did it do it?\n",
    "\n",
    "因此，通过所有这些代码，我们的模型学会了仅仅从标记的例子中识别猫和狗。但是它是如何做到的呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Our Image Recognizer Learned\n",
    "\n",
    "# 我们的图像识别器学到了什么"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage we have an image recognizer that is working very well, but we have no idea what it is actually doing! Although many people complain that deep learning results in impenetrable \"black box\" models (that is, something that gives predictions but that no one can understand), this really couldn't be further from the truth. There is a vast body of research showing how to deeply inspect deep learning models, and get rich insights from them. Having said that, all kinds of machine learning models (including deep learning, and traditional statistical models) can be challenging to fully understand, especially when considering how they will behave when coming across data that is very different to the data used to train them. We'll be discussing this issue throughout this book.\n",
    "\n",
    "\n",
    "在这个阶段，我们有一个运行良好的图像识别器，但是我们不知道它实际上在做什么!虽然许多人抱怨深度学习导致了难以理解的 “黑盒” 模型 (即给出预测但没有人能理解的东西)，这其实与事实相去甚远。有大量的研究表明如何深入检查深度学习模型，并从中获得丰富的见解。尽管如此，各种机器学习模型 (包括深度学习和传统统计模型) 都很难完全理解，尤其是在考虑它们遇到与用于训练它们的数据截然不同的数据时的表现时。我们将在这本书中讨论这个问题。\n",
    "\n",
    "In 2013 a PhD student, Matt Zeiler, and his supervisor, Rob Fergus, published the paper [\"Visualizing and Understanding Convolutional Networks\"](https://arxiv.org/pdf/1311.2901.pdf), which showed how to visualize the neural network weights learned in each layer of a model. They carefully analyzed the model that won the 2012 ImageNet competition, and used this analysis to greatly improve the model, such that they were able to go on to win the 2013 competition! <<img_layer1>> is the picture that they published of the first layer's weights.\n",
    "\n",
    "2013，一名博士生Matt Zeiler和他的导师Rob Fergus发表了论文 [“可视化和理解卷积网络”](https://arxiv.org/pdf/1311.2901.pdf)，其中展示了如何可视化在模型的每一层中习得的神经网络权重。他们仔细分析了赢得2012图像网比赛的模型，并利用这一分析大大改进了模型，使他们能够继续赢得2013比赛! <<img_layer1>> 是他们发布的第一层权重的图片。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/layer1.png\" alt=\"Activations of the first layer of a CNN\" width=\"300\" caption=\"Activations of the first layer of a CNN (courtesy of Matthew D. Zeiler and Rob Fergus)\" id=\"img_layer1\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This picture requires some explanation. For each layer, the image part with the light gray background shows the reconstructed weights pictures, and the larger section at the bottom shows the parts of the training images that most strongly matched each set of weights. For layer 1, what we can see is that the model has discovered weights that represent diagonal, horizontal, and vertical edges, as well as various different gradients. (Note that for each layer only a subset of the features are shown; in practice there are thousands across all of the layers.) These are the basic building blocks that the model has learned for computer vision. They have been widely analyzed by neuroscientists and computer vision researchers, and it turns out that these learned building blocks are very similar to the basic visual machinery in the human eye, as well as the handcrafted computer vision features that were developed prior to the days of deep learning. The next layer is represented in <<img_layer2>>.\n",
    "\n",
    "这张图片需要解释一下。对于每一层，具有浅灰色背景的图像部分显示重建的权重图片，底部较大的部分显示了训练图像中与每组权重最匹配的部分。对于第一层，我们可以看到的是，模型发现了代表对角线、水平和垂直边缘的权重，以及各种不同的梯度。(请注意，对于每个层，仅显示特征子集; 实际上，所有层上都有数千个。)这些是模型为计算机视觉学习的基本构建块。神经科学家和计算机视觉研究人员已经对它们进行了广泛的分析，结果发现这些习得的构建模块与人眼中的基本视觉机械非常相似，以及在深度学习之前开发的手工计算机视觉功能。下一层用 <<img_layer2>> 表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/layer2.png\" alt=\"Activations of the second layer of a CNN\" width=\"800\" caption=\"Activations of the second layer of a CNN (courtesy of Matthew D. Zeiler and Rob Fergus)\" id=\"img_layer2\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For layer 2, there are nine examples of weight reconstructions for each of the features found by the model. We can see that the model has learned to create feature detectors that look for corners, repeating lines, circles, and other simple patterns. These are built from the basic building blocks developed in the first layer. For each of these, the right-hand side of the picture shows small patches from actual images which these features most closely match. For instance, the particular pattern in row 2, column 1 matches the gradients and textures associated with sunsets.\n",
    "\n",
    "\n",
    "在第2层，对于该模型发现的每个特征，有九个权重重构示例。我们可以看到，该模型已学会创建特征检测器，以寻找角，重复线，圆形和其他的简单图案。这些根据从第一层开发的基本构建块构建的。对于其中的每一个，图片的右侧显示了实际图像中的小块，这些小块与这些特征最匹配。例如，第2行第1列中的特定图案与与日落关联的渐变和纹理相匹配。\n",
    "\n",
    "<<img_layer3>> shows the image from the paper showing the results of reconstructing the features of layer 3.\n",
    "\n",
    "<<Img_layer3>> 显示了来自纸张的图像，显示了重建第3层特征的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/chapter2_layer3.PNG\" alt=\"Activations of the third layer of a CNN\" width=\"800\" caption=\"Activations of the third layer of a CNN (courtesy of Matthew D. Zeiler and Rob Fergus)\" id=\"img_layer3\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see by looking at the righthand side of this picture, the features are now able to identify and match with higher-level semantic components, such as car wheels, text, and flower petals. Using these components, layers four and five can identify even higher-level concepts, as shown in <<img_layer4>>.\n",
    "\n",
    "你可以通过查看图片的右侧看到，这些功能现在能够识别并匹配更高级别的语义组件，例如车轮、文本、和花瓣。使用这些组件，第四层和第五层可以识别更高级别的概念，如 <<img_layer4>> 所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/chapter2_layer4and5.PNG\" alt=\"Activations of layers 4 and 5 of a CNN\" width=\"800\" caption=\"Activations of layers 4 and 5 of a CNN (courtesy of Matthew D. Zeiler and Rob Fergus)\" id=\"img_layer4\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This article was studying an older model called *AlexNet* that only contained five layers. Networks developed since then can have hundreds of layers—so you can imagine how rich the features developed by these models can be! \n",
    "\n",
    "\n",
    "本文正在研究一个名为 *AlexNet* 的旧模型，它只包含五层。从那时起开发的网络可以有数百个层-所以你可以想象这些模型开发的功能有多丰富!\n",
    "\n",
    "When we fine-tuned our pretrained model earlier, we adapted what those last layers focus on (flowers, humans, animals) to specialize on the cats versus dogs problem. More generally, we could specialize such a pretrained model on many different tasks. Let's have a look at some examples. \n",
    "\n",
    "在之前对我们的预训练模型进行精调时，我们调整了那些最后几层的重点 (花朵、人类、动物)，以专门研究猫与狗的问题。更笼统地说，我们可以将这种经过预训练的模型专门用于许多不同的任务。让我们来看一些例子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Recognizers Can Tackle Non-Image Tasks\n",
    "\n",
    "# 图像识别器可以处理非图像任务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An image recognizer can, as its name suggests, only recognize images. But a lot of things can be represented as images, which means that an image recogniser can learn to complete many tasks.\n",
    "\n",
    "\n",
    "顾名思义，图像识别器只能识别图像。但是很多东西可以被表示为图像，这意味着图像识别器可以学习完成许多任务。\n",
    "\n",
    "For instance, a sound can be converted to a spectrogram, which is a chart that shows the amount of each frequency at each time in an audio file. Fast.ai student Ethan Sutin used this approach to easily beat the published accuracy of a state-of-the-art [environmental sound detection model](https://medium.com/@etown/great-results-on-audio-classification-with-fastai-library-ccaf906c5f52) using a dataset of 8,732 urban sounds. fastai's `show_batch` clearly shows how each different sound has a quite distinctive spectrogram, as you can see in <<img_spect>>.\n",
    "\n",
    "例如，声音可以转换为频谱图，这是一个图表，显示音频文件中每次每个频率的数量。Fast.ai学生Ethan Sutin使用这种方法用8,732种城市声音的数据集轻松击败最新发布的 [环境声音检测模型](https://medium.com/@etown/great-results-on-audio-classification-with-fastai-library-ccaf906c5f52) 的精度。Fastai的 `show_batch` 清楚地显示了每种不同的声音如何具有非常独特的频谱图，正如你在 <<img_spect>> 中看到的那样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"show_batch with spectrograms of sounds\" width=\"400\" caption=\"show_batch with spectrograms of sounds\" id=\"img_spect\" src=\"images/att_00012.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A time series can easily be converted into an image by simply plotting the time series on a graph. However, it is often a good idea to try to represent your data in a way that makes it as easy as possible to pull out the most important components. In a time series, things like seasonality and anomalies are most likely to be of interest. There are various transformations available for time series data. For instance, fast.ai student Ignacio Oguiza created images from a time series dataset for olive oil classification, using a technique called Gramian Angular Difference Field (GADF); you can see the result in <<ts_image>>. He then fed those images to an image classification model just like the one you see in this chapter. His results, despite having only 30 training set images, were well over 90% accurate, and close to the state of the art.\n",
    "\n",
    "通过简单地在图表上绘制时间序列，可以很容易地将时间序列转换成图像。但是，尝试以尽可能容易地提取最重要的组件的方式表示数据通常是一个好方法。在时间序列中，季节性和异常等最有可能引起人们的兴趣。有多种转换可用于时间序列数据。例如，fast.ai学生Ignacio Oguiza使用称为格拉姆角差场(GADF)的技术从用于橄榄油分类的时间序列数据集创建了图像; 你可以在 <<ts_image>> 中看到结果。然后，他将这些图像提供给图像分类模型，就像你在本章中看到的那样。尽管只有30个训练集图像，但他的结果仍准确率超过90％，并且接近最先进的水平。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Converting a time series into an image\" width=\"700\" caption=\"Converting a time series into an image\" id=\"ts_image\" src=\"images/att_00013.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting fast.ai student project example comes from Gleb Esman. He was working on fraud detection at Splunk, using a dataset of users' mouse movements and mouse clicks. He turned these into pictures by drawing an image where the position, speed, and acceleration of the mouse pointer was displayed using coloured lines, and the clicks were displayed using [small colored circles](https://www.splunk.com/en_us/blog/security/deep-learning-with-splunk-and-tensorflow-for-security-catching-the-fraudster-in-neural-networks-with-behavioral-biometrics.html), as shown in <<splunk>>. He then fed this into an image recognition model just like the one we've used in this chapter, and it worked so well that it led to a patent for this approach to fraud analytics!\n",
    "\n",
    "另一个有趣的fast.ai学生项目示例来自Gleb Esman。他在Splunk从事欺诈检测的工作，使用的是用户鼠标移动和鼠标点击的数据集。他通过绘制一幅图像，将这些图像转化为图片，其中鼠标指针的位置、速度和加速度用彩色线条显示，点击用 [小彩色圆圈](https://www.splunk.com/en_us/blog/security/deep-learning-with-splunk-and-tensorflow-for-security-catching-the-fraudster-in-neural-networks-with-behavioral-biometrics.html)，如 <<splunk>> 所示。然后，他将其输入到一个图像识别模型中，就像我们在本章中使用的模型一样，该模型运行良好，以至于这种欺诈分析方法获得了专利!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Converting computer mouse behavior to an image\" width=\"450\" caption=\"Converting computer mouse behavior to an image\" id=\"splunk\" src=\"images/att_00014.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example comes from the paper [\"Malware Classification with Deep Convolutional Neural Networks\"](https://ieeexplore.ieee.org/abstract/document/8328749) by Mahmoud Kalash et al., which explains that \"the malware binary file is divided into 8-bit sequences which are then converted to equivalent decimal values. This decimal vector is reshaped and a gray-scale image is generated that represents the malware sample,\" like in <<malware_proc>>.\n",
    "\n",
    "另一个例子来自Mahmoud Kalash等人的论文 [《深度卷积神经网络的恶意软件分类》](https://ieeexplore.ieee.org/abstract/document/8328749)，其解释说 “恶意软件二进制文件被分成8位序列，然后转换为等效的十进制值。对该十进制矢量进行调整，并生成代表恶意软件样本的灰度图像，“就像 <<malware_proc>> 中一样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Malware classification process\" width=\"623\" caption=\"Malware classification process\" id=\"malware_proc\" src=\"images/att_00055.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors then show \"pictures\" generated through this process of malware in different categories, as shown in <<malware_eg>>.\n",
    "\n",
    "然后，作者在不同的类别中显示通过恶意软件的这个过程生成的 “图片”，如 <<malware_eg>> 所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Malware examples\" width=\"650\" caption=\"Malware examples\" id=\"malware_eg\" src=\"images/att_00056.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the different types of malware look very distinctive to the human eye. The model the researchers trained based on this image representation was more accurate at malware classification than any previous approach shown in the academic literature. This suggests a good rule of thumb for converting a dataset into an image representation: if the human eye can recognize categories from the images, then a deep learning model should be able to do so too.\n",
    "\n",
    "\n",
    "正如你所看到的，不同类型的恶意软件在人眼中看来大相径庭。研究人员基于这种图像表示训练的模型在恶意软件分类方面比学术文献中显示的任何以前的方法都更准确。这表明了将数据集转换为图像表示的一个很好的经验法则: 如果人眼能够从图像中识别类别，那么深度学习模型也应该能够这样做。\n",
    "\n",
    "In general, you'll find that a small number of general approaches in deep learning can go a long way, if you're a bit creative in how you represent your data! You shouldn't think of approaches like the ones described here as \"hacky workarounds,\" because actually they often (as here) beat previously state-of-the-art results. These really are the right ways to think about these problem domains.\n",
    "\n",
    "通常，如果你在表达数据的方式上有点创意，你会发现深度学习中的少量通用方法会大有帮助!你不应该考虑此处所说的 “黑客解决方法” 这样的方法，因为实际上它们经常 (像这里一样) 击败以前最先进的结果。这些确实是考虑这些问题领域的正确方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jargon Recap\n",
    "\n",
    "# 行话回顾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just covered a lot of information so let's recap briefly, <<dljargon>> provides a handy vocabulary.\n",
    "\n",
    "\n",
    "我们刚刚介绍了很多信息，所以让我们简单回顾一下，<<dljarmps>> 提供了一个方便的词汇。\n",
    "\n",
    "```asciidoc\n",
    "[[dljargon]]\n",
    ".Deep learning vocabulary\n",
    "[options=\"header\"]\n",
    "|=====\n",
    "| Term | Meaning\n",
    "|Label | The data that we're trying to predict, such as \"dog\" or \"cat\"\n",
    "|Architecture | The _template_ of the model that we're trying to fit; the actual mathematical function that we're passing the input data and parameters to\n",
    "|Model | The combination of the architecture with a particular set of parameters\n",
    "|Parameters | The values in the model that change what task it can do, and are updated through model training\n",
    "|Fit | Update the parameters of the model such that the predictions of the model using the input data match the target labels\n",
    "|Train | A synonym for _fit_\n",
    "|Pretrained model | A model that has already been trained, generally using a large dataset, and will be fine-tuned\n",
    "|Fine-tune | Update a pretrained model for a different task\n",
    "|Epoch | One complete pass through the input data\n",
    "|Loss | A measure of how good the model is, chosen to drive training via SGD\n",
    "|Metric | A measurement of how good the model is, using the validation set, chosen for human consumption\n",
    "|Validation set | A set of data held out from training, used only for measuring how good the model is\n",
    "|Training set | The data used for fitting the model; does not include any data from the validation set\n",
    "|Overfitting | Training a model in such a way that it _remembers_ specific features of the input data, rather than generalizing well to data not seen during training\n",
    "|CNN | Convolutional neural network; a type of neural network that works particularly well for computer vision tasks\n",
    "|=====\n",
    "```\n",
    "\n",
    "```asciidoc\n",
    "[[行业术语]]\n",
    ".深度学习词汇\n",
    "[Options=\"header\"]\n",
    "|=====\n",
    "| 术语 | 含义\n",
    "|标签 | 我们试图预测的数据，如 “狗” 或 “猫”\n",
    "|架构 | 我们试图拟合的模型的 _模板_; 我们将输入数据和参数传递给的实际数学函数\n",
    "|模型 | 架构与一组特定参数的组合\n",
    "|参数 | 模型中改变其可以执行的任务的值，并通过模型训练进行更新\n",
    "|拟合 | 更新模型的参数，以便使用输入数据的模型的预测与目标标签匹配\n",
    "|训练 | 一个“拟合”的同义词\n",
    "|预训练模型 | 已经训练过的模型，一般使用大型数据集，并将进行精调\n",
    "|精调 | 更新不同任务的预训练模型\n",
    "|迭代 | 输入数据一次完整传递\n",
    "|损失 | 衡量模型有多好，选择通过随机梯度下降推动培训\n",
    "|评估指标 | 使用验证集对模型的好坏程度进行衡量，以供人类使用\n",
    "|验证集 | 训练出来的一组数据，仅用于衡量模型的好坏\n",
    "|训练集 | 用于拟合模型的数据; 不包括来自验证集的任何数据\n",
    "|过拟合 | 训练一个模型，使它能够“记住”输入数据的具体特征，而不是把它推广到训练时没见过的数据\n",
    "|CNN | 卷积神经网络; 一种特别适用于计算机视觉任务的神经网络\n",
    "|=====\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this vocabulary in hand, we are now in a position to bring together all the key concepts introduced so far. Take a moment to review those definitions and read the following summary. If you can follow the explanation, then you're well equipped to understand the discussions to come.\n",
    "\n",
    "\n",
    "有了这些词汇，我们现在能够将迄今为止介绍的所有关键概念汇集在一起。花点时间回顾这些定义，并阅读以下摘要。如果你可以按照说明进行操作，那么你就可以很好地理解接下来的讨论。\n",
    "\n",
    "*Machine learning* is a discipline where we define a program not by writing it entirely ourselves, but by learning from data. *Deep learning* is a specialty within machine learning that uses *neural networks* with multiple *layers*. *Image classification* is a representative example (also known as *image recognition*). We start with *labeled data*; that is, a set of images where we have assigned a *label* to each image indicating what it represents. Our goal is to produce a program, called a *model*, which, given a new image, will make an accurate *prediction* regarding what that new image represents.\n",
    "\n",
    "\n",
    "*机器学习* 是一门学科，我们定义为一个程序不是完全由自己编写，而是通过从数据中学习。*深度学习* 是机器学习中的一个专业，它使用具有多个 *层* 的 *神经网络*。*图像分类* 是一个代表性示例 (也称为 *图像识别*)。我们从 *带标签的数据* 开始; 也就是说，一组图像，我们已经为每个图像分配了一个 *标签* ，指示它代表什么。我们的目标是制作一个名为 *模型* 的程序，给定一个新图像，它将对新图像代表什么做出准确的 *预测*。\n",
    "\n",
    "Every model starts with a choice of *architecture*, a general template for how that kind of model works internally. The process of *training* (or *fitting*) the model is the process of finding a set of *parameter values* (or *weights*) that specialize that general architecture into a model that works well for our particular kind of data. In order to define how well a model does on a single prediction, we need to define a *loss function*, which determines how we score a prediction as good or bad.\n",
    "\n",
    "\n",
    "每个模型都从选择 *架构* 开始，这是这种模型在内部如何工作的通用模板。*训练* (或 *拟合*) 模型的过程就是找出一组 *参数值* (或 *权重*) 的过程，它将通用架构专门化为一个模型，该模型适用于我们特定类型的数据。为了定义一个模型在单个预测中的表现，我们需要定义一个 *损失函数*，该函数确定我们如何对预测进行评分。\n",
    "\n",
    "To make the training process go faster, we might start with a *pretrained model*—a model that has already been trained on someone else's data. We can then adapt it to our data by training it a bit more on our data, a process called *fine-tuning*.\n",
    "\n",
    "\n",
    "为了使训练过程更快，我们可以从一个 *预训练模型* 开始 -- 一个已经在别人的数据上训练过的模型。然后，我们可以通过在我们的数据上多训练一点来使它适应我们的数据，这个过程叫做 *精调*。\n",
    "\n",
    "When we train a model, a key concern is to ensure that our model *generalizes*—that is, that it learns general lessons from our data which also apply to new items it will encounter, so that it can make good predictions on those items. The risk is that if we train our model badly, instead of learning general lessons it effectively memorizes what it has already seen, and then it will make poor predictions about new images. Such a failure is called *overfitting*. In order to avoid this, we always divide our data into two parts, the *training set* and the *validation set*. We train the model by showing it only the training set and then we evaluate how well the model is doing by seeing how well it performs on items from the validation set. In this way, we check if the lessons the model learns from the training set are lessons that generalize to the validation set. In order for a person to assess how well the model is doing on the validation set overall, we define a *metric*. During the training process, when the model has seen every item in the training set, we call that an *epoch*.\n",
    "\n",
    "\n",
    "当我们训练一个模型时，一个关键的问题是确保我们的模型 *一般性* -- 也就是说，它从我们的数据中吸取了一般性的经验，这些经验也适用于它将遇到的新项目，这样它就可以对这些项目做出良好的预测。风险在于，如果我们训练得不好，没有学习一般性的经验，它会有效地记住它已经见过的东西，然后它会对新的图像做出糟糕的预测。这样的失败被称为 *过拟合*。为了避免这种情况，我们总是将我们的数据分成两部分，*训练集* 和 *验证集*。我们通过只显示训练集来训练模型，然后通过查看模型在验证集中的项目上的表现来评估模型的表现。通过这种方式，我们检查模型从训练集中学到的经验是否可推广到验证集的经验。为了让人们评估模型在整个验证集中的表现，我们定义了一个 *评估指标*。在训练过程中，当模型遇到训练集中的每个项目时，我们称之为 *迭代*。\n",
    "\n",
    "All these concepts apply to machine learning in general. That is, they apply to all sorts of schemes for defining a model by training it with data. What makes deep learning distinctive is a particular class of architectures: the architectures based on *neural networks*. In particular, tasks like image classification rely heavily on *convolutional neural networks*, which we will discuss shortly.\n",
    "\n",
    "所有这些概念一般都适用于机器学习。也就是说，它们适用于通过用数据训练模型来定义模型的各种方案。使深度学习与众不同的是一类特定的架构: 基于 *神经网络* 的架构。特别是像图像分类这样的任务在很大程度上依赖于“卷积神经网络”，我们将在稍后讨论。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Is Not Just for Image Classification\n",
    "\n",
    "## 深度学习不仅仅用于图像分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning's effectiveness for classifying images has been widely discussed in recent years, even showing _superhuman_ results on complex tasks like recognizing malignant tumors in CT scans. But it can do a lot more than this, as we will show here.\n",
    "\n",
    "\n",
    "近年来，深度学习对图像进行分类的有效性已被广泛讨论，甚至在诸如CT扫描中识别恶性肿瘤等复杂任务上显示出“超人”的结果。但是它能做的不止这些，正如我们将在这里展示的。\n",
    "\n",
    "For instance, let's talk about something that is critically important for autonomous vehicles: localizing objects in a picture. If a self-driving car doesn't know where a pedestrian is, then it doesn't know how to avoid one! Creating a model that can recognize the content of every individual pixel in an image is called *segmentation*. Here is how we can train a segmentation model with fastai, using a subset of the [*Camvid* dataset](http://www0.cs.ucl.ac.uk/staff/G.Brostow/papers/Brostow_2009-PRL.pdf) from the paper \"Semantic Object Classes in Video: A High-Definition Ground Truth Database\" by Gabruel J. Brostow, Julien Fauqueur, and Roberto Cipolla:\n",
    "\n",
    "例如，让我们来谈谈对自动驾驶汽车至关重要的事情: 在图片中定位对象。如果自动驾驶汽车不知道行人在哪里，那么它就不知道如何避开行人!创建一个能够识别图像中每个像素内容的模型称为 *分割* 。以下是我们如何使用fastai训练分割模型，使用 [* Camvid * 数据集](http://www0.cs.ucl.ac.uk/staff/G.Brostow/papers/Brostow_2009-PRL.pdf) 的子集， 摘自Gabruel J.Brostow，Julien Fauqueur和Roberto Cipolla的论文《视频中的语义对象类: 高清地面实况数据库》:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.CAMVID_TINY)\n",
    "dls = SegmentationDataLoaders.from_label_func(\n",
    "    path, bs=8, fnames = get_image_files(path/\"images\"),\n",
    "    label_func = lambda o: path/'labels'/f'{o.stem}_P{o.suffix}',\n",
    "    codes = np.loadtxt(path/'codes.txt', dtype=str)\n",
    ")\n",
    "\n",
    "learn = unet_learner(dls, resnet34)\n",
    "learn.fine_tune(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not even going to walk through this code line by line, because it is nearly identical to our previous example! (Although we will be doing a deep dive into segmentation models in <<chapter_arch_details>>, along with all of the other models that we are briefly introducing in this chapter, and many, many more.)\n",
    "\n",
    "\n",
    "我们甚至不会逐行地介绍这个代码，因为它几乎与前面的示例相同!(尽管我们将深入研究 <<chapter_arch_details>> 中的分割模型，以及我们在本章中简要介绍的所有其他模型，以及许多许多模型。)\n",
    "\n",
    "We can visualize how well it achieved its task, by asking the model to color-code each pixel of an image. As you can see, it nearly perfectly classifies every pixel in every object. For instance, notice that all of the cars are overlaid with the same color and all of the trees are overlaid with the same color (in each pair of images, the lefthand image is the ground truth label and the right is the prediction from the model):\n",
    "\n",
    "通过要求模型对图像的每个像素进行颜色编码，我们可以形象地看到其完成任务的程度。正如你所看到的，它几乎完美地对每个对象中的每个像素进行了分类。例如，请注意，所有的汽车都用相同的颜色覆盖，所有的树木都用相同的颜色覆盖 (在每对图像中，左边的图像是地面实况标签，右边是模型的预测):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results(max_n=6, figsize=(7,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One other area where deep learning has dramatically improved in the last couple of years is natural language processing (NLP). Computers can now generate text, translate automatically from one language to another, analyze comments, label words in sentences, and much more. Here is all of the code necessary to train a model that can classify the sentiment of a movie review better than anything that existed in the world just five years ago:\n",
    "\n",
    "深度学习在过去几年中有显著改善的另一个领域是自然语言处理 (NLP)。现在，计算机可以生成文本，自动将一种语言翻译成另一种语言，分析注释，在句子中标记单词等等。以下是训练一个模型所必需的所有代码，该模型可以比五年前的世界上任何事物更好地对电影评论的情感进行分类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *\n",
    "\n",
    "dls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test')\n",
    "learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n",
    "learn.fine_tune(4, 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clean\n",
    "If you hit a \"CUDA out of memory error\" after running this cell, click on the menu Kernel, then restart. Instead of executing the cell above, copy and paste the following code in it:\n",
    "\n",
    "\n",
    "# 清除\n",
    "如果您在运行此单元格后遇到 “CUDA内存不足错误”，请单击菜单内核，然后重新启动。不要执行上面的单元格，而是在其中复制并粘贴以下代码:\n",
    "\n",
    "```\n",
    "from fastai.text.all import *\n",
    "\n",
    "dls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test', bs=32)\n",
    "learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n",
    "learn.fine_tune(4, 1e-2)\n",
    "```\n",
    "\n",
    "This reduces the batch size to 32 (we will explain this later). If you keep hitting the same error, change 32 to 16.\n",
    "\n",
    "这将批次大小减少到32 (我们将在后面解释这一点)。如果你仍然遇到相同的错误，请将32更改为16。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is using the [\"IMDb Large Movie Review dataset\"](https://ai.stanford.edu/~ang/papers/acl11-WordVectorsSentimentAnalysis.pdf) from the paper \"Learning Word Vectors for Sentiment Analysis\" by Andrew Maas et al. It works well with movie reviews of many thousands of words, but let's test it out on a very short one to see how it does its thing:\n",
    "\n",
    "此模型使用的是 [“IMDb大型电影评论数据集”](https://ai.stanford.edu/~ang/papers/acl11-WordVectorsSentimentAnalysis.pdf)，来自Andrew Maas等人的论文 “学习词向量以进行情感分析”。它适用于成千上万个单词的电影评论，但是让我们在很简短的单词上对其进行测试，看看它是如何做到的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.predict(\"I really liked that movie!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the model has considered the review to be positive. The second part of the result is the index of \"pos\" in our data vocabulary and the last part is the probabilities attributed to each class (99.6% for \"pos\" and 0.4% for \"neg\"). \n",
    "\n",
    "\n",
    "在这里，我们可以看到该模型认为该评论是正面的。结果的第二部分是数据词汇中 “正面的” 的索引，最后一部分是归因于每个类别的概率 (“正面的” 为99.6%，“负面的” 为0.4%)。\n",
    "\n",
    "Now it's your turn! Write your own mini movie review, or copy one from the internet, and you can see what this model thinks about it. \n",
    "\n",
    "现在轮到你了!你自己写一篇电影小短评，或者从网上复制一篇，就可以看到这个模型对此的看法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sidebar: The Order Matters\n",
    "\n",
    "# 侧边栏: 顺序很重要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Jupyter notebook, the order in which you execute each cell is very important. It's not like Excel, where everything gets updated as soon as you type something anywhere—it has an inner state that gets updated each time you execute a cell. For instance, when you run the first cell of the notebook (with the \"CLICK ME\" comment), you create an object called `learn` that contains a model and data for an image classification problem. If we were to run the cell just shown in the text (the one that predicts if a review is good or not) straight after, we would get an error as this `learn` object does not contain a text classification model. This cell needs to be run after the one containing:\n",
    "\n",
    "\n",
    "在Jupyter笔记本中，执行每个单元格的顺序非常重要。这不像Excel，当你在任何地方键入内容时，所有内容都会立即更新 -- 它具有一个内部状态，该状态在你每次执行单元格时都会更新。例如，当你运行笔记本的第一个单元格 (带有 “单击我” 注释) 时，你创建了一个名为 `学习` 的对象，其中包含图像分类问题的模型和数据。如果我们直接运行文本中显示的单元格 (预测评论好坏的单元格)，我们会得到一个错误信息，因为这个 `learn` 对象不包含文本分类模型。此单元格需要在包含以下内容的单元格之后运行:\n",
    "\n",
    "\n",
    "```python\n",
    "from fastai.text.all import *\n",
    "\n",
    "\n",
    "dls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test')\n",
    "learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, \n",
    "                                metrics=accuracy)\n",
    "learn.fine_tune(4, 1e-2)\n",
    "```\n",
    "\n",
    "\n",
    "The outputs themselves can be deceiving, because they include the results of the last time the cell was executed; if you change the code inside a cell without executing it, the old (misleading) results will remain.\n",
    "\n",
    "\n",
    "输出本身可能是欺骗性的，因为它们包括上次执行单元格的结果; 如果您在不执行单元格的情况下更改了单元格内的代码，旧的 (误导性的) 结果将保留下来。\n",
    "\n",
    "Except when we mention it explicitly, the notebooks provided on the [book website](https://book.fast.ai/) are meant to be run in order, from top to bottom. In general, when experimenting, you will find yourself executing cells in any order to go fast (which is a super neat feature of Jupyter Notebook), but once you have explored and arrived at the final version of your code, make sure you can run the cells of your notebooks in order (your future self won't necessarily remember the convoluted path you took otherwise!). \n",
    "\n",
    "\n",
    "除了当我们明确提到时，[图书网站](https://book.fast.ai/) 上提供的笔记本 应该从上到下按顺序运行。一般来说，在实验时，你会发现自己以任何顺序执行单元格都可以快速执行 (这是Jupyter笔记本的一个超好用的功能)，但是一旦你探索并获得了代码的最终版本，请确保可以按顺序运行笔记本的单元格(未来的自己不一定记得你走过的那些弯路!)。\n",
    "\n",
    "In command mode, pressing `0` twice will restart the *kernel* (which is the engine powering your notebook). This will wipe your state clean and make it as if you had just started in the notebook. Choose Run All Above from the Cell menu to run all cells above the point where you are. We have found this to be very useful when developing the fastai library.\n",
    "\n",
    "在命令模式下，按 `0` 两次将重新启动 *内核* (这是为笔记本供电的引擎)。这将清除你的状态，使其像刚刚在笔记本中启动一样。从 “单元格” 菜单中选择 “运行所有以上内容” 以运行你所在点上方的所有单元格。我们发现这在开发fastai库时非常有用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End sidebar\n",
    "\n",
    "# 结束侧边栏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ever have any questions about a fastai method, you should use the function `doc`, passing it the method name:\n",
    "\n",
    "\n",
    "如果对fastai方法有任何疑问，你应该使用函数doc并将方法名称传递给它：\n",
    "\n",
    "```python\n",
    "doc(learn.predict)\n",
    "```\n",
    "\n",
    "\n",
    "This will make a small window pop up with content like this:\n",
    "\n",
    "\n",
    "这将弹出一个小窗口，其中包含以下内容：\n",
    "\n",
    "<img src=\"images/doc_ex.png\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief one-line explanation is provided by `doc`. The \"Show in docs\" link take you to the full documentation, where you'll find all the details and lots of examples. Also, most of fastai's methods are just a handful of lines, so you can click the \"source\" link to see exactly what's going on behind the scenes.\n",
    "\n",
    "\n",
    "`doc` 会提供一个简短的单行解释。“在文档中显示” 链接将带你到完整的文档，在那里你会找到所有的细节和许多例子。此外，fastai的大多数方法只是少数几行，所以你可以点击 “源” 链接来查看幕后到底是什么情况。\n",
    "\n",
    "Let's move on to something much less sexy, but perhaps significantly more widely commercially useful: building models from plain *tabular* data.\n",
    "\n",
    "让我们继续讨论一些不那么性感，但可能在商业上更广泛有用的东西: 从简单的 *表格形式* 的数据构建模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> jargon: Tabular: Data that is in the form of a table, such as from a spreadsheet, database, or CSV file. A tabular model is a model that tries to predict one column of a table based on information in other columns of the table.\n",
    "\n",
    "> 行话: 表格形式：表格形式的数据，例如来自电子表格、数据库或CSV文件的数据。表格模型是一种试图根据表中其他列中的信息预测表中某一列的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that looks very similar too. Here is the code necessary to train a model that will predict whether a person is a high-income earner, based on their socioeconomic background:\n",
    "\n",
    "事实证明，这看起来也很相似。以下是训练一个模型所必需的代码，该模型将根据一个人的社会经济背景来预测他是否是一个高收入者:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.tabular.all import *\n",
    "path = untar_data(URLs.ADULT_SAMPLE)\n",
    "\n",
    "dls = TabularDataLoaders.from_csv(path/'adult.csv', path=path, y_names=\"salary\",\n",
    "    cat_names = ['workclass', 'education', 'marital-status', 'occupation',\n",
    "                 'relationship', 'race'],\n",
    "    cont_names = ['age', 'fnlwgt', 'education-num'],\n",
    "    procs = [Categorify, FillMissing, Normalize])\n",
    "\n",
    "learn = tabular_learner(dls, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, we had to tell fastai which columns are *categorical* (that is, contain values that are one of a discrete set of choices, such as `occupation`) and which are *continuous* (that is, contain a number that represents a quantity, such as `age`).\n",
    "\n",
    "\n",
    "如你所见，我们必须告诉fastai哪些列是 *离散的* (也就是说，包含一组离散选项中的一个值，例如 `occupation`) 以及哪些是 *连续的* (即，包含一个表示数量的数字，如 `age`)。\n",
    "\n",
    "There is no pretrained model available for this task (in general, pretrained models are not widely available for any tabular modeling tasks, although some organizations have created them for internal use), so we don't use `fine_tune` in this case. Instead we use `fit_one_cycle`, the most commonly used method for training fastai models *from scratch* (i.e. without transfer learning):\n",
    "\n",
    "没有预训练模型可用于此任务 (通常，尽管某些组织已创建了供内部使用的预训练模型，但它们并未广泛用于任何表格建模任务)，所以在这种情况下，我们不使用 `fine_tune`。相反，我们使用 `fit_one_cycle`，这是 *从零开始* 训练fastai模型的最常用方法（即无迁移学习):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is using the [*Adult* dataset](http://robotics.stanford.edu/~ronnyk/nbtree.pdf), from the paper \"Scaling Up the Accuracy of Naive-Bayes Classifiers: a Decision-Tree Hybrid\" by Rob Kohavi, which contains some demographic data about individuals (like their education, marital status, race, sex, and whether or not they have an annual income greater than \\$50k). The model is over 80\\% accurate, and took around 30 seconds to train.\n",
    "\n",
    "此模型使用的 [*成人*数据集](http://robotics.stanford.edu/~ronnyk/nbtree.pdf)，来自Rob Kohavi的论文 《提高朴素贝叶斯分类器的准确性: 决策树混合》，其中包含一些关于个人的人口统计数据 (如他们的教育，婚姻状况、种族、性别以及他们的年收入是否大于5万美元)。该模型的准确率超过80\\%，训练时间约为30秒。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one more. Recommendation systems are very important, particularly in e-commerce. Companies like Amazon and Netflix try hard to recommend products or movies that users might like. Here's how to train a model that will predict movies people might like, based on their previous viewing habits, using the [MovieLens dataset](https://doi.org/10.1145/2827872):\n",
    "\n",
    "让我们再看一个例子。推荐系统非常重要，尤其是在电子商务中。像亚马逊和网飞这样的公司致力于向用户推荐他们可能喜欢的产品或电影。以下是如何训练一个模型，根据人们以前的观看习惯，使用 [MovieLens数据集](https://doi.org/10.1145/2827872 ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Stateful' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-fd77de03facb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfastai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muntar_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mURLs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mML_SAMPLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCollabDataLoaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m'ratings.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollab_learner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfine_tune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fastai/collab.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtabular\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fastai/tabular/all.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fastai/basics.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlearner\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fastai/callback/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mfuncs_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mas_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStateful\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mGetAttr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;34m\"Basic class handling tweaks of the training loop by changing a `Learner` in various events\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0m_default\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrun_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrun_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'learn'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Stateful' is not defined"
     ]
    }
   ],
   "source": [
    "from fastai.collab import *\n",
    "path = untar_data(URLs.ML_SAMPLE)\n",
    "dls = CollabDataLoaders.from_csv(path/'ratings.csv')\n",
    "learn = collab_learner(dls, y_range=(0.5,5.5))\n",
    "learn.fine_tune(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is predicting movie ratings on a scale of 0.5 to 5.0 to within around 0.6 average error. Since we're predicting a continuous number, rather than a category, we have to tell fastai what range our target has, using the `y_range` parameter.\n",
    "\n",
    "\n",
    "该模型预测的电影收视率范围为0.5至5.0，平均误差在0.6左右。由于我们要预测的是连续的数字，而不是类别，所以我们必须使用 `y_range` 参数告诉fastai我们的目标范围是什么。\n",
    "\n",
    "Although we're not actually using a pretrained model (for the same reason that we didn't for the tabular model), this example shows that fastai lets us use `fine_tune` anyway in this case (you'll learn how and why this works in <<chapter_pet_breeds>>). Sometimes it's best to experiment with `fine_tune` versus `fit_one_cycle` to see which works best for your dataset.\n",
    "\n",
    "\n",
    "虽然我们实际上没有使用预训练模型 (出于与表格模型相同的原因)，这个例子表明fastai允许我们在这种情况下使用 `fine_tune` (你将在 <<chapter_pet_breeds>> 中了解该方法的工作原理和原因)。有时最好尝试 `fine_tune` 与 `fit_one_cycle`，看看哪个最适合你的数据集。\n",
    "\n",
    "We can use the same `show_results` call we saw earlier to view a few examples of user and movie IDs, actual ratings, and predictions:\n",
    "\n",
    "我们可以使用前面看到的相同的 `show_results` 调用来查看用户和电影id、实际评级和预测的一些示例:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sidebar: Datasets: Food for Models\n",
    "\n",
    "### 侧边栏: 数据集:模型的食物"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ve already seen quite a few models in this section, each one trained using a different dataset to do a different task. In machine learning and deep learning, we can’t do anything without data. So, the people that create datasets for us to train our models on are the (often underappreciated) heroes. Some of the most useful and important datasets are those that become important *academic baselines*; that is, datasets that are widely studied by researchers and used to compare algorithmic changes. Some of these become household names (at least, among households that train models!), such as MNIST, CIFAR-10, and ImageNet.\n",
    "\n",
    "\n",
    "你已经在本节中看到了相当多的模型，每个模型都使用不同的数据集进行了训练，以执行不同的任务。在机器学习和深度学习中，没有数据我们什么都做不了。因此，为我们创建数据集来训练我们的模型的人是 (经常被低估的) 英雄。一些最有用和最重要的是那些成为重要的 *学术基线* 的数据集; 也就是说，研究人员广泛研究并用于比较算法变化的数据集。其中一些成为家喻户晓的名字 (至少在训练模型的领域!)，如MNIST、CIFAR-10和ImageNet。\n",
    "\n",
    "The datasets used in this book have been selected because they provide great examples of the kinds of data that you are likely to encounter, and the academic literature has many examples of model results using these datasets to which you can compare your work.\n",
    "\n",
    "\n",
    "选择本书中使用的数据集是因为它们提供了你可能会遇到的各种数据的优秀示例，并且学术文献提供了许多使用这些数据集进行模型结果的示例，你可以将它们与你的工作进行比较。\n",
    "\n",
    "Most datasets used in this book took the creators a lot of work to build. For instance, later in the book we’ll be showing you how to create a model that can translate between French and English. The key input to this is a French/English parallel text corpus prepared back in 2009 by Professor Chris Callison-Burch of the University of Pennsylvania. This dataset contains over 20 million sentence pairs in French and English. He built the dataset in a really clever way: by crawling millions of Canadian web pages (which are often multilingual) and then using a set of simple heuristics to transform URLs of French content onto URLs pointing to the same content in English.\n",
    "\n",
    "\n",
    "本书中使用的大多数数据集花费了创作者大量的工作来构建。例如，在本书的后面，我们将向你展示如何创建一个可以在法语和英语之间翻译的模型。这方面的关键输入是宾夕法尼亚大学的Chris Callison-Burch教授在2009年准备的法语/英语平行文本语料库。该数据集包含2000万多个法语和英语句子对。他以一种非常聪明的方式构建了数据集: 通过抓取数百万个加拿大网页 (通常是多语言的) 然后使用一组简单的启发式方法将法语内容的网址转换为指向相同英语内容的网址。\n",
    "\n",
    "As you look at datasets throughout this book, think about where they might have come from, and how they might have been curated. Then think about what kinds of interesting datasets you could create for your own projects. (We’ll even take you step by step through the process of creating your own image dataset soon.)\n",
    "\n",
    "\n",
    "当你在本书中查看数据集时，想想它们可能来自哪里，以及它们可能是如何被策划的。然后想想你可以为自己的项目创建什么样的有趣数据集。(我们甚至会带你逐步完成创建自己的图像数据集的过程。)\n",
    "\n",
    "fast.ai has spent a lot of time creating cut-down versions of popular datasets that are specially designed to support rapid prototyping and experimentation, and to be easier to learn with. In this book we will often start by using one of the cut-down versions and later scale up to the full-size version (just as we're doing in this chapter!). In fact, this is how the world’s top practitioners do their modeling in practice; they do most of their experimentation and prototyping with subsets of their data, and only use the full dataset when they have a good understanding of what they have to do.\n",
    "\n",
    "fast.ai花了大量时间创建流行数据集的缩减版本，这些数据集是专门设计用来支持快速原型制作和实验的，并且更容易学习。在这本书里，我们通常会从使用一个精简版本开始，然后扩展到全尺寸版本 (就像我们在本章中所做的那样!)。实际上，这是世界顶级从业人员在实践中进行建模的方式；他们会使用数据的子集进行大部分的实验和制作原型，并且只有在充分了解自己的工作后才使用完整的数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End sidebar\n",
    "\n",
    "# 结束侧边栏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the models we trained showed a training and validation loss. A good validation set is one of the most important pieces of the training process. Let's see why and learn how to create one.\n",
    "\n",
    "我们训练的每个模型都显示了训练和验证损失。一个好的验证集是训练过程中最重要的部分之一。让我们看看为什么，并学习如何创建一个验证集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Sets and Test Sets\n",
    "\n",
    "## 验证集和测试集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've discussed, the goal of a model is to make predictions about data. But the model training process is fundamentally dumb. If we trained a model with all our data, and then evaluated the model using that same data, we would not be able to tell how well our model can perform on data it hasn’t seen. Without this very valuable piece of information to guide us in training our model, there is a very good chance it would become good at making predictions about that data but would perform poorly on new data.\n",
    "\n",
    "\n",
    "正如我们所讨论的，模型的目标是对数据进行预测。但是模型训练过程从根本上来说是愚蠢的。如果我们用所有数据训练一个模型，然后用相同的数据评估这个模型，我们就无法判断我们的模型在它没有见过的数据上的表现如何。如果没有这条非常有价值的信息来指导我们训练模型，它很有可能会擅长对这些数据进行预测，但在新数据上表现不佳。\n",
    "\n",
    "To avoid this, our first step was to split our dataset into two sets: the *training set* (which our model sees in training) and the *validation set*, also known as the *development set* (which is used only for evaluation). This lets us test that the model learns lessons from the training data that generalize to new data, the validation data.\n",
    "\n",
    "\n",
    "为了避免这种情况，我们的第一步是将数据集分成两个集合: *训练集* (我们的模型在训练中遇到) 和 *验证集* ，也称为 *开发集* (仅用于评估)。这使我们可以测试模型是否从训练数据中吸取了教训，这些数据概括为新数据，即验证数据。\n",
    "\n",
    "One way to understand this situation is that, in a sense, we don't want our model to get good results by \"cheating.\" If it makes an accurate prediction for a data item, that should be because it has learned characteristics of that kind of item, and not because the model has been shaped by *actually having seen that particular item*.\n",
    "\n",
    "\n",
    "理解这种情况的一种方法是，从某种意义上说，我们不希望我们的模型通过 “作弊” 获得好的结果。如果它对数据项进行了准确的预测，那应该是因为它已经了解了该类数据项的特征，并不是因为模型是由 *实际上已经见过了那个特定的项目* 塑造的。\n",
    "\n",
    "Splitting off our validation data means our model never sees it in training and so is completely untainted by it, and is not cheating in any way. Right?\n",
    "\n",
    "\n",
    "分离我们的验证数据意味着我们的模型从未在训练中遇到过它，因此完全不受其影响，并且不会以任何方式作弊。对吧？\n",
    "\n",
    "In fact, not necessarily. The situation is more subtle. This is because in realistic scenarios we rarely build a model just by training its weight parameters once. Instead, we are likely to explore many versions of a model through various modeling choices regarding network architecture, learning rates, data augmentation strategies, and other factors we will discuss in upcoming chapters. Many of these choices can be described as choices of *hyperparameters*. The word reflects that they are parameters about parameters, since they are the higher-level choices that govern the meaning of the weight parameters.\n",
    "\n",
    "事实上，不一定。这个情况更加微妙。这是因为在现实场景中，我们很少仅仅通过训练一次权重参数来构建模型。相反，我们可能会通过关于网络架构、学习率、数据增强策略和我们将在接下来的章节中讨论的其他因素的各种建模选择来探索模型的许多版本。其中许多选择可以描述为 *超参数* 的选择。这个词反映了它们是关于参数的参数，因为它们是控制权重参数含义的更高级别的选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that even though the ordinary training process is only looking at predictions on the training data when it learns values for the weight parameters, the same is not true of us. We, as modelers, are evaluating the model by looking at predictions on the validation data when we decide to explore new hyperparameter values! So subsequent versions of the model are, indirectly, shaped by us having seen the validation data. Just as the automatic training process is in danger of overfitting the training data, we are in danger of overfitting the validation data through human trial and error and exploration.\n",
    "\n",
    "\n",
    "问题在于，即使普通的训练过程在学习权重参数的值时仅查看训练数据的预测，但对我们而言并非如此。作为建模者，当我们决定探索新的超参数值时，我们正在通过查看验证数据的预测来评估模型!因此，模型的后续版本是由我们遇到的验证数据而间接形成的。正如自动训练过程存在过拟合训练数据的危险一样，我们也存在通过人工试错和探索而产生过拟合验证数据的危险。\n",
    "\n",
    "The solution to this conundrum is to introduce another level of even more highly reserved data, the *test set*. Just as we hold back the validation data from the training process, we must hold back the test set data even from ourselves. It cannot be used to improve the model; it can only be used to evaluate the model at the very end of our efforts. In effect, we define a hierarchy of cuts of our data, based on how fully we want to hide it from training and modeling processes: training data is fully exposed, the validation data is less exposed, and test data is totally hidden. This hierarchy parallels the different kinds of modeling and evaluation processes themselves—the automatic training process with back propagation, the more manual process of trying different hyper-parameters between training sessions, and the assessment of our final result.\n",
    "\n",
    "\n",
    "解决这个难题的方案是引入另一个更高度数据保留的级别，即 *测试集*。正如我们保留训练过程中的验证数据一样，我们也必须保留自己的测试数据。它不能用来改进模型; 它只能在我们工作的的最后阶段用于评估模型。实际上，我们根据要在训练和建模过程中隐藏数据的程度来定义数据分割的层次结构: 训练数据完全暴露，验证数据暴露较少，测试数据完全隐藏。这种层次结构与不同类型的建模和评估过程本身相似 -- 反向传播的自动训练过程，在训练课程之间尝试不同超参数的更多手动过程，以及对我们最终结果的评估。\n",
    "\n",
    "The test and validation sets should have enough data to ensure that you get a good estimate of your accuracy. If you're creating a cat detector, for instance, you generally want at least 30 cats in your validation set. That means that if you have a dataset with thousands of items, using the default 20% validation set size may be more than you need. On the other hand, if you have lots of data, using some of it for validation probably doesn't have any downsides.\n",
    "\n",
    "\n",
    "测试和验证集应该有足够的数据来确保你对准确性有一个很好的估计。例如，如果要创建猫检测器，则通常需要在验证集中至少包含30只猫。这意味着，如果你有一个包含数千个项目的数据集，则使用默认的大约超出你需要的20%的验证集。另一方面，如果你有很多数据，使用其中一些进行验证则可能没有任何弊端。\n",
    "\n",
    "Having two levels of \"reserved data\"—a validation set and a test set, with one level representing data that you are virtually hiding from yourself—may seem a bit extreme. But the reason it is often necessary is because models tend to gravitate toward the simplest way to do good predictions (memorization), and we as fallible humans tend to gravitate toward fooling ourselves about how well our models are performing. The discipline of the test set helps us keep ourselves intellectually honest. That doesn't mean we *always* need a separate test set—if you have very little data, you may need to just have a validation set—but generally it's best to use one if at all possible.\n",
    "\n",
    "\n",
    "有两个级别的 “保留数据”-- 验证集和测试集，其中一个级别代表你实际上对自己隐藏的数据-可能看起来有点极端。但是经常需要这样做的原因是，模型倾向于采用最简单的方法来进行良好的预测（记忆），而我们，作为易犯错误的人类，则倾向于使自己迷惑我们的模型表现如何。测试集的约束可以帮助我们保持理智上的诚实。这并不意味着我们 *总是* 需要一个单独的测试集 -- 如果你的数据很少，你可能只需要有一个验证集 -- 但是一般来说，如果可能的话，最好使用一个。\n",
    "\n",
    "This same discipline can be critical if you intend to hire a third party to perform modeling work on your behalf. A third party might not understand your requirements accurately, or their incentives might even encourage them to misunderstand them. A good test set can greatly mitigate these risks and let you evaluate whether their work solves your actual problem.\n",
    "\n",
    "\n",
    "如果你打算雇佣第三方代表你执行建模工作，同样的约束可能是至关重要的。第三方可能无法准确理解您的要求，或者他们的动机甚至会鼓励他们误解这些要求。一个好的测试集可以大大减轻这些风险，让你评估他们的工作是否解决了你的实际问题。\n",
    "\n",
    "To put it bluntly, if you're a senior decision maker in your organization (or you're advising senior decision makers), the most important takeaway is this: if you ensure that you really understand what test and validation sets are and why they're important, then you'll avoid the single biggest source of failures we've seen when organizations decide to use AI. For instance, if you're considering bringing in an external vendor or service, make sure that you hold out some test data that the vendor *never gets to see*. Then *you* check their model on your test data, using a metric that *you* choose based on what actually matters to you in practice, and *you* decide what level of performance is adequate. (It's also a good idea for you to try out some simple baseline yourself, so you know what a really simple model can achieve. Often it'll turn out that your simple model performs just as well as one produced by an external \"expert\"!)\n",
    "\n",
    "坦率地说，如果你是组织中的高级决策者 (或者为高级决策者提供建议)，那么最重要的收获是: 如果你确保你真正理解什么是测试和验证集以及它们的重要性，那么，当组织决定使用AI时，你将避免我们看到的最大失败来源。例如，如果你正在考虑引进一个外部供应商或服务，确保你保存了一些供应商 *永远遇不到* 的测试数据。然后 *你* 根据自己在实践中实际要选择的指标，在测试数据上检查其模型，然后确定适当的性能水平。(自己尝试一些简单的基准也是一个好主意，这样你就知道一个真正简单的模型可以实现什么。通常情况下，你会发现你的简单模型的性能与外部 “专家” 制作的模型一样好!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Judgment in Defining Test Sets\n",
    "\n",
    "# 在定义测试集时使用判断"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do a good job of defining a validation set (and possibly a test set), you will sometimes want to do more than just randomly grab a fraction of your original dataset. Remember: a key property of the validation and test sets is that they must be representative of the new data you will see in the future. This may sound like an impossible order! By definition, you haven’t seen this data yet. But you usually still do know some things.\n",
    "\n",
    "\n",
    "为了做好定义验证集 (可能还有测试集) 的工作，你有时需要做的不仅仅是随机抓取原始数据集的一小部分。记住: 验证和测试集的一个关键属性是它们必须代表你将来会看到的新数据。这可能听起来像一个不可能完成的命令!根据定义，你尚未见过此数据。但你通常还是知道一些事情。\n",
    "\n",
    "It's instructive to look at a few example cases. Many of these examples come from predictive modeling competitions on the [Kaggle](https://www.kaggle.com/) platform, which is a good representation of problems and methods you might see in practice.\n",
    "\n",
    "\n",
    "看几个例子会很有启发性。这些例子中的许多来自[Kaggle]( https://www.kaggle.com/ ) 平台，这很好地表示了你在实践中可能遇到的问题和方法。\n",
    "\n",
    "One case might be if you are looking at time series data. For a time series, choosing a random subset of the data will be both too easy (you can look at the data both before and after the dates your are trying to predict) and not representative of most business use cases (where you are using historical data to build a model for use in the future). If your data includes the date and you are building a model to use in the future, you will want to choose a continuous section with the latest dates as your validation set (for instance, the last two weeks or last month of available data).\n",
    "\n",
    "\n",
    "一种情况可能是你正在查看时间序列数据。对于时间序列来说，选择数据的随机子集太容易了 (你可以在尝试预测的日期之前和之后查看数据)，并且不代表大多数业务用例 (你将使用历史数据来构建将来使用的模型)。如果你的数据包括日期，并且你正在构建一个将来使用的模型，则需要选择一个以最新日期的连续部分作为你的验证集 (例如，最近两周或最后一个月的可用数据)。\n",
    "\n",
    "Suppose you want to split the time series data in <<timeseries1>> into training and validation sets.\n",
    "\n",
    "假设你要将 <<timeseries1>> 中的时间序列数据拆分为训练集和验证集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/timeseries1.png\" width=\"400\" id=\"timeseries1\" caption=\"A time series\" alt=\"A serie of values\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random subset is a poor choice (too easy to fill in the gaps, and not indicative of what you'll need in production), as we can see in <<timeseries2>>.\n",
    "\n",
    "随机子集是一个糟糕的选择 (太容易填补空白，并不能说明你在生产中需要什么)，正如我们在 <<timeseries2>> 中看到的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/timeseries2.png\" width=\"400\" id=\"timeseries2\" caption=\"A poor training subset\" alt=\"Random training subset\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, use the earlier data as your training set (and the later data for the validation set), as shown in <<timeseries3>>.\n",
    "\n",
    "相反，请使用较早的数据作为你的训练集 (以及验证集的后续数据)，如 <<timeseries3>> 中所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/timeseries3.png\" width=\"400\" id=\"timeseries3\" caption=\"A good training subset\" alt=\"Training subset using the data up to a certain timestamp\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, Kaggle had a competition to [predict the sales in a chain of Ecuadorian grocery stores](https://www.kaggle.com/c/favorita-grocery-sales-forecasting). Kaggle's training data ran from Jan 1 2013 to Aug 15 2017, and the test data spanned Aug 16 2017 to Aug 31 2017. That way, the competition organizer ensured that entrants were making predictions for a time period that was *in the future*, from the perspective of their model. This is similar to the way quant hedge fund traders do *back-testing* to check whether their models are predictive of future periods, based on past data.\n",
    "\n",
    "例如，Kaggle有一项竞争，以[预测厄瓜多尔连锁杂货店的销售额](https://www.kaggle.com/c/favorita-grocery-sales-forecasting)。Kaggle的训练数据从2013年1月1日到2017年8月15日，测试数据从2017年8月16日到2017年8月31日。通过这种方式，竞赛组织者确保参赛者从他们的模型的角度对 *未来* 的时间段进行预测。这类似于量化对冲基金交易员根据过去的数据进行 *回溯测试*，以根据过去的数据检查其模型是否可预测未来时期的方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second common case is when you can easily anticipate ways the data you will be making predictions for in production may be *qualitatively different* from the data you have to train your model with.\n",
    "\n",
    "\n",
    "第二个常见的情况是，你可以很容易地预测你将在生产中预测的数据可能与你必须训练模型的数据有*质量上的不同*。\n",
    "\n",
    "In the Kaggle [distracted driver competition](https://www.kaggle.com/c/state-farm-distracted-driver-detection), the independent variables are pictures of drivers at the wheel of a car, and the dependent variables are categories such as texting, eating, or safely looking ahead. Lots of pictures are of the same drivers in different positions, as we can see in <<img_driver>>. If you were an insurance company building a model from this data, note that you would be most interested in how the model performs on drivers it hasn't seen before (since you would likely have training data only for a small group of people). In recognition of this, the test data for the competition consists of images of people that don't appear in the training set.\n",
    "\n",
    "在Kaggle的[分心的司机比赛](https://www.kaggle.com/c/state-farm-distracted-driver-detection)中，自变量是开车司机的照片，因变量是诸如发短信、进食或安全向前视之类的类别。正如我们在 <img_driver>> 中看到的，很多照片是相同的驾驶员在不同的位置。如果你是一家保险公司，根据这些数据建立一个模型，请注意，你将对该模型在以前从未见过的司机上的性能表现最感兴趣 (因为你可能只有一小部分人的训练数据)。考虑到这一点，比赛的测试数据由未出现在训练集中的人的图像组成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/driver.PNG\" width=\"600\" id=\"img_driver\" caption=\"Two pictures from the training data\" alt=\"Two pictures from the training data, showing the same driver\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you put one of the images in <<img_driver>> in your training set and one in the validation set, your model will have an easy time making a prediction for the one in the validation set, so it will seem to be performing better than it would on new people. Another perspective is that if you used all the people in training your model, your model might be overfitting to particularities of those specific people, and not just learning the states (texting, eating, etc.).\n",
    "\n",
    "\n",
    "如果你将其中一个图像放在训练集中的 <<img_driver>> 中，并将一个图像放在验证集中，你的模型将很容易对验证集中的模型进行预测，因此它似乎比在新人身上表现得更好。另一个观点是，如果你用所有的人来训练你的模型，你的模型可能会过拟合那些特定的人的特殊性，而不仅仅是学习状态 (发短信、吃饭、等)。\n",
    "\n",
    "A similar dynamic was at work in the [Kaggle fisheries competition](https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring) to identify the species of fish caught by fishing boats in order to reduce illegal fishing of endangered populations.  The test set consisted of boats that didn't appear in the training data.  This means that you'd want your validation set to include boats that are not in the training set.\n",
    "\n",
    "\n",
    "在 [Kaggle渔业竞赛](https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring) 中也有类似的动态，用以识别渔船捕获的鱼类种类，以减少濒危种群的非法捕捞。测试集由未出现在训练数据中的船只组成。这意味着你希望你的验证集包括不在训练集中的船只。\n",
    "\n",
    "Sometimes it may not be clear how your validation data will differ.  For instance, for a problem using satellite imagery, you'd need to gather more information on whether the training set just contained certain geographic locations, or if it came from geographically scattered data.\n",
    "\n",
    "有时可能不清楚你的验证数据会有什么不同。例如，对于使用卫星图像的问题，你需要收集有关训练集是否仅包含某些地理位置或它是否来自地理位置分散的数据的更多信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have gotten a taste of how to build a model, you can decide what you want to dig into next.\n",
    "\n",
    "既然你已经了解了如何构建模型，那么可以决定接下来你想深入研究的内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A _Choose Your Own Adventure_ moment\n",
    "\n",
    "## _选择你自己的冒险_ 的时刻"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to learn more about how to use deep learning models in practice, including how to identify and fix errors, create a real working web application, and avoid your model causing unexpected harm to your organization or society more generally, then keep reading the next two chapters. If you would like to start learning the foundations of how deep learning works under the hood, skip to <<chapter_mnist_basics>>. (Did you ever read _Choose Your Own Adventure_ books as a kid? Well, this is kind of like that… except with more deep learning than that book series contained.)\n",
    "\n",
    "\n",
    "如果你想了解更多关于如何在实践中使用深度学习模型的信息，包括如何识别和修复错误，请创建一个真正工作的web应用程序，并避免你的模型对你的组织或更普遍地对社会造成意外伤害，然后继续阅读接下来的两章。如果你想开始学习深度学习如何在幕后工作的基础，请跳至 <<chapter_mnist_basics>>。(你小时候读过 _选择你自己的冒险_ 书籍吗？嗯，这有点类似…… 除了比那个系列的书籍包含更多的深度学习内容。)\n",
    "\n",
    "You will need to read all these chapters to progress further in the book, but it is totally up to you which order you read them in. They don't depend on each other. If you skip ahead to <<chapter_mnist_basics>>, we will remind you at the end to come back and read the chapters you skipped over before you go any further.\n",
    "\n",
    "你需要阅读所有这些章节才能在书中进一步学习，但完全由你决定阅读它们的顺序。他们并不互相依赖。如果你提前跳到 <<chapter_mnist_basics>>，我们会在最后提醒你返回并阅读你跳过的章节，然后再继续。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questionnaire\n",
    "\n",
    "## 问卷调查"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be hard to know in pages and pages of prose what the key things are that you really need to focus on and remember. So, we've prepared a list of questions and suggested steps to complete at the end of each chapter. All the answers are in the text of the chapter, so if you're not sure about anything here, reread that part of the text and make sure you understand it. Answers to all these questions are also available on the [book's website](https://book.fast.ai). You can also visit [the forums](https://forums.fast.ai) if you get stuck to get help from other folks studying this material.\n",
    "\n",
    "在一页又一页的套话中很难知道你真正需要关注和记住的关键是什么。因此，我们准备了一个问题列表，并在每章末尾提出了建议的步骤。所有的答案都在这一章的文本中，所以如果你有任何不确定的内容，请重新阅读正文的那一部分，并确保你理解它。所有这些问题的答案也可以在 [图书网站](https://book.fast.ai) 上找到。如果你遇到困难，并想向其他学习此材料的人寻求帮助，也可以访问 [论坛](https://forums.fast.ai)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Do you need these for deep learning?\n",
    "\n",
    "1.深度学习需要这些吗？\n",
    "\n",
    "   - Lots of math T / F\n",
    "   - Lots of data T / F\n",
    "   - Lots of expensive computers T / F\n",
    "   - A PhD T / F\n",
    "   \n",
    "   \n",
    "   - 许多数学知识      对/错\n",
    "   - 大量的数据        对/错\n",
    "   - 许多昂贵的计算机   对/错\n",
    "   - 博士学位          对/错\n",
    "   \n",
    "1. Name five areas where deep learning is now the best in the world.\n",
    "1. What was the name of the first device that was based on the principle of the artificial neuron?\n",
    "1. Based on the book of the same name, what are the requirements for parallel distributed processing (PDP)?\n",
    "1. What were the two theoretical misunderstandings that held back the field of neural networks?\n",
    "1. What is a GPU?\n",
    "1. Open a notebook and execute a cell containing: `1+1`. What happens?\n",
    "1. Follow through each cell of the stripped version of the notebook for this chapter. Before executing each cell, guess what will happen.\n",
    "1. Complete the Jupyter Notebook online appendix.\n",
    "1. Why is it hard to use a traditional computer program to recognize images in a photo?\n",
    "1. What did Samuel mean by \"weight assignment\"?\n",
    "1. What term do we normally use in deep learning for what Samuel called \"weights\"?\n",
    "1. Draw a picture that summarizes Samuel's view of a machine learning model.\n",
    "1. Why is it hard to understand why a deep learning model makes a particular prediction?\n",
    "1. What is the name of the theorem that shows that a neural network can solve any mathematical problem to any level of accuracy?\n",
    "1. What do you need in order to train a model?\n",
    "1. How could a feedback loop impact the rollout of a predictive policing model?\n",
    "1. Do we always have to use 224×224-pixel images with the cat recognition model?\n",
    "1. What is the difference between classification and regression?\n",
    "1. What is a validation set? What is a test set? Why do we need them?\n",
    "1. What will fastai do if you don't provide a validation set?\n",
    "1. Can we always use a random sample for a validation set? Why or why not?\n",
    "1. What is overfitting? Provide an example.\n",
    "1. What is a metric? How does it differ from \"loss\"?\n",
    "1. How can pretrained models help?\n",
    "1. What is the \"head\" of a model?\n",
    "1. What kinds of features do the early layers of a CNN find? How about the later layers?\n",
    "1. Are image models only useful for photos?\n",
    "1. What is an \"architecture\"?\n",
    "1. What is segmentation?\n",
    "1. What is `y_range` used for? When do we need it?\n",
    "1. What are \"hyperparameters\"?\n",
    "1. What's the best way to avoid failures when using AI in an organization?\n",
    "\n",
    "\n",
    "1. 列举当前深度学习在世界上表现最好的五个领域。\n",
    "1. 基于人工神经元原理的第一个设备的名称是什么？\n",
    "1. 基于同名书籍，并行分布式处理（PDP）的要求是什么？\n",
    "1. 阻碍神经网络领域的两个理论误解是什么？\n",
    "1. 什么是显卡？\n",
    "1. 打开一个notebook，执行一个包含: `1+1` 的单元格。会发生什么事？\n",
    "1. 遵循本章笔记本精简版本的每个单元格。在执行每个单元格之前，先猜测会发生什么。\n",
    "1. 完成Jupyter Notebook在线附录。\n",
    "1. 为什么很难使用传统的计算机程序来识别照片中的图像？\n",
    "1. Samuel所说的 “权重分配” 是什么意思？\n",
    "1. 我们在深度学习中通常使用什么术语来表示Samuel所说的 “权重”？\n",
    "1. 画一幅画，总结Samuel对机器学习模型的看法。\n",
    "1. 为什么很难理解为何深度学习模型会做出特定的预测？\n",
    "1. 表明神经网络可以解决任何数学问题到任何精度水平的定理的名称是什么？\n",
    "1. 为了训练一个模型，你需要什么？\n",
    "1. 反馈回路如何影响预测性警务模型的推出？\n",
    "1. 在猫识别模型中，我们是否总是需要使用224×224像素的图像？\n",
    "1. 分类和回归的区别是什么？\n",
    "1. 什么是验证集？什么是测试集？我们为什么需要它们？\n",
    "1. 如果你不提供验证集，fastai会怎么做？\n",
    "1. 我们可以始终使用随机样本作为验证集吗？ 为什么或者为什么不？\n",
    "1. 什么是过拟合？举一个例子。\n",
    "1. 什么是评估指标？它与 “损失” 有什么不同？\n",
    "1. 预训练模型如何提供帮助？\n",
    "1. 什么是模型的 “头”？\n",
    "1. 卷积神经网络前面的层会发现哪些功能？后面的层呢？\n",
    "1. 图像模型只对照片有用吗？\n",
    "1. 什么是 “架构”？\n",
    "1. 什么是分割？\n",
    "1. `y_range`有什么用？我们什么时候需要它？\n",
    "1. 什么是 “超参数”？\n",
    "1. 在组织中使用AI时，避免失败的最佳方法是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Research\n",
    "\n",
    "# 进一步研究"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each chapter also has a \"Further Research\" section that poses questions that aren't fully answered in the text, or gives more advanced assignments. Answers to these questions aren't on the book's website; you'll need to do your own research!\n",
    "\n",
    "每章还有一个 “进一步研究” 部分，提出文本中没有完全回答的问题，或者给出更高级的任务。这些问题的答案不在这本书的网站上; 你需要自己做研究!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why is a GPU useful for deep learning? How is a CPU different, and why is it less effective for deep learning?\n",
    "1. Try to think of three areas where feedback loops might impact the use of machine learning. See if you can find documented examples of that happening in practice.\n",
    "\n",
    "\n",
    "1. 为什么显卡对深度学习有用？显卡会造成什么不同，为什么它对深度学习的影响较低？\n",
    "1. 尝试考虑反馈循环可能影响机器学习使用的三个方面。看看你是否能找到实践中发生的记录在案的例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
