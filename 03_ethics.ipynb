{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "!pip install -Uqq fastbook\n",
    "import fastbook\n",
    "fastbook.setup_book()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[[chapter_ethics]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ethics\n",
    "\n",
    "# 数据伦理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sidebar: Acknowledgement: Dr. Rachel Thomas\n",
    "\n",
    "### 侧边栏: 感谢: Rachel Thomas博士"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter was co-authored by Dr. Rachel Thomas, the cofounder of fast.ai, and founding director of the Center for Applied Data Ethics at the University of San Francisco. It largely follows a subset of the syllabus she developed for the [Introduction to Data Ethics](https://ethics.fast.ai) course.\n",
    "\n",
    "本章由fast.ai的联合创始人、旧金山大学应用数据伦理中心的创始主任Rachel Thomas博士共同撰写。本章在很大程度上遵循了她为 [数据伦理导论](https://ethics.fast.ai) 课程开发的教学大纲的子集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End sidebar\n",
    "\n",
    "### 结束侧边栏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed in Chapters 1 and 2, sometimes machine learning models can go wrong. They can have bugs. They can be presented with data that they haven't seen before, and behave in ways we don't expect. Or they could work exactly as designed, but be used for something that we would much prefer they were never, ever used for.\n",
    "\n",
    "正如我们在第1章和第2章中讨论的那样，有时机器学习模型可能会出错。他们可能有bug。可以向它们显示以前从未见过的数据，并以我们意想不到的方式表现。或者它们可以完全按设计工作，但可以用于我们更希望它们从未使用过的东西。\n",
    "\n",
    "Because deep learning is such a powerful tool and can be used for so many things, it becomes particularly important that we consider the consequences of our choices. The philosophical study of *ethics* is the study of right and wrong, including how we can define those terms, recognize right and wrong actions, and understand the connection between actions and consequences. The field of *data ethics* has been around for a long time, and there are many academics focused on this field. It is being used to help define policy in many jurisdictions; it is being used in companies big and small to consider how best to ensure good societal outcomes from product development; and it is being used by researchers who want to make sure that the work they are doing is used for good, and not for bad.\n",
    "\n",
    "\n",
    "因为深度学习是一个如此强大的工具，可以用于很多事情，所以考虑我们选择的后果变得尤为重要。*伦理学* 的哲学研究是对与错的研究，包括我们如何定义那些术语，认识对与错的行动，理解行动与后果之间的联系。*数据伦理* 这个领域已经存在很长时间了，有很多学者关注这个领域。它被用来帮助定义许多司法管辖区的政策; 它被用于大大小小的公司，以考虑如何最好地确保产品开发的良好社会成果; 研究人员使用它，希望确保他们正在做的工作是好事，而不是坏事。\n",
    "\n",
    "As a deep learning practitioner, therefore, it is likely that at some point you are going to be put in a situation where you need to consider data ethics. So what is data ethics? It's a subfield of ethics, so let's start there.\n",
    "\n",
    "因此，作为一名深度学习从业者，很可能在某个时候你会陷入需要考虑数据伦理的境地。那么什么是数据伦理呢？这是伦理学的一个分支，所以让我们从这里开始。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> J: At university, philosophy of ethics was my main thing (it would have been the topic of my thesis, if I'd finished it, instead of dropping out to join the real world). Based on the years I spent studying ethics, I can tell you this: no one really agrees on what right and wrong are, whether they exist, how to spot them, which people are good, and which bad, or pretty much anything else. So don't expect too much from the theory! We're going to focus on examples and thought starters here, not theory.\n",
    "\n",
    "> J: 在大学里，伦理哲学是我的主要研究内容 (如果我完成了，它将是我论文的主题，而不是辍学进入现实世界)。根据我花在伦理学上的时间，我可以告诉你: 没有人真正同意什么是对的，什么是错的，它们是否存在，如何发现它们，哪些人是好的，哪些人不好，或者几乎其他什么。所以不要对理论期望过高！我们将在这里重点关注示例和思想开创者，而不是理论。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In answering the question [\"What Is Ethics\"](https://www.scu.edu/ethics/ethics-resources/ethical-decision-making/what-is-ethics/), The Markkula Center for Applied Ethics says that the term refers to:\n",
    "\n",
    "\n",
    "在回答 [“什么是伦理”](https://www.scu.edu/ethics/ethics-resources/ethical-decision-making/what-is-ethics/)的问题时，马库拉应用伦理中心表示，该术语指的是:\n",
    "\n",
    "- Well-founded standards of right and wrong that prescribe what humans ought to do\n",
    "- The study and development of one's ethical standards.\n",
    "\n",
    "\n",
    "- 正确的对与错标准规定了人类应该做什么\n",
    "- 研究和发展个人伦理标准。\n",
    "\n",
    "There is no list of right answers. There is no list of do and don't. Ethics is complicated, and context-dependent. It involves the perspectives of many stakeholders. Ethics is a muscle that you have to develop and practice. In this chapter, our goal is to provide some signposts to help you on that journey.\n",
    "\n",
    "\n",
    "没有正确答案的列表。没有可做和不可做的清单。伦理是复杂的，并且依赖于上下文环境。它涉及许多利益攸关方的观点。伦理是你必须发展和实践的力量。在本章中，我们的目标是提供一些路标来帮助你踏上旅程。\n",
    "\n",
    "Spotting ethical issues is best to do as part of a collaborative team. This is the only way you can really incorporate different perspectives. Different people's backgrounds will help them to see things which may not be obvious to you. Working with a team is helpful for many \"muscle-building\" activities, including this one.\n",
    "\n",
    "\n",
    "最好将伦理问题作为合作团队的一部分来做。这是你真正融入不同观点的唯一方法。不同人的背景会帮助他们看到对你来说可能不明显的东西。与团队合作有助于许多 “增肌” 活动，包括这项活动。\n",
    "\n",
    "This chapter is certainly not the only part of the book where we talk about data ethics, but it's good to have a place where we focus on it for a while. To get oriented, it's perhaps easiest to look at a few examples. So, we picked out three that we think illustrate effectively some of the key topics.\n",
    "\n",
    "本当然不是本书中讨论数据伦理的唯一部分，但是最好有一段时间让我们专注于此。为了找明方向，看几个例子也许最容易。所以，我们挑选了三个我们认为可以有效说明一些关键主题的东西。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Examples for Data Ethics\n",
    "\n",
    "## 数据伦理的主要示例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to start with three specific examples that illustrate three common ethical issues in tech:\n",
    "\n",
    "\n",
    "我们将从三个具体的例子开始，说明技术中的三个常见伦理问题:\n",
    "\n",
    "1.  *Recourse processes*—Arkansas's buggy healthcare algorithms left patients stranded.\n",
    "2.  *Feedback loops*—YouTube's recommendation system helped unleash a conspiracy theory boom.\n",
    "3.  *Bias*—When a traditionally African-American name is searched for on Google, it displays ads for criminal background checks.\n",
    "\n",
    "\n",
    "1. *求助程序* - 阿肯色州的错误医疗算法让患者陷入困境。\n",
    "2. *反馈循环* - YouTube的推荐系统帮助释放了阴谋论热潮。\n",
    "3. *偏见* - 当在谷歌上搜索传统上属于非裔美国人的名字时，它会显示用于犯罪背景调查的广告。\n",
    "\n",
    "In fact, for every concept that we introduce in this chapter, we are going to provide at least one specific example. For each one, think about what you could have done in this situation, and what kinds of obstructions there might have been to you getting that done. How would you deal with them? What would you look out for?\n",
    "\n",
    "事实上，对于我们在本章中介绍的每个概念，我们将至少提供一个具体的例子。对于每个例子，请考虑一下在这种情况下你可以做些什么，以及你可能会遇到什么样的障碍。你将如何处理它们？你所寻求的是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bugs and Recourse: Buggy Algorithm Used for Healthcare Benefits\n",
    "\n",
    "### Bug和求援: 错误算法被应用于医疗保健福利"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Verge investigated software used in over half of the US states to determine how much healthcare people receive, and documented their findings in the article [\"What Happens When an Algorithm Cuts Your Healthcare\"](https://www.theverge.com/2018/3/21/17144260/healthcare-medicaid-algorithm-arkansas-cerebral-palsy). After implementation of the algorithm in Arkansas, hundreds of people (many with severe disabilities) had their healthcare drastically cut. For instance, Tammy Dobbs, a woman with cerebral palsy who needs an aid to help her to get out of bed, to go to the bathroom, to get food, and more, had her hours of help suddenly reduced by 20 hours a week. She couldn’t get any explanation for why her healthcare was cut. Eventually, a court case revealed that there were mistakes in the software implementation of the algorithm, negatively impacting people with diabetes or cerebral palsy. However, Dobbs and many other people reliant on these healthcare benefits live in fear that their benefits could again be cut suddenly and inexplicably.\n",
    "\n",
    "前沿网调查了在美国超过一半的州使用的软件，以确定人们获得了多少医疗保健，并在文章 [“当算法削减你的医疗保健时会发生什么”](https://www.theverge.com/2018/3/21/17144260/healthcare-medicaid-algorithm-arkansas-cerebral-palsy)。在阿肯色州实施该算法后，数百人 (许多患有严重残疾的人) 的医疗保健大幅削减。例如，Tammy Dobbs，一个患有脑瘫的女人，她需要人来帮助她起床、上厕所、获取食物等等，她的帮助时间突然每周减少了20小时。对于为什么她的医疗保健被削减了，她没有获得任何解释。最终，一个法庭案例显示该算法的软件实施存在错误，对糖尿病或脑瘫患者产生了负面影响。然而，Dobbs和许多其他依赖这些医疗保健福利的人生活在担心之中，他们的福利可能会突然又莫名其妙地被削减。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback Loops: YouTube's Recommendation System\n",
    "\n",
    "### 反馈循环: 视频网站油管的推荐系统"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feedback loops can occur when your model is controlling the next round of data you get. The data that is returned quickly becomes flawed by the software itself.\n",
    "\n",
    "\n",
    "当你的模型控制你获得的下一轮数据时，可能会出现反馈循环。快速返回的数据已被软件本身破坏。\n",
    "\n",
    "For instance, YouTube has 1.9 billion users, who watch over 1 billion hours of YouTube videos a day. Its recommendation algorithm (built by Google), which was designed to optimize watch time, is responsible for around 70% of the content that is watched. But there was a problem: it led to out-of-control feedback loops, leading the *New York Times* to run the headline [\"YouTube Unleashed a Conspiracy Theory Boom. Can It Be Contained?\"](https://www.nytimes.com/2019/02/19/technology/youtube-conspiracy-stars.html). Ostensibly recommendation systems are predicting what content people will like, but they also have a lot of power in determining what content people even see.\n",
    "\n",
    "例如，油管拥有19亿用户，每天观看超过10亿小时的视频。它的推荐算法 (由谷歌构建) 旨在优化观看时间，负责大约70% 的观看内容。但是有一个问题: 它导致了失控的反馈循环，因此《纽约时报》刊登了标题为 [“视频网站油管引发了阴谋论热潮。可以遏制它？”](https://www.nytimes.com/2019/02/19/technology/youtube-conspiracy-stars.html)的文章。表面上的推荐系统正在预测人们会喜欢什么内容，但是它们在决定人们甚至看到什么内容方面也具有强大的功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias: Professor Lantanya Sweeney \"Arrested\"\n",
    "\n",
    "### 偏见: Lantanya Sweeney教授 “被捕”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dr. Latanya Sweeney is a professor at Harvard and director of the university's data privacy lab. In the paper [\"Discrimination in Online Ad Delivery\"](https://arxiv.org/abs/1301.6822) (see <<lantanya_arrested>>) she describes her discovery that Googling her name resulted in advertisements saying \"Latanya Sweeney, arrested?\" even though she is the only known Latanya Sweeney and has never been arrested. However when she Googled other names, such as \"Kirsten Lindquist,\" she got more neutral ads, even though Kirsten Lindquist has been arrested three times.\n",
    "\n",
    "Latanya Sweeney博士是哈佛大学的教授，也是该大学数据隐私实验室的主任。在论文 [“网络广告投放中的歧视”](https://arxiv.org/abs/1301.6822) (见 <<lantanya_arrested>>) 她描述了自己的发现，谷歌搜索她的名字会出现广告说 “Latanya Sweeney被捕了？“ 尽管她是唯一已知的Latanya Sweeney，从未被捕。然而，当她在谷歌上搜索其他名字时，比如 “Kirsten Lindquist”，她得到了更多中立的广告，尽管Kirsten Lindquist已经被捕三次。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image1.png\" id=\"lantanya_arrested\" caption=\"Google search showing ads about Professor Lantanya Sweeney's arrest record\" alt=\"Screenshot of google search showing ads about Professor Lantanya Sweeney's arrest record\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being a computer scientist, she studied this systematically, and looked at over 2000 names. She found a clear pattern where historically Black names received advertisements suggesting that the person had a criminal record, whereas, white names had more neutral advertisements.\n",
    "\n",
    "\n",
    "作为一名计算机科学家，她系统地研究了这一点，并查看了2000多个名字。她发现了一个清晰的模式，历史上有黑人血统的名字收到的广告暗示该人有犯罪记录，而白人名字则带有更多中立的广告。\n",
    "\n",
    "This is an example of bias. It can make a big difference to people's lives—for instance, if a job applicant is Googled it may appear that they have a criminal record when they do not.\n",
    "\n",
    "这是偏见的一个例子。它会对人们的生活产生很大的影响 -- 例如，如果一个求职者被谷歌搜索，他们可能会有犯罪记录，然而实际上他们并没有。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Does This Matter?\n",
    "\n",
    "### 这有什么关系呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One very natural reaction to considering these issues is: \"So what? What's that got to do with me? I'm a data scientist, not a politician. I'm not one of the senior executives at my company who make the decisions about what we do. I'm just trying to build the most predictive model I can.\"\n",
    "\n",
    "\n",
    "考虑这些问题的一个非常自然的反应是: “那又怎样？这和我有什么关系？我是数据科学家，不是政治家。我不是我公司的高级管理人员之一，他们决定我们做什么。我只是想建立我能做到的最具预测性的模型。”\n",
    "\n",
    "These are very reasonable questions. But we're going to try to convince you that the answer is that everybody who is training models absolutely needs to consider how their models will be used, and consider how to best ensure that they are used as positively as possible. There are things you can do. And if you don't do them, then things can go pretty badly.\n",
    "\n",
    "\n",
    "这些是非常合理的问题。但是我们将试图说服你，答案是每个训练模型的人都绝对需要考虑如何使用他们的模型，并考虑如何最好地确保尽可能积极地使用它们。有些事情你可以做。如果你不做，事情会变得很糟糕。\n",
    "\n",
    "One particularly hideous example of what happens when technologists focus on technology at all costs is the story of IBM and Nazi Germany. In 2001, a Swiss judge ruled that it was not unreasonable \"to deduce that IBM's technical assistance facilitated the tasks of the Nazis in the commission of their crimes against humanity, acts also involving accountancy and classification by IBM machines and utilized in the concentration camps themselves.\"\n",
    "\n",
    "\n",
    "当技术人员不惜一切代价关注技术时，一个特别可怕的例子是IBM和纳粹德国的故事。2001，一名瑞士法官裁定，“推断出IBM的技术援助助长了了纳粹犯下反人类罪的任务，该行为还涉及IBM机器的会计和分类，并在集中营中使用。”\n",
    "\n",
    "IBM, you see, supplied the Nazis with data tabulation products necessary to track the extermination of Jews and other groups on a massive scale. This was driven from the top of the company, with marketing to Hitler and his leadership team. Company President Thomas Watson personally approved the 1939 release of special IBM alphabetizing machines to help organize the deportation of Polish Jews. Pictured in <<meeting>> is Adolf Hitler (far left) meeting with IBM CEO Tom Watson Sr. (second from left), shortly before Hitler awarded Watson a special “Service to the Reich” medal in 1937.\n",
    "\n",
    "你看，IBM向纳粹提供了大规模跟踪犹太人和其他团体灭绝所需的数据列表产品。这是由公司高层推动的，向希特勒和他的领导团队进行营销。公司总裁Thomas Watson亲自批准了1939年发布的特殊IBM字母拼写机，以帮助组织对波兰犹太人驱逐出境。在图片《会议》中，Adolf Hitler（最左）与IBM首席执行官Tom Watson Sr.（左二）会面，不久之后，希特勒于1937年授予Watson特殊的“为帝国服务”勋章。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image2.png\" id=\"meeting\" caption=\"IBM CEO Tom Watson Sr. meeting with Adolf Hitler\" alt=\"A picture of IBM CEO Tom Watson Sr. meeting with Adolf Hitler\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this was not an isolated incident—the organization's involvement was extensive. IBM and its subsidiaries provided regular training and maintenance onsite at the concentration camps: printing off cards, configuring machines, and repairing them as they broke frequently. IBM set up categorizations on its punch card system for the way that each person was killed, which group they were assigned to, and the logistical information necessary to track them through the vast Holocaust system. IBM's code for Jews in the concentration camps  was 8: some 6,000,000 were killed. Its code for Romanis was 12 (they were labeled by the Nazis as \"asocials,\" with over 300,000 killed in the *Zigeunerlager*, or “Gypsy camp”). General executions were coded as 4, death in the gas chambers as 6.\n",
    "\n",
    "但这不是一个孤立的事件 -- 该组织的参与非常广泛的。IBM及其子公司在集中营现场提供定期培训和维护: 打印卡片，配置机器，并进行维修，因为它们经常损坏。IBM在其打孔卡系统上设置了分类，以分类每个人被杀死的方式，他们被分配到的人群以及在庞大的大屠杀系统中追踪他们所必需的后勤信息。IBM在集中营中给犹太人的编码是8：大约6 000 000人被杀。罗马人的编码是12（他们被纳粹标记为“反社会”，在“齐格纳拉格”（“ Zigeunerlager”）或“吉普赛难民营”中有30万人被杀）。一般处决被编码为4，毒气室死亡的为6。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image3.jpeg\" id=\"punch_card\" caption=\"A punch card used by IBM in concentration camps\" alt=\"Picture of a punch card used by IBM in concentration camps\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the project managers and engineers and technicians involved were just living their ordinary lives. Caring for their families, going to the church on Sunday, doing their jobs the best they could. Following orders. The marketers were just doing what they could to meet their business development goals. As Edwin Black, author of *IBM and the Holocaust* (Dialog Press) observed: \"To the blind technocrat, the means were more important than the ends. The destruction of the Jewish people became even less important because the invigorating nature of IBM's technical achievement was only heightened by the fantastical profits to be made at a time when bread lines stretched across the world.\"\n",
    "\n",
    "\n",
    "当然，所涉及的项目经理和工程技术人员只是过着他们平凡的生活。照顾家人，周日去教堂，尽最大努力做好工作。服从命令。营销人员只是尽他们所能来实现他们的业务发展目标。正如 *IBM和大屠杀* (对话出版社) 的作者Edwin Black所说: “对盲目的技术官僚来说，手段比目的更重要。犹太人的毁灭变得更加不重要，因为只有在面包生产线遍及全球的时候才能获得可观的利润，才能增强IBM技术成就的活力。\"\n",
    "\n",
    "Step back for a moment and consider: How would you feel if you discovered that you had been part of a system that ended up hurting society? Would you be open to finding out? How can you help make sure this doesn't happen? We have described the most extreme situation here, but there are many negative societal consequences linked to AI and machine learning being observed today, some of which we'll describe in this chapter.\n",
    "\n",
    "\n",
    "退一步考虑一下: 如果你发现自己曾经是最终伤害社会的系统的一部分，你会有什么感觉？你愿意知道吗？你如何才能确保不会发生这种情况？我们在这里描述了最极端的情况，但是如今，我们观察到与人工智能和机器学习相关的许多负面社会后果，我们将在本章中进行描述。\n",
    "\n",
    "It's not just a moral burden, either. Sometimes technologists pay very directly for their actions. For instance, the first person who was jailed as a result of the Volkswagen scandal, where the car company was revealed to have cheated on its diesel emissions tests, was not the manager that oversaw the project, or an executive at the helm of the company. It was one of the engineers, James Liang, who just did what he was told.\n",
    "\n",
    "\n",
    "这也不仅仅是道德负担。有时，技术人员会非常直接地为自己的行为付出代价。例如，因大众汽车丑闻而被监禁的第一人不是汽车公司的负责人，也不是负责该项目的高管，而是其中一名工程师James Liang，只是按照指示做了事情。\n",
    "\n",
    "Of course, it's not all bad—if a project you are involved in turns out to make a huge positive impact on even one person, this is going to make you feel pretty great!\n",
    "\n",
    "\n",
    "当然，这并不全是坏事 -- 如果你参与的一个项目最终对一个人产生了巨大的积极影响，这将会让你感觉非常棒！\n",
    "\n",
    "Okay, so hopefully we have convinced you that you ought to care. But what should you do? As data scientists, we're naturally inclined to focus on making our models better by optimizing some metric or other. But optimizing that metric may not actually lead to better outcomes. And even if it *does* help create better outcomes, it almost certainly won't be the only thing that matters. Consider the pipeline of steps that occurs between the development of a model or an algorithm by a researcher or practitioner, and the point at which this work is actually used to make some decision. This entire pipeline needs to be considered *as a whole* if we're to have a hope of getting the kinds of outcomes we want.\n",
    "\n",
    "\n",
    "好吧，希望我们已经说服你，你应该注意。但是你应该怎么做呢？作为数据科学家，我们自然倾向于通过优化某些指标或其他指标来改进我们的模型。但是优化这个指标实际上可能不会带来更好的结果。即使它确实有助于产生更好的结果，但几乎肯定不是唯一重要的事情。考虑一下在研究人员或从业人员开发模型或算法与实际使用这项工作来做出某些决定的时间点之间发生的步骤。如果我们希望获得想要的结果，则需要将整个管道视为一个“整体”。\n",
    "\n",
    "Normally there is a very long chain from one end to the other. This is especially true if you are a researcher, where you might not even know if your research will ever get used for anything, or if you're involved in data collection, which is even earlier in the pipeline. But no one is better placed to inform everyone involved in this chain about the capabilities, constraints, and details of your work than you are. Although there's no \"silver bullet\" that can ensure your work is used the right way, by getting involved in the process, and asking the right questions, you can at the very least ensure that the right issues are being considered.\n",
    "\n",
    "\n",
    "通常从一端到另一端之间的链很长。尤其是如果你是一名研究人员，你甚至可能不知道你的研究是否会被用于任何事情，或者你是否参与了数据收集，这一点尤其正确，这甚至还处于开发阶段.但是，没有人比你更适合向这个链条中的每个人介绍你的功能，制约和工作细节。尽管没有 “灵丹妙药” 可以确保你的工作得到正确的使用，通过参与该过程并提出正确的问题，你至少可以确保正在考虑正确的问题。。\n",
    "\n",
    "Sometimes, the right response to being asked to do a piece of work is to just say \"no.\" Often, however, the response we hear is, \"If I don’t do it, someone else will.\" But consider this: if you’ve been picked for the job, you’re the best person they’ve found to do it—so if you don’t do it, the best person isn’t working on that project. If the first five people they ask all say no too, even better!\n",
    "\n",
    "有时候，被要求做一件工作的正确反应是说 “不” 然而，我们经常听到的回答是，“如果我不做，别人会做的。\"但是考虑一下: 如果你被选中做这份工作，你是他们找到的最好的人 -- 所以如果你不做，最好的人没有在那个项目上工作。如果他们问的前五个人也都说不，那就更好了！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating Machine Learning with Product Design\n",
    "\n",
    "## 集成机器学习与产品设计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presumably the reason you're doing this work is because you hope it will be used for something. Otherwise, you're just wasting your time. So, let's start with the assumption that your work will end up somewhere. Now, as you are collecting your data and developing your model, you are making lots of decisions. What level of aggregation will you store your data at? What loss function should you use? What validation and training sets should you use? Should you focus on simplicity of implementation, speed of inference, or accuracy of the model? How will your model handle out-of-domain data items? Can it be fine-tuned, or must it be retrained from scratch over time?\n",
    "\n",
    "\n",
    "你做这项工作的原因大概是因为你希望将其用于某些方面。否则，你只是在浪费时间。所以，让我们从假设你的工作会在某个地方结束开始。现在，当你收集数据和开发模型时，你会做出很多决策。你要将数据存储在什么级别的聚合中？你应该使用什么损失函数？你应该使用哪些验证集和训练集？你应该专注于实现的简易性、推断的速度还是模型的准确性？你的模型将如何处理域外数据项？它能被精调吗，还是必须随着时间的推移从头开始重新训练？\n",
    "\n",
    "These are not just algorithm questions. They are data product design questions. But the product managers, executives, judges, journalists, doctors… whoever ends up developing and using the system of which your model is a part will not be well-placed to understand the decisions that you made, let alone change them.\n",
    "\n",
    "\n",
    "这些不仅仅是算法问题。它们是数据产品设计的问题。但是产品经理、行政部门、法官、记者，医生…… 无论是谁最终开发和使用了你的模型所组成的系统，都不会很好地理解你所做的决策，更不用说改变它们了。\n",
    "\n",
    "For instance, two studies found that Amazon’s facial recognition software produced [inaccurate](https://www.nytimes.com/2018/07/26/technology/amazon-aclu-facial-recognition-congress.html) and [racially biased](https://www.theverge.com/2019/1/25/18197137/amazon-rekognition-facial-recognition-bias-race-gender) results. Amazon claimed that the researchers should have changed the default parameters, without explaining how this would have changed the biased results. Furthermore, it turned out that [Amazon was not instructing police departments](https://gizmodo.com/defense-of-amazons-face-recognition-tool-undermined-by-1832238149) that used its software to do this either. There was, presumably, a big distance between the researchers that developed these algorithms and the Amazon documentation staff that wrote the guidelines provided to the police. A lack of tight integration led to serious problems for society at large, the police, and Amazon themselves. It turned out that their system erroneously matched 28 members of congress to criminal mugshots!  (And the Congresspeople wrongly matched to criminal mugshots were disproportionately people of color, as seen in <<congressmen>>.)\n",
    "\n",
    "例如，两项研究发现亚马逊的面部识别软件产生了 [不准确](https://www.nytimes.com/2018/07/26/technology/amazon-aclu-facial-recognition-congress.html)和[种族偏见](https://www.theverge.com/2019/1/25/18197137/amazon-rekognition-facial-recognition-bias-race-gender)的结果。亚马逊声称，研究人员应该改变默认参数，但没有解释这将如何改变有偏见的结果。此外，事实证明 [亚马逊没有指示警察部门](https://gizmodo.com/defense-of-amazons-face-recognition-tool-undermined-by-1832238149)也使用了它的软件来做这件事。据推测，开发这些算法的研究人员和编写向警方提供的指南的亚马逊文档工作人员之间相距甚远。缺乏紧密的整合导致整个社会，警察和亚马逊自身面临严重问题。事实证明，他们的系统错误地将28名国会议员与犯罪照片相匹配！(国会议员错误地与犯罪照片相匹配，他们是不成比例的有色人种，如 <<congressmen>> 所示。)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image4.png\" id=\"congressmen\" caption=\"Congresspeople matched to criminal mugshots by Amazon software\" alt=\"Picture of the congresspeople matched to criminal mugshots by Amazon software, they are disproportionatedly people of color\" width=\"500\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data scientists need to be part of a cross-disciplinary team. And researchers need to work closely with the kinds of people who will end up using their research. Better still is if the domain experts themselves have learned enough to be able to train and debug some models themselves—hopefully there are a few of you reading this book right now!\n",
    "\n",
    "\n",
    "数据科学家需要成为跨学科团队的一员。研究人员需要与那些最终会使用他们的研究的人密切合作。更好的情况是，如果领域专家自己已经学到了足够的知识，能够自己训练和调试一些模型 -- 希望你们中的一些人正在阅读这本书!\n",
    "\n",
    "The modern workplace is a very specialized place. Everybody tends to have well-defined jobs to perform. Especially in large companies, it can be hard to know what all the pieces of the puzzle are. Sometimes companies even intentionally obscure the overall project goals that are being worked on, if they know that their employees are not going to like the answers. This is sometimes done by compartmentalising pieces as much as possible.\n",
    "\n",
    "\n",
    "现代工作场所是一个非常专业化的地方。每个人都倾向于有明确的工作要做。尤其是在大公司，很难知道拼图的所有部分是什么。有时，如果公司知道他们的员工不会喜欢这些答案，他们甚至会故意模糊正在进行的总体项目目标。这有时是通过尽可能多地划分碎片来实现的。\n",
    "\n",
    "In other words, we're not saying that any of this is easy. It's hard. It's really hard. We all have to do our best. And we have often seen that the people who do get involved in the higher-level context of these projects, and attempt to develop cross-disciplinary capabilities and teams, become some of the most important and well rewarded members of their organizations. It's the kind of work that tends to be highly appreciated by senior executives, even if it is sometimes considered rather uncomfortable by middle management.\n",
    "\n",
    "换句话说，我们并不是说这一切都很容易。很难。真的很难。我们都必须尽力而为。我们经常看到，那些参与到这些项目的更高层次环境的人员，并试图发展跨学科能力和团队，成为他们组织中最重要和最有回报的成员。尽管中层管理人员有时认为这很不舒服，但这种工作往往会得到高级管理人员的高度评价。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics in Data Ethics\n",
    "\n",
    "## 数据伦理主题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data ethics is a big field, and we can't cover everything. Instead, we're going to pick a few topics that we think are particularly relevant:\n",
    "\n",
    "\n",
    "数据伦理是一个很大的领域，我们不可能什么都覆盖。相反，我们将选择一些我们认为特别相关的主题:\n",
    "\n",
    "- The need for recourse and accountability\n",
    "- Feedback loops\n",
    "- Bias\n",
    "- Disinformation\n",
    "\n",
    "\n",
    "- 追索权和问责制的必要性\n",
    "- 反馈回路\n",
    "- 偏见\n",
    "- 虚假信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at each in turn.\n",
    "\n",
    "让我们依次看一下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recourse and Accountability\n",
    "\n",
    "### 追索权和问责制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a complex system, it is easy for no one person to feel responsible for outcomes. While this is understandable, it does not lead to good results. In the earlier example of the Arkansas healthcare system in which a bug led to people with cerebral palsy losing access to needed care, the creator of the algorithm blamed government officials, and government officials blamed those who implemented the software. NYU professor [Danah Boyd](https://www.youtube.com/watch?v=NTl0yyPqf3E) described this phenomenon: \"Bureaucracy has often been used to shift or evade responsibility... Today's algorithmic systems are extending bureaucracy.\"\n",
    "\n",
    "\n",
    "在一个复杂的系统中，很容易就会没有人对结果负责。虽然这是可以理解的，但并没有带来好的结果。在阿肯色州医疗保健系统的较早示例中，一个错误导致脑瘫患者无法获得所需的护理，该算法的创建者指责政府官员，政府官员指责那些执行该软件的人。纽约大学教授 [Danah Boyd](https://www.youtube.com/watch?v=NTl0yyPqf3E) 描述了这种现象: “官僚主义经常被用来转移或逃避责任.当今的算法系统正在扩大官僚作风。\"\n",
    "\n",
    "An additional reason why recourse is so necessary is because data often contains errors. Mechanisms for audits and error correction are crucial. A database of suspected gang members maintained by California law enforcement officials was found to be full of errors, including 42 babies who had been added to the database when they were less than 1 year old (28 of whom were marked as “admitting to being gang members”). In this case, there was no process in place for correcting mistakes or removing people once they’d been added. Another example is the US credit report system: in a large-scale study of credit reports by the Federal Trade Commission (FTC) in 2012, it was found that 26% of consumers had at least one mistake in their files, and 5% had errors that could be devastating.  Yet, the process of getting such errors corrected is incredibly slow and opaque. When public radio reporter [Bobby Allyn](https://www.washingtonpost.com/posteverything/wp/2016/09/08/how-the-careless-errors-of-credit-reporting-agencies-are-ruining-peoples-lives/) discovered that he was erroneously listed as having a firearms conviction, it took him \"more than a dozen phone calls, the handiwork of a county court clerk and six weeks to solve the problem. And that was only after I contacted the company’s communications department as a journalist.\"\n",
    "\n",
    "\n",
    "追索权如此必要的另一个原因是数据经常包含错误。审计和纠错机制至关重要。加州执法官员维护的一个涉嫌帮派成员的数据库被发现充满了错误，包括42名不到1岁的婴儿被添加到数据库中(其中28人被标记为 “承认是帮派成员”)。在这种情况下，一旦人们被添加，就没有纠正错误或删除他们的流程。另一个例子是美国的信用报告系统: 在2012年联邦贸易委员会 (FTC) 对信用报告的大规模研究中，发现26%的消费者的档案中至少有一个错误，5%的消费者则具有毁灭性的错误。然而，纠正这些错误的过程非常缓慢和不透明。当公共广播记者 [Bobby Allyn](https://www.washingtonpost.com/posteverything/wp/2016/09/08/how-the-careless-errors-of-credit-reporting-agencies-are-ruining-peoples-lives/) 发现他被错误地定为持枪罪犯，他花了 “十几个电话，一个当地法院书记员的工作和六个星期的时间解决了这个问题。 那只是在我以记者的身份联系公司的公关部门之后。\"\n",
    "\n",
    "As machine learning practitioners, we do not always think of it as our responsibility to understand how our algorithms end up being implemented in practice. But we need to.\n",
    "\n",
    "作为机器学习从业者，我们并不总是认为理解我们的算法最终如何在实践中运行是我们的责任。但我们需要去理解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback Loops\n",
    "\n",
    "### 反馈循环"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explained in <<chapter_intro>> how an algorithm can interact with its enviromnent to create a feedback loop, making predictions that reinforce actions taken in the real world, which lead to predictions even more pronounced in the same direction. \n",
    "As an example, let's again consider YouTube's recommendation system. A couple of years ago the Google team talked about how they had introduced reinforcement learning (closely related to deep learning, but where your loss function represents a result potentially a long time after an action occurs) to improve YouTube's recommendation system. They described how they used an algorithm that made recommendations such that watch time would be optimized.\n",
    "\n",
    "\n",
    "我们在 <<chapter_intro>> 中解释了算法如何与其环境交互以创建反馈循环，做出预测以加强现实世界中采取的行动，这导致在相同方向上的预测更加明显。\n",
    "举个例子，让我们再次考虑YouTube的推荐系统。几年前，谷歌团队讨论了他们是如何引入强化学习的 (与深度学习密切相关，但是你的损失函数代表了采取行动后可能需要很长时间才能产生的结果)，以改善YouTube的推荐系统。他们描述了如何使用提出建议的算法来优化观看时间。\n",
    "\n",
    "However, human beings tend to be drawn to controversial content. This meant that videos about things like conspiracy theories started to get recommended more and more by the recommendation system. Furthermore, it turns out that the kinds of people that are interested in conspiracy theories are also people that watch a lot of online videos! So, they started to get drawn more and more toward YouTube. The increasing number of conspiracy theorists watching videos on YouTube resulted in the algorithm recommending more and more conspiracy theory and other extremist content, which resulted in more extremists watching videos on YouTube, and more people watching YouTube developing extremist views, which led to the algorithm recommending more extremist content... The system was spiraling out of control.\n",
    "\n",
    "\n",
    "然而，人类倾向于被有争议的内容所吸引。这意味着关于阴谋论之类的东西的视频开始被推荐系统推荐得越来越多。此外，事实证明，对阴谋理论感兴趣的人也是观看大量在线视频的人！因此，他们开始越来越受到YouTube的吸引。越来越多的阴谋论者在油管上观看视频，导致算法推荐越来越多的阴谋论和其他极端主义内容，这导致越来越多的极端分子在油管上观看视频，越来越多的人观看油管形成极端主义观点，这导致算法推荐更多极端主义内容...系统失控了。\n",
    "\n",
    "And this phenomenon was not contained to this particular type of content. In June 2019 the *New York Times* published an article on YouTube's recommendation system, titled [\"On YouTube’s Digital Playground, an Open Gate for Pedophiles\"](https://www.nytimes.com/2019/06/03/world/americas/youtube-pedophiles.html). The article started with this chilling story:\n",
    "\n",
    "这种现象并不包含在这种特定类型的内容中。2019年6月，*纽约时报* 在YouTube的推荐系统上发表了一篇文章，标题为 [“在油管的数字游乐场，为恋童癖者打开的大门”](https://www.nytimes.com/2019/06/03/world/americas/youtube-pedophiles.html)。文章从这个令人不寒而栗的故事开始:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> : Christiane C. didn’t think anything of it when her 10-year-old daughter and a friend uploaded a video of themselves playing in a backyard pool… A few days later… the video had thousands of views. Before long, it had ticked up to 400,000... “I saw the video again and I got scared by the number of views,” Christiane said. She had reason to be. YouTube’s automated recommendation system… had begun showing the video to users who watched other videos of prepubescent, partially clothed children, a team of researchers has found.\n",
    "\n",
    "\n",
    "> : 当Christiane C.10岁的女儿和一个朋友上传了一段他们在后院游泳池玩耍的视频时，她没有多想…… 几天后…… 这段视频有数以千计的浏览量。没过多久，它就涨到了400,000...“我又看了一遍视频，我被观看的次数吓到了，” Christiane说。她这样是有理由的。一组研究人员发现，YouTube的自动推荐系统…已开始向观看了青春期，衣着不整的孩子的其他视频的用户显示该视频。\n",
    "\n",
    "> : On its own, each video might be perfectly innocent, a home movie, say, made by a child. Any revealing frames are fleeting and appear accidental. But, grouped together, their shared features become unmistakable.\n",
    "\n",
    ">: 就其本身而言，每一段视频都可能是完全无辜的，比如说，一部由孩子制作的家庭电影。任何坦露的画面都是短暂的，看起来都是偶然的。但是，把它们组合在一起，它们的共同特征变得显而易见。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YouTube's recommendation algorithm had begun curating playlists for pedophiles, picking out innocent home videos that happened to contain prepubescent, partially clothed children. \n",
    "\n",
    "\n",
    "油管的推荐算法已开始整理恋童癖者的播放列表，挑选出恰好包含青春期前，穿着部分衣服的孩子的无辜家庭视频。\n",
    "\n",
    "No one at Google planned to create a system that turned family videos into porn for pedophiles. So what happened?\n",
    "\n",
    "\n",
    "谷歌没人计划创建一个系统，将家庭视频转换为恋童癖者的色情内容。所以到底发生了什么事呢？\n",
    "\n",
    "Part of the problem here is the centrality of metrics in driving a financially important system. When an algorithm has a metric to optimize, as you have seen, it will do everything it can to optimize that number. This tends to lead to all kinds of edge cases, and humans interacting with a system will search for, find, and exploit these edge cases and feedback loops for their advantage.\n",
    "\n",
    "\n",
    "这里的部分问题是衡量指标在推动一个财务上重要的系统中的中心地位。正如你所看到的，当算法具有优化指标时，它会尽一切可能优化该数字。这往往会导致各种极端情况，并且与系统交互的人员将搜索，查找和利用这些极端情况和反馈循环以获取其优势。\n",
    "\n",
    "There are signs that this is exactly what has happened with YouTube's recommendation system. *The Guardian* ran an article called [\"How an ex-YouTube Insider Investigated its Secret Algorithm\"](https://www.theguardian.com/technology/2018/feb/02/youtube-algorithm-election-clinton-trump-guillaume-chaslot) about Guillaume Chaslot, an ex-YouTube engineer who created AlgoTransparency, which tracks these issues. Chaslot published the chart in <<ethics_yt_rt>>, following the release of Robert Mueller's \"Report on the Investigation Into Russian Interference in the 2016 Presidential Election.\"\n",
    "\n",
    "有迹象表明，这正是油管的推荐系统所发生的事情。*卫报* 发表了一篇名为 [“一位前油管内部人士如何调查其秘密算法”](https://www.theguardian.com/technology/2018/feb/02/youtube-algorithm-election-clinton-trump-guillaume-chaslot) 的文章，是关于Guillaume Chaslot的，一位前YouTube工程师，他创建了跟踪这些问题的AlgoTransparency。在Robert Mueller的 “关于俄罗斯干涉2016总统选举的调查报告” 发布之后，Chaslot在 <ethics_yt_rt> 中发表了这张图表。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image18.jpeg\" id=\"ethics_yt_rt\" caption=\"Coverage of the Mueller report\" alt=\"Coverage of the Mueller report\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Russia Today's coverage of the Mueller report was an extreme outlier in terms of how many channels were recommending it. This suggests the possibility that Russia Today, a state-owned Russia media outlet, has been successful in gaming YouTube's recommendation algorithm. Unfortunately, the lack of transparency of systems like this makes it hard to uncover the kinds of problems that we're discussing.\n",
    "\n",
    "\n",
    "从推荐的渠道数量来看，《今日俄罗斯》对穆勒报告的报道是一个极端的异常值。这表明俄罗斯国有媒体 《今日俄罗斯》 有可能在YouTube推荐算法游戏中取得成功。不幸的是，像这样的系统缺乏透明度，使得我们很难发现我们正在讨论的各种问题。\n",
    "\n",
    "One of our reviewers for this book, Aurélien Géron, led YouTube's video classification team from 2013 to 2016 (well before the events discussed here). He pointed out that it's not just feedback loops involving humans that are a problem. There can also be feedback loops without humans! He told us about an example from YouTube:\n",
    "\n",
    "\n",
    "我们这本书的评论家之一Aurélien Géron在2013年至2016年期间领导了油管的视频分类团队 (远在这里讨论的事件之前)。他指出，问题不仅仅是涉及人类的反馈循环。也可以有没有人类的反馈循环!他告诉我们一个来自油管的例子:\n",
    "\n",
    "> : One important signal to classify the main topic of a video is the channel it comes from. For example, a video uploaded to a cooking channel is very likely to be a cooking video. But how do we know what topic a channel is about? Well… in part by looking at the topics of the videos it contains! Do you see the loop? For example, many videos have a description which indicates what camera was used to shoot the video. As a result, some of these videos might get classified as videos about “photography.” If a channel has such a misclassified video, it might be classified as a “photography” channel, making it even more likely for future videos on this channel to be wrongly classified as “photography.” This could even lead to runaway virus-like classifications! One way to break this feedback loop is to classify videos with and without the channel signal. Then when classifying the channels, you can only use the classes obtained without the channel signal. This way, the feedback loop is broken.\n",
    "\n",
    "\n",
    ">: 对视频的主要主题进行分类的一个重要信号是其来源的频道。例如，上传到烹饪频道的视频很可能是烹饪视频。但是我们怎么知道一个频道是关于什么主题的呢？嗯…… 部分是通过查看它包含的视频的主题!你看到循环了吗？例如，许多视频都有一个描述，该说明指示使用什么相机拍摄视频。因此，其中一些视频可能会被归类为关于 “摄影” 的视频。如果一个频道有这样一个错误分类的视频，它可能被归类为 “摄影” 频道，使得这个频道上的未来视频更有可能被错误地归类为 “摄影”。\"这甚至可能导致类似病毒般的失控分类！打破这种反馈循环的一种方法是对带有和不带有频道信号的视频进行分类。然后在对频道进行分类时，你只能使用在没有频道信号的情况下获得的类。这样，反馈回路就中断了。\n",
    "\n",
    "There are positive examples of people and organizations attempting to combat these problems. Evan Estola, lead machine learning engineer at Meetup, [discussed the example](https://www.youtube.com/watch?v=MqoRzNhrTnQ) of men expressing more interest than women in tech meetups. taking gender into account could therefore cause Meetup’s algorithm to recommend fewer tech meetups to women, and as a result, fewer women would find out about and attend tech meetups, which could cause the algorithm to suggest even fewer tech meetups to women, and so on in a self-reinforcing feedback loop. So, Evan and his team made the ethical decision for their recommendation algorithm to not create such a feedback loop, by explicitly not using gender for that part of their model. It is encouraging to see a company not just unthinkingly optimize a metric, but consider its impact. According to Evan, \"You need to decide which feature not to use in your algorithm... the most optimal algorithm is perhaps not the best one to launch into production.\"\n",
    "\n",
    "\n",
    "有一些积极的例子表明，一些人们和组织正在试图解决这些问题。Evan Estola，Meetup的首席机器学习工程师，[讨论了这样一个例子](https://www.youtube.com/watch?v=MqoRzNhrTnQ )： 男人比女人对科技聚会更感兴趣。因此，考虑到性别可能会导致Meetup的算法向女性推荐更少的技术会议，因此，更少的女性会发现并参加技术会议，这可能会导致算法在自我强化反馈循环中向女性推荐更少的技术会议，等等。因此，Evan和他的团队做出了道德决策，要求他们的推荐算法不要通过在模型的那一部分中明确使用性别来创建这种反馈循环。看到一家公司不仅不加思索地优化了指标，还考虑了其影响，这是非常令人鼓舞的。根据Evan的说法，“你需要决定在你的算法中不使用哪个功能...最优算法也许不是投入生产的最佳算法。\"\n",
    "\n",
    "While Meetup chose to avoid such an outcome, Facebook provides an example of allowing a runaway feedback loop to run wild. Like YouTube, it tends to radicalize users interested in one conspiracy theory by introducing them to more. As Renee DiResta, a researcher on proliferation of disinformation, [writes](https://www.fastcompany.com/3059742/social-network-algorithms-are-distorting-reality-by-boosting-conspiracy-theories):\n",
    "\n",
    "虽然Meetup选择避免这样的结果，但脸书提供了一个允许失控的反馈循环失控的例子。像油管一样，它倾向于通过向用户介绍更多内容来激进化对一个阴谋论感兴趣的用户。正如虚假信息扩散的研究人员Renee DiResta [写道](https://www.fastcompany.com/3059742/social-network-algorithms-are-distorting-reality-by-boosting-conspiracy-theories ):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> : Once people join a single conspiracy-minded [Facebook] group, they are algorithmically routed to a plethora of others. Join an anti-vaccine group, and your suggestions will include anti-GMO, chemtrail watch, flat Earther (yes, really), and \"curing cancer naturally groups. Rather than pulling a user out of the rabbit hole, the recommendation engine pushes them further in.\"\n",
    "\n",
    ">: 一旦人们加入一个有阴谋思想的 [脸书] 小组，他们就会在算法上被很多其他人所吸引。加入一个抗疫苗小组，你的建议将包括抗转基因生物，化学凝结尾监视，地平论支持者（是的，真的）和“自然治愈癌症”的小组。推荐引擎没有把用户从兔子洞中拉出来，而是把他们推得更远。\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is extremely important to keep in mind that this kind of behavior can happen, and to either anticipate a feedback loop or take positive action to break it when you see the first signs of it in your own projects. Another thing to keep in mind is *bias*, which, as we discussed briefly in the previous chapter, can interact with feedback loops in very troublesome ways.\n",
    "\n",
    "牢记这种行为会发生是非常重要的，并且当你在自己的项目中看到反馈循环时，要预料到反馈循环或采取积极的行动来打破它。要记住的另一件事是 *偏见*，正如我们在前一章中简要讨论的那样，它可能会以非常麻烦的方式与反馈循环产生相互作用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias\n",
    "\n",
    "### 偏见"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussions of bias online tend to get pretty confusing pretty fast. The word \"bias\" means so many different things. Statisticians often think when data ethicists are talking about bias that they're talking about the statistical definition of the term bias. But they're not. And they're certainly not talking about the biases that appear in the weights and biases which are the parameters of your model!\n",
    "\n",
    "\n",
    "网上关于偏见的讨论往往会很快变得相当混乱。“偏见” 这个词的含义非常之多。统计学家经常认为，当数据伦理学家谈论偏见时，他们是在谈论偏见一词的统计定义。但事实并非如此。而且他们当然不是在谈论权重和偏差（这些偏差是模型的参数）中出现的偏差！\n",
    "\n",
    "What they're talking about is the social science concept of bias. In [\"A Framework for Understanding Unintended Consequences of Machine Learning\"](https://arxiv.org/abs/1901.10002) MIT's Harini Suresh and John Guttag describe six types of bias in machine learning, summarized in <<bias>> from their paper.\n",
    "\n",
    "他们谈论的是偏见的社会科学概念。在 [“理解机器学习的非预期后果的框架”](https://arxiv.org/abs/1901.10002)中，麻省理工学院的Harini Suresh和John Guttag描述了机器学习中的六种偏见，总结在他们的论文中的 <<bias>> 中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/pipeline_diagram.svg\" id=\"bias\" caption=\"Bias in machine learning can come from multiple sources (courtesy of Harini Suresh and John V. Guttag)\" alt=\"A diagram showing all sources where bias can appear in machine learning\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll discuss four of these types of bias, those that we've found most helpful in our own work (see the paper for details on the others).\n",
    "\n",
    "我们将讨论其中四种偏见，这些偏见对我们自己的工作最有帮助（有关其他详细信息，请参阅论文）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Historical bias\n",
    "\n",
    "#### 历史偏见"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Historical bias* comes from the fact that people are biased, processes are biased, and society is biased. Suresh and Guttag say: \"Historical bias is a fundamental, structural issue with the first step of the data generation process and can exist even given perfect sampling and feature selection.\"\n",
    "\n",
    "\n",
    "*历史偏见* 来源于人有偏见，过程有偏见，社会有偏见。Suresh和Guttag说: “历史偏见是数据生成过程第一步的基本结构性问题，即使给定完美的采样和特征选择，这种偏见也可能存在。”\n",
    "\n",
    "For instance, here are a few examples of historical *race bias* in the US, from the *New York Times* article [\"Racial Bias, Even When We Have Good Intentions\"](https://www.nytimes.com/2015/01/04/upshot/the-measuring-sticks-of-racial-bias-.html) by the University of Chicago's Sendhil Mullainathan:\n",
    "\n",
    "\n",
    "例如，这里有一些美国历史 *种族偏见* 的例子，来自 *纽约时报* 的由芝加哥大学的Sendhil Mullainathan发表的文章 [“种族偏见，即使我们有良好的意愿”](https://www.nytimes.com/2015/01/04/upshot/the-measuring-sticks-of-racial-bias-.html ) :\n",
    "\n",
    "  - When doctors were shown identical files, they were much less likely to recommend cardiac catheterization (a helpful procedure) to Black patients.\n",
    "  - When bargaining for a used car, Black people were offered initial prices $700 higher and received far smaller concessions.\n",
    "  - Responding to apartment rental ads on Craigslist with a Black name elicited fewer responses than with a white name.\n",
    "  - An all-white jury was 16 percentage points more likely to convict a Black defendant than a white one, but when a jury had one Black member it convicted both at the same rate.\n",
    "\n",
    "\n",
    "  - 当向医生显示相同的病情时，他们向黑人患者推荐心脏导管插入术（一种有效的手术）的可能性要小得多。。\n",
    "  - 在为二手车讨价还价时，给黑人的起步价要高出700美元，而且优惠幅度要小得多。\n",
    "  - 在Craigslist上的公寓租赁广告，用黑人名字比用白人名字引起的回应少。\n",
    "  - 全白人陪审团对黑人被告定罪的可能性比白人被告高16个百分点，但是当陪审团有一名黑人成员时，两种被告的定罪率相同。\n",
    "\n",
    "The COMPAS algorithm, widely used for sentencing and bail decisions in the US, is an example of an important algorithm that, when tested by [ProPublica](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing), showed clear racial bias in practice (<<bail_algorithm>>).\n",
    "\n",
    "在美国广泛用于判决和保释判决的COMPAS算法是一个重要算法的例子，当经过[ProPublica](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing ) 测试时，在实践中表现出明显的种族偏见 (<<bail_algorithm>>)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image6.png\" id=\"bail_algorithm\" caption=\"Results of the COMPAS algorithm\" alt=\"Table showing the COMPAS algorithm is more likely to give bail to white people, even if they re-offend more\" width=\"700\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any dataset involving humans can have this kind of bias: medical data, sales data, housing data, political data, and so on. Because underlying bias is so pervasive, bias in datasets is very pervasive. Racial bias even turns up in computer vision, as shown in the example of autocategorized photos shared on Twitter by a Google Photos user shown in <<google_photos>>.\n",
    "\n",
    "任何涉及人类的数据集都可能有这种偏见: 医疗数据、销售数据、住房数据、政治数据等等。因为潜在的偏见是如此普遍，数据集中的偏见也是非常普遍的。种族偏见甚至出现在计算机视觉中，如 <<google_photos>> 中显示的谷歌照片用户在推特上共享的自动分类照片示例所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image7.png\" id=\"google_photos\" caption=\"One of these labels is very wrong...\" alt=\"Screenshot of the use of Google photos labeling a black user and her friend as gorillas\" width=\"450\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, that is showing what you think it is: Google Photos classified a Black user's photo with their friend as \"gorillas\"! This algorithmic misstep got a lot of attention in the media. “We’re appalled and genuinely sorry that this happened,” a company spokeswoman said. “There is still clearly a lot of work to do with automatic image labeling, and we’re looking at how we can prevent these types of mistakes from happening in the future.”\n",
    "\n",
    "\n",
    "是的，正如你所想的: 谷歌照片将黑人用户和他们朋友的照片归类为“大猩猩”！这种算法失误引起了媒体的广泛关注。“我们感到震惊，并为发生的事情感到由衷的歉意，” 公司发言人说。“显然，自动图像标记还有很多工作要做，我们正在研究如何防止将来发生此类错误。”\n",
    "\n",
    "Unfortunately, fixing problems in machine learning systems when the input data has problems is hard. Google's first attempt didn't inspire confidence, as coverage by *The Guardian* suggested (<<gorilla-ban>>).\n",
    "\n",
    "不幸的是，当输入数据出现问题时，很难解决机器学习系统中的问题。谷歌的首次尝试并没有激发人们的信心，因为 *卫报* 的报道建议实施(<<大猩猩禁令>>)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image8.png\" id=\"gorilla-ban\" caption=\"Google's first response to the problem\" alt=\"Pictures of a headlines from the Guardian, showing Google removed gorillas and other moneys from the possible labels of its algorithm\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These kinds of problems are certainly not limited to just Google. MIT researchers studied the most popular online computer vision APIs to see how accurate they were. But they didn't just calculate a single accuracy number—instead, they looked at the accuracy across four different groups, as illustrated in <<face_recognition>>.\n",
    "\n",
    "这类问题当然不仅限于谷歌。麻省理工学院的研究人员研究了最流行的在线计算机视觉应用程序接口，以了解它们的准确性。但是他们不只是计算一个准确度数字，而是研究了四个不同组的准确度，如 <<face_recognition>> 所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image9.jpeg\" id=\"face_recognition\" caption=\"Error rate per gender and race for various facial recognition systems\" alt=\"Table showing how various facial recognition systems perform way worse on darker shades of skin and females\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IBM's system, for instance, had a 34.7% error rate for darker females, versus 0.3% for lighter males—over 100 times more errors! Some people incorrectly reacted to these experiments by claiming that the difference was simply because darker skin is harder for computers to recognize. However, what actually happened was that, after the negative publicity that this result created, all of the companies in question dramatically improved their models for darker skin, such that one year later they were nearly as good as for lighter skin. So what this actually showed is that the developers failed to utilize datasets containing enough darker faces, or test their product with darker faces.\n",
    "\n",
    "\n",
    "例如，IBM的系统中，对肤色较深的女性错误率为34.7％，而对肤色较浅的男性错误率为0.3％，错误率高出100倍！有些人错误地对这些实验做出了反应，声称它们之间的差异仅仅是因为皮肤较黑更难让计算机识别。然而，实际情况是，在此结果引起负面影响之后，所有相关公司都大大改善了深色肤色的模型，一年后，它们几乎与浅色肤色的一样好。因此，这实际上表明，开发人员未能利用包含足够深色面孔的数据集，或者用深色面孔测试他们的产品。\n",
    "\n",
    "One of the MIT researchers, Joy Buolamwini, warned: \"We have entered the age of automation overconfident yet underprepared. If we fail to make ethical and inclusive artificial intelligence, we risk losing gains made in civil rights and gender equity under the guise of machine neutrality.\"\n",
    "\n",
    "\n",
    "麻省理工学院的一名研究人员Joy Buolamwini警告说: “我们已经进入了对自动化过度自信但准备不足的时代。如果我们无法制造出具有道德和包容性的人工智能，那么我们将冒着以机器中立为幌子失去在民权和性别平等方面取得收益的风险。\"\n",
    "\n",
    "Part of the issue appears to be a systematic imbalance in the makeup of popular datasets used for training models. The abstract to the paper [\"No Classification Without Representation: Assessing Geodiversity Issues in Open Data Sets for the Developing World\"](https://arxiv.org/abs/1711.08536) by Shreya Shankar et al. states, \"We analyze two large, publicly available image data sets to assess geo-diversity and find that these data sets appear to exhibit an observable amerocentric and eurocentric representation bias. Further, we analyze classifiers trained on these data sets to assess the impact of these training distributions and find strong differences in the relative performance on images from different locales.\" <<image_provenance>> shows one of the charts from the paper, showing the geographic makeup of what was, at the time (and still are, as this book is being written) the two most important image datasets for training models.\n",
    "\n",
    "问题的一部分似乎是用于训练模型的流行数据集结构上的系统失衡。在Shreya Shankar等人的论文 [“没有代表性的分类: 评估发展中世界开放数据集中的地理多样性问题”](https://arxiv.org/abs/1711.08536 )摘要中提到，“我们分析了两个可公开获得的大型图像数据集，以评估地域多样性，并发现这些数据集似乎表现出可观察到的以美洲为中心和以欧洲为中心的代表偏见。此外，我们分析了在这些数据集上训练的分类器，以评估这些训练分布的影响，并发现来自不同区域的图像在相对性能方面存在很大差异。“<< image_provenance >>显示了论文中的一张图表，显示了当时（到本书编写时仍是）用于训练模型的两个最重要的图像数据集的地理组成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image10.png\" id=\"image_provenance\" caption=\"Image provenance in popular training sets\" alt=\"Graphs showing how the vast majority of images in popular training datasets come from the US or Western Europe\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vast majority of the images are from the United States and other Western countries, leading to models trained on ImageNet performing worse on scenes from other countries and cultures. For instance, research found that such models are worse at identifying household items (such as soap, spices, sofas, or beds) from lower-income countries. <<object_detect>> shows an image from the paper, [\"Does Object Recognition Work for Everyone?\"](https://arxiv.org/pdf/1906.02659.pdf) by Terrance DeVries et al. of Facebook AI Research that illustrates this point.\n",
    "\n",
    "绝大多数图像来自美国和其他西方国家，导致在ImageNet上训练的模型在其他国家和文化的场景中表现更差。例如，研究发现，这种模式在识别低收入国家的家居用品 (如肥皂，香料，沙发或床) 方面更糟糕。<<Object_detect>> 显示了Facebook人工智能研究的Terrance DeVries等的论文中的图像，[“对象识别对每个人都有效吗？”](https://arxiv.org/pdf/1906.02659.pdf)，其中阐明了这一点。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image17.png\" id=\"object_detect\" caption=\"Object detection in action\" alt=\"Figure showing an object detection algorithm performing better on western products\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we can see that the lower-income soap example is a very long way away from being accurate, with every commercial image recognition service predicting \"food\" as the most likely answer!\n",
    "\n",
    "\n",
    "在这个例子中，我们可以看到，低收入肥皂示例距离准确还有很长的路要走，每个商业图像识别服务都将“食物”预测为最可能的答案！\n",
    "\n",
    "As we will discuss shortly, in addition, the vast majority of AI researchers and developers are young white men. Most projects that we have seen do most user testing using friends and families of the immediate product development group. Given this, the kinds of problems we just discussed should not be surprising.\n",
    "\n",
    "\n",
    "正如我们即将讨论的那样，此外，绝大多数人工智能研究人员和开发人员都是年轻的白人。我们看到的很多项目都使用直接产品开发团队的朋友和家人进行大多数的用户测试。鉴于此，我们刚刚讨论的各种问题应该不足为奇。\n",
    "\n",
    "Similar historical bias is found in the texts used as data for natural language processing models. This crops up in downstream machine learning tasks in many ways. For instance, it [was widely reported](https://nypost.com/2017/11/30/google-translates-algorithm-has-a-gender-bias/) that until last year Google Translate showed systematic bias in how it translated the Turkish gender-neutral pronoun \"o\" into English: when applied to jobs which are often associated with males it used \"he,\" and when applied to jobs which are often associated with females it used \"she\" (<<turkish_gender>>).\n",
    "\n",
    "在用作自然语言处理模型数据的文本中也发现了类似的历史偏见。这以多种方式出现在下游机器学习任务中。例如，它 [被广泛报道](https://nypost.com/2017/11/30/google-translates-algorithm-has-a-gender-bias/)，直到去年，谷歌翻译在将土耳其语中性代词 “o” 翻译成英语方面还表现出系统的偏见: 当应用于通常与男性相关的工作时，使用 “他”，当应用于通常与女性相关的工作时，则使用 “她” (<<turkish_gender>>)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image11.png\" id=\"turkish_gender\" caption=\"Gender bias in text data sets\" alt=\"Figure showing gender bias in data sets used to train language models showing up in translations\" width=\"600\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also see this kind of bias in online advertisements. For instance, a [study](https://arxiv.org/abs/1904.02095) in 2019 by Muhammad Ali et al. found that even when the person placing the ad does not intentionally discriminate, Facebook will show ads to very different audiences based on race and gender. Housing ads with the same text, but picture either a white or a Black family, were shown to racially different audiences.\n",
    "\n",
    "我们在网络广告中也看到这种偏见。例如，Muhammad Ali等人在2019年的一项[研究](https://arxiv.org/abs/1904.02095 ) 发现，即使放置广告的人没有故意歧视，脸书也会根据种族和性别向非常不同的受众展示广告。展示带有相同文字但带有白人或黑人家庭图片的广告时，会向不同种族的受众展示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measurement bias\n",
    "\n",
    "#### 测量偏见"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper [\"Does Machine Learning Automate Moral Hazard and Error\"](https://scholar.harvard.edu/files/sendhil/files/aer.p20171084.pdf) in *American Economic Review*, Sendhil Mullainathan and Ziad Obermeyer look at a model that tries to answer the question: using historical electronic health record (EHR) data, what factors are most predictive of stroke? These are the top predictors from the model:\n",
    "\n",
    "\n",
    "在 *美国经济评论* 的论文 [“机器学习是否自动化了道德风险和错误”](https://scholar.harvard.edu/files/sendhil/files/aer.p20171084.pdf)中，Sendhil Mullainathan和Ziad Obermeyer看了一个试图回答这个问题的模型: 使用历史电子健康记录 (EHR) 数据，哪些因素最能预测中风？这些是模型中排名靠前的预测指标:\n",
    "\n",
    "  - Prior stroke\n",
    "  - Cardiovascular disease\n",
    "  - Accidental injury\n",
    "  - Benign breast lump\n",
    "  - Colonoscopy\n",
    "  - Sinusitis\n",
    "\n",
    "\n",
    "  - 既往卒中\n",
    "  - 心血管疾病\n",
    "  - 意外伤害\n",
    "  - 良性乳房肿块\n",
    "  - 结肠镜检查\n",
    "  - 鼻窦炎\n",
    "\n",
    "However, only the top two have anything to do with a stroke! Based on what we've studied so far, you can probably guess why. We haven’t really measured *stroke*, which occurs when a region of the brain is denied oxygen due to an interruption in the blood supply. What we’ve measured is who had symptoms, went to a doctor, got the appropriate tests, *and* received a diagnosis of stroke. Actually having a stroke is not the only thing correlated with this complete list—it's also correlated with being the kind of person who actually goes to the doctor (which is influenced by who has access to healthcare, can afford their co-pay, doesn't experience racial or gender-based medical discrimination, and more)! If you are likely to go to the doctor for an *accidental injury*, then you are likely to also go the doctor when you are having a stroke.\n",
    "\n",
    "\n",
    "然而，只有前两个与中风有关！根据到目前为止的研究，你可能会猜出原因。我们还没有真正测量 *中风*，当由于血液供应中断而使大脑某个区域缺氧时，会发生这种情况。我们测量的是谁有症状，去看医生，做了相应的检查，*并* 得出了中风的诊断。实际上，中风并不是唯一与这个完整列表相关的事情 -- 它也与真正去看医生的人相关（受到医疗保健，能够负担其自付费用，不经历基于种族或性别的医疗歧视，等等的影响)！如果你可能因为意外伤害去看医生，那么中风时，也可能会去看医生。\n",
    "\n",
    "This is an example of *measurement bias*. It occurs when our models make mistakes because we are measuring the wrong thing, or measuring it in the wrong way, or incorporating that measurement into the model inappropriately.\n",
    "\n",
    "这是 *测量偏见* 的示例。当我们的模型出错时，就会发生这种情况，因为我们测量了错误的东西，或者以错误的方式测量了它，或者不恰当地将这种测量纳入了模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregation bias\n",
    "\n",
    "#### 聚合偏见"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Aggregation bias* occurs when models do not aggregate data in a way that incorporates all of the appropriate factors, or when a model does not include the necessary interaction terms, nonlinearities, or so forth. This can particularly occur in medical settings. For instance, the way diabetes is treated is often based on simple univariate statistics and studies involving small groups of heterogeneous people. Analysis of results is often done in a way that does not take account of different ethnicities or genders. However, it turns out that diabetes patients have [different complications across ethnicities](https://www.ncbi.nlm.nih.gov/pubmed/24037313), and HbA1c levels (widely used to diagnose and monitor diabetes) [differ in complex ways across ethnicities and genders](https://www.ncbi.nlm.nih.gov/pubmed/22238408). This can result in people being misdiagnosed or incorrectly treated because medical decisions are based on a model that does not include these important variables and interactions.\n",
    "\n",
    "*聚合偏见*，当模型没有以包含所有适当因素的方式聚合数据时，或者当模型不包括必要的交互项、非线性等时，会发生聚合偏见。这尤其可能发生在医疗环境中。例如，糖尿病的治疗方式通常基于简单的单变量统计数据和涉及一小群异质人群的研究。结果分析通常是以不考虑不同种族或性别的方式进行的。然而，事实证明，糖尿病患者有 [种族之间的并发症不同](https://www.ncbi.nlm.nih.gov/pubmed/24037313)，而且HbA1c水平 (广泛用于诊断和监测糖尿病) [在不同种族和性别中存在复杂的差异](https://www.ncbi.nlm.nih.gov/pubmed/22238408)。这可能会导致人们被误诊或治疗不当，因为医疗决策基于不包含这些重要变量和相互作用的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Representation bias\n",
    "\n",
    "#### 表征偏见"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The abstract of the paper [\"Bias in Bios: A Case Study of Semantic Representation Bias in a High-Stakes Setting\"](https://arxiv.org/abs/1901.09451) by Maria De-Arteaga et al. notes that there is gender imbalance in occupations (e.g., females are more likely to be nurses, and males are more likely to be pastors), and says that: \"differences in true positive rates between genders are correlated with existing gender imbalances in occupations, which may compound these imbalances.\"\n",
    "\n",
    "\n",
    "Maria De-Arteaga等人的论文[“Bios中的偏见: 高风险设置中语义表征偏见的案例研究”](https://arxiv.org/abs/1901.09451) 的摘要中指出，职业中存在性别不平衡 (例如，女性更有可能成为护士，而男性更有可能成为牧师)，并说: “性别之间的真实阳性的率差异与职业中现有的性别不平衡相关，这可能加剧这些不平衡。\"\n",
    "\n",
    "In other words, the researchers noticed that models predicting occupation did not only *reflect* the actual gender imbalance in the underlying population, but actually *amplified* it! This type of *representation bias* is quite common, particularly for simple models. When there is some clear, easy-to-see underlying relationship, a simple model will often simply assume that this relationship holds all the time. As <<representation_bias>> from the paper shows, for occupations that had a higher percentage of females, the model tended to overestimate the prevalence of that occupation.\n",
    "\n",
    "换句话说，研究人员注意到，预测职业的模型不仅“反映”了潜在人口中的实际性别失衡，而且还“放大”了这一事实！这种类型的 *表征偏见* 非常常见，尤其是对于简单的模型。当存在一些清晰、易于理解的潜在关系时，简单的模型通常会简单地假设这种关系一直存在。如论文中 <<representation_bias>> 所示，对于女性比例较高的职业，该模型往往高估了该职业的普遍性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image12.png\" id=\"representation_bias\" caption=\"Model error in predicting occupation plotted against percentage of women in said occupation\" alt=\"Graph showing how model predictions overamplify existing bias\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, in the training dataset 14.6% of surgeons were women, yet in the model predictions only 11.6% of the true positives were women. The model is thus amplifying the bias existing in the training set.\n",
    "\n",
    "\n",
    "例如，在训练数据集中，14.6% 的外科医生是女性，然而在模型预测中，只有11.6% 的真阳性是女性。因此，该模型放大了训练集中存在的偏见。\n",
    "\n",
    "Now that we've seen that those biases exist, what can we do to mitigate them?\n",
    "\n",
    "既然我们已经看到这些偏见存在，那么我们该怎么做才能将其缓解呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing different types of bias\n",
    "\n",
    "### 解决不同类型的偏差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different types of bias require different approaches for mitigation. While gathering a more diverse dataset can address representation bias, this would not help with historical bias or measurement bias.  All datasets contain bias.  There is no such thing as a completely debiased dataset.  Many researchers in the field have been converging on a set of proposals to enable better documentation of the decisions, context, and specifics about how and why a particular dataset was created, what scenarios it is appropriate to use in, and what the limitations are. This way, those using a particular dataset will not be caught off guard by its biases and limitations.\n",
    "\n",
    "不同类型的偏见需要不同的缓解方法。虽然收集更多样化的数据集可以解决表征偏见，但这对历史偏见或测量偏见无济于事。所有数据集都包含偏见。完全没有偏见的数据集是不存在的。该领域的许多研究人员已经就一系列建议达成一致，以便更好地记录决策、背景和关于如何以及为什么创建特定数据集的细节，它适合在哪些场景中使用，以及有哪些局限。这样，那些使用特定数据集的人不会因为其偏见和局限而措手不及。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often hear the question—\"Humans are biased, so does algorithmic bias even matter?\" This comes up so often, there must be some reasoning that makes sense to the people that ask it, but it doesn't seem very logically sound to us! Independently of whether this is logically sound, it's important to realize that algorithms (particularly machine learning algorithms!) and people are different. Consider these points about machine learning algorithms:\n",
    "\n",
    "\n",
    "我们经常听到这样一个问题 -- “人类也有偏见，所以算法偏见很重要吗？“ 这种情况经常出现，必须有一些对提出要求的人来说有意义的推理，但对我们来说，这似乎不太符合逻辑！不管这在逻辑上是否合理，重要的是要意识到算法 (尤其是机器学习算法!) 和人是不同的。要考虑机器学习算法的以下几点:\n",
    "\n",
    "  - _Machine learning can create feedback loops_:: Small amounts of bias can rapidly increase exponentially due to feedback loops.\n",
    "  - _Machine learning can amplify bias_:: Human bias can lead to larger amounts of machine learning bias.\n",
    "  - _Algorithms & humans are used differently_:: Human decision makers and algorithmic decision makers are not used in a plug-and-play interchangeable way in practice.\n",
    "  - _Technology is power_:: And with that comes responsibility.\n",
    "\n",
    "\n",
    "  - _机器学习会创建反馈循环_:: 少量的偏差会由于反馈循环而迅速成倍增加。。\n",
    "  - _机器学习会放大偏见_:: 人为偏见会导致更多的机器学习偏见。\n",
    "  - _算法&人类的使用方式不同_:: 在实践中，人类决策者和算法决策者在实践中不是以即插即用的互换方式使用。。\n",
    "  - _技术就是力量_:: 随之而来的是责任。\n",
    "\n",
    "As the Arkansas healthcare example showed, machine learning is often implemented in practice not because it leads to better outcomes, but because it is cheaper and more efficient. Cathy O'Neill, in her book *Weapons of Math Destruction* (Crown), described the pattern of how the privileged are processed by people, whereas the poor are processed by algorithms. This is just one of a number of ways that algorithms are used differently than human decision makers. Others include:\n",
    "\n",
    "\n",
    "正如阿肯色州医疗保健的例子所显示的，机器学习通常在实践中实施，不是因为它能带来更好的结果，而是因为它更便宜、更高效。Cathy O'Neill在她的 *数学破坏武器* (皇冠) 中描述了人们如何处理特权，而穷人则通过算法处理的模式。这只是不同于人类决策者使用算法的多种方式之一。其他包括:\n",
    "\n",
    "  - People are more likely to assume algorithms are objective or error-free (even if they’re given the option of a human override).\n",
    "  - Algorithms are more likely to be implemented with no appeals process in place.\n",
    "  - Algorithms are often used at scale.\n",
    "  - Algorithmic systems are cheap.\n",
    "\n",
    "\n",
    "  - 人们更有可能假设算法是客观的或没有错误的 (即使可以选择人工替代)。\n",
    "  - 算法更有可能在没有适当的申诉流程的情况下实施。\n",
    "  - 算法通常被大规模使用。\n",
    "  - 算法系统便宜。\n",
    "\n",
    "Even in the absence of bias, algorithms (and deep learning especially, since it is such an effective and scalable algorithm) can lead to negative societal problems, such as when used for *disinformation*.\n",
    "\n",
    "即使在没有偏见的情况下，算法（尤其是深度学习，因为它是一种有效且可扩展的算法）也可能导致负面的社会问题，例如在用于 *虚假信息* 时。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disinformation\n",
    "\n",
    "### 虚假信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Disinformation* has a history stretching back hundreds or even thousands of years. It is not necessarily about getting someone to believe something false, but rather often used to sow disharmony and uncertainty, and to get people to give up on seeking the truth.  Receiving conflicting accounts can lead people to assume that they can never know whom or what to trust.\n",
    "\n",
    "\n",
    "*虚假信息* 的历史可以追溯到数百甚至数千年前。这不一定是为了让某人相信一些虚假的东西，而是经常用来散布不和谐和不确定性，并让人们放弃寻求真相。接收有冲突的描述会导致人们认为他们永远不知道该信任谁或信任什么。\n",
    "\n",
    "Some people think disinformation is primarily about false information or *fake news*, but in reality, disinformation can often contain seeds of truth, or half-truths taken out of context. Ladislav Bittman was an intelligence officer in the USSR who later defected to the US and wrote some books in the 1970s and 1980s on the role of disinformation in Soviet propaganda operations. In *The KGB and Soviet Disinformation* (Pergamon) he wrote, \"Most campaigns are a carefully designed mixture of facts, half-truths, exaggerations, and deliberate lies.\"\n",
    "\n",
    "\n",
    "有些人认为虚假信息主要是关于错误信息或 *虚假新闻* ，但实际上，虚假信息往往包含真相的种子，或断章取义的半真半假。Ladislav Bittman是前苏联的一名情报官员，后来叛逃到美国，并在1970年代和1980年代写了一些关于虚假信息在苏联宣传行动中的作用的书。在 *克格勃和苏联的虚假信息* 中，他写道，“大多数竞选活动都是精心设计的事实、半真半假、夸大其实和蓄意谎言的混合物。”\n",
    "\n",
    "In the US this has hit close to home in recent years, with the FBI detailing a massive disinformation campaign linked to Russia in the 2016 election. Understanding the disinformation that was used in this campaign is very educational. For instance, the FBI found that the Russian disinformation campaign often organized two separate fake \"grass roots\" protests, one for each side of an issue, and got them to protest at the same time! The [*Houston Chronicle*](https://www.houstonchronicle.com/local/gray-matters/article/A-Houston-protest-organized-by-Russian-trolls-12625481.php) reported on one of these odd events (<<texas>>).\n",
    "\n",
    "\n",
    "在美国，这种情况近几年来已越发露骨，联邦调查局详细描述了在2016年大选中与俄罗斯有关的大规模虚假信息活动。了解这场活动中使用的虚假信息非常有教育意义。例如，联邦调查局发现，俄罗斯的虚假信息宣传活动经常组织两次单独的假“草根”抗议活动，每次针对问题的一方，并让他们同时进行抗议！[*休斯顿纪事报*](https://www.houstonchronicle.com/local/gray-matters/article/A-Houston-protest-organized-by-Russian-trolls-12625481.php) 报告了这些奇怪事件之一 (<<texas>>)。\n",
    "\n",
    "> : A group that called itself the \"Heart of Texas\" had organized it on social media—a protest, they said, against the \"Islamization\" of Texas. On one side of Travis Street, I found about 10 protesters. On the other side, I found around 50 counterprotesters. But I couldn't find the rally organizers. No \"Heart of Texas.\" I thought that was odd, and mentioned it in the article: What kind of group is a no-show at its own event? Now I know why. Apparently, the rally's organizers were in Saint Petersburg, Russia, at the time. \"Heart of Texas\" is one of the internet troll groups cited in Special Prosecutor Robert Mueller's recent indictment of Russians attempting to tamper with the U.S. presidential election.\n",
    "\n",
    ">: 一个自称为 “德克萨斯之心” 的团体在社交媒体上组织了它 -- 他们说，这是反对德克萨斯 “伊斯兰化” 的抗议。在特拉维斯街的一边，我发现了大约10名抗议者。在另一边，我发现了大约50名反抗议者。但是我找不到集会的组织者。并没有“德克萨斯之心”。我觉得这很奇怪，并在文章中提到了这一点: 什么样的团体在自己的活动中没有出现？现在我知道为什么了。显然，集会的组织者当时在俄罗斯圣彼得堡。“德克萨斯之心” 是特别检察官Robert Mueller最近对俄罗斯人试图篡改美国总统选举的指控中引用的互联网巨魔团体之一。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image13.png\" id=\"texas\" caption=\"Event organized by the group Heart of Texas\" alt=\"Screenshot of an event organized by the group Heart of Texas\" width=\"300\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disinformation often involves coordinated campaigns of inauthentic behavior.  For instance, fraudulent accounts may try to make it seem like many people hold a particular viewpoint.  While most of us like to think of ourselves as independent-minded, in reality we evolved to be influenced by others in our in-group, and in opposition to those in our out-group.  Online discussions can influence our viewpoints, or alter the range of what we consider acceptable viewpoints. Humans are social animals, and as social animals we are extremely influenced by the people around us. Increasingly, radicalization occurs in online environments; influence is coming from people in the virtual space of online forums and social networks.\n",
    "\n",
    "\n",
    "虚假信息通常涉及不真实行为的协调活动。例如，欺诈性账户可能试图让它看起来像是许多人持有一个特定的观点。尽管我们大多数人喜欢将自己视为思想独立的人，但实际上，我们逐渐会受到团队内其他人的影响，与团队外的人相反。在线讨论会影响我们的观点，或改变我们认为可接受的观点的范围。人类是群居动物，作为群居动物，我们受到周围人的极大影响。激进现象越来越多地出现在网络环境中; 影响来自在线论坛和社交网络虚拟空间中的人们。\n",
    "\n",
    "Disinformation through autogenerated text is a particularly significant issue, due to the greatly increased capability provided by deep learning. We discuss this issue in depth when we delve into creating language models, in <<chapter_nlp>>.\n",
    "\n",
    "\n",
    "由于深度学习所提供的功能大大增加，通过自动生成的文本生成的虚假信息是一个特别重要的问题。当我们在 <<chapter_nlp>> 中深入研究创建语言模型时，我们会深入讨论这个问题。\n",
    "\n",
    "One proposed approach is to develop some form of digital signature, to implement it in a seamless way, and to create norms that we should only trust content that has been verified. The head of the Allen Institute on AI, Oren Etzioni, wrote such a proposal in an article titled [\"How Will We Prevent AI-Based Forgery?\"](https://hbr.org/2019/03/how-will-we-prevent-ai-based-forgery): \"AI is poised to make high-fidelity forgery inexpensive and automated, leading to potentially disastrous consequences for democracy, security, and society. The specter of AI forgery means that we need to act to make digital signatures de rigueur as a means of authentication of digital content.\"\n",
    "\n",
    "\n",
    "建议的一种方法是开发某种形式的数字签名，以无缝的方式实现它，并创建规范，使我们仅信任已验证的内容。艾伦人工智能研究所的负责人Oren Etzioni在一篇名为 [“我们将如何防止基于人工智能的伪造？”](https://hbr.org/2019/03/how-will-we-prevent-ai-based-forgery)的文章中建议道: “人工智能有望使廉价的高保真伪造变得廉价和自动化，从而给民主、安全和社会带来潜在的灾难性后果。人工智能伪造的幽灵意味着我们需要采取行动使数字签名成为必需的手段，以作为对数字内容进行身份验证的手段。\"\n",
    "\n",
    "Whilst we can't hope to discuss all the ethical issues that deep learning, and algorithms more generally, brings up, hopefully this brief introduction has been a useful starting point you can build on. We'll now move on to the questions of how to identify ethical issues, and what to do about them.\n",
    "\n",
    "尽管我们不能希望讨论深度学习以及更一般的算法带来的所有伦理问题，但希望此简短的介绍是你可以建立的一个有用的起点。现在，我们将继续讨论如何识别伦理问题，以及如何处理这些问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying and Addressing Ethical Issues\n",
    "\n",
    "## 识别和解决伦理问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mistakes happen. Finding out about them, and dealing with them, needs to be part of the design of any system that includes machine learning (and many other systems too).  The issues raised within data ethics are often complex and interdisciplinary, but it is crucial that we work to address them.\n",
    "\n",
    "\n",
    "当错误发生时，了解它们并处理它们，需要成为任何包括机器学习 (以及许多其他系统) 的系统设计的一部分。数据伦理中提出的问题通常是复杂和跨学科的，但我们必须努力解决这些问题。\n",
    "\n",
    "So what can we do?  This is a big topic, but a few steps towards addressing ethical issues are:\n",
    "\n",
    "\n",
    "那么，我们能做些什么呢？ 这是一个很大的话题，但是一些解决伦理问题的步骤是：\n",
    "\n",
    "- Analyze a project you are working on.\n",
    "- Implement processes at your company to find and address ethical risks.\n",
    "- Support good policy.\n",
    "- Increase diversity.\n",
    "\n",
    "\n",
    "- 分析你正在进行的项目。\n",
    "- 在你的公司实施流程，以发现和解决伦理风险。\n",
    "- 支持良好的政策。\n",
    "- 增加多样性。\n",
    "\n",
    "Let's walk through each of these steps, starting with analyzing a project you are working on.\n",
    "\n",
    "让我们从分析你正在处理的项目开始，逐步完成每个步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze a Project You Are Working On\n",
    "\n",
    "### 分析你正在处理的项目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to miss important issues when considering ethical implications of your work. One thing that helps enormously is simply asking the right questions. Rachel Thomas recommends considering the following questions throughout the development of a data project:\n",
    "\n",
    "\n",
    "在考虑工作的伦理影响时，很容易错过重要问题。有一件事非常有效，那就是简单地提出正确的问题。Rachel Thomas建议在整个数据项目开发过程中考虑以下问题:\n",
    "\n",
    "  - Should we even be doing this?\n",
    "  - What bias is in the data?\n",
    "  - Can the code and data be audited?\n",
    "  - What are the error rates for different sub-groups?\n",
    "  - What is the accuracy of a simple rule-based alternative?\n",
    "  - What processes are in place to handle appeals or mistakes?\n",
    "  - How diverse is the team that built it?\n",
    "\n",
    "\n",
    "- 我们应该这么做吗？\n",
    "- 数据中有什么偏见吗？\n",
    "- 可以对代码和数据进行审核吗？\n",
    "- 不同子组的错误率是多少？\n",
    "- 一个简单的基于规则的替代方法的准确性是多少？\n",
    "- 有什么程序来处理上诉或错误？\n",
    "- 构建它的团队会有怎么样的多样化？\n",
    "\n",
    "These questions may be able to help you identify outstanding issues, and possible alternatives that are easier to understand and control. In addition to asking the right questions, it's also important to consider practices and processes to implement.\n",
    "\n",
    "\n",
    "这些问题也许可以帮助你确定悬而未决的问题，以及更容易理解和控制的可能替代方案。除了提出正确的问题之外，考虑要实施的实践和流程也很重要。\n",
    "\n",
    "One thing to consider at this stage is what data you are collecting and storing. Data often ends up being used for different purposes than what it was originally collected for. For instance, IBM began selling to Nazi Germany well before the Holocaust, including helping with Germany’s 1933 census conducted by Adolf Hitler, which was effective at identifying far more Jewish people than had previously been recognized in Germany. Similarly, US census data was used to round up Japanese-Americans (who were US citizens) for internment during World War II. It is important to recognize how data and images collected can be weaponized later. Columbia professor [Tim Wu wrote](https://www.nytimes.com/2019/04/10/opinion/sunday/privacy-capitalism.html) that “You must assume that any personal data that Facebook or Android keeps are data that governments around the world will try to get or that thieves will try to steal.”\n",
    "\n",
    "在此阶段要考虑的一件事是要收集和存储哪些数据。数据最终通常被用于不同于最初收集目的的目的。例如，IBM在大屠杀发生之前就已开始向纳粹德国出售产品，包括帮助阿道夫·希特勒在1933年进行的德国人口普查，该普查在识别犹太人方面比以前在德国的识别要有效得多。同样，在第二次世界大战期间，美国人口普查数据被用来对日裔美国人（美国公民）进行拘留。重要的是要认识到以后如何对收集的数据和图像进行武器化。哥伦比亚教授 [Tim Wu写道]( https://www.nytimes.com/2019/04/10/opinion/sunday/privacy-capitalism.html) “你必须假设脸书或安卓保留的任何个人数据是世界各国政府将尝试获取的数据或窃贼将试图窃取的数据。”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processes to Implement\n",
    "\n",
    "### 实施流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Markkula Center has released [An Ethical Toolkit for Engineering/Design Practice](https://www.scu.edu/ethics-in-technology-practice/ethical-toolkit/) that includes some concrete practices to implement at your company, including regularly scheduled sweeps to proactively search for ethical risks (in a manner similar to cybersecurity penetration testing), expanding the ethical circle to include the perspectives of a variety of stakeholders, and considering the terrible people (how could bad actors abuse, steal, misinterpret, hack, destroy, or weaponize what you are building?). \n",
    "\n",
    "\n",
    "马库拉中心发布了 [工程/设计实践伦理工具包](https://www.scu.edu/ethics-in-technology-practice/ethical-toolkit/)，其中包括要在贵公司实施的一些具体做法，包括定期进行扫描以主动搜索伦理风险 (以类似于网络安全渗透测试的方式)，扩大伦理圈，以涵盖各种利益相关者的观点，并考虑到坏人 (坏人会如何滥用，窃取，曲解，黑客攻击，摧毁或武器化你正在构建的东西？)。\n",
    "\n",
    "Even if you don't have a diverse team, you can still try to pro-actively include the perspectives of a wider group, considering questions such as these (provided by the Markkula Center):\n",
    "\n",
    "\n",
    "即使你没有一个多样化的团队，你仍然可以尝试考虑诸如此类的问题 (由马库拉中心提供)，来积极主动地吸取更广泛的群体的观点:\n",
    "\n",
    "  - Whose interests, desires, skills, experiences, and values have we simply assumed, rather than actually consulted?\n",
    "  - Who are all the stakeholders who will be directly affected by our product? How have their interests been protected? How do we know what their interests really are—have we asked?\n",
    "  - Who/which groups and individuals will be indirectly affected in significant ways?\n",
    "  - Who might use this product that we didn’t expect to use it, or for purposes we didn’t initially intend?\n",
    "\n",
    "- 我们只是简单地假定而不是实际咨询了谁的兴趣，愿望，技能，经验和价值观？\n",
    "- 谁将是直接受到我们产品影响的所有利益相关者？ 他们的利益如何得到保护？ 我们如何知道他们真正的兴趣是什么-我们问过吗？\n",
    "- 谁/哪些群体和个人将受到重大间接影响？\n",
    "- 谁可能会使用我们不希望使用的这种产品，或者用于我们最初不打算使用的目的？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ethical lenses\n",
    "\n",
    "#### 伦理视角"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful resource from the Markkula Center is its [Conceptual Frameworks in Technology and Engineering Practice](https://www.scu.edu/ethics-in-technology-practice/conceptual-frameworks/). This considers how different foundational ethical lenses can help identify concrete issues, and lays out the following approaches and key questions:\n",
    "\n",
    "\n",
    "马库拉中心的另一个很实用的资源是 [技术和工程实践中的概念框架](https://www.scu.edu/ethics-in-technology-practice/conceptual-frameworks/)。其中考虑了不同的基本伦理视角如何帮助确定具体问题，并提出了以下方法和关键问题：\n",
    "\n",
    "  - The rights approach:: Which option best respects the rights of all who have a stake?\n",
    "  - The justice approach:: Which option treats people equally or proportionately?\n",
    "  - The utilitarian approach:: Which option will produce the most good and do the least harm?\n",
    "  - The common good approach:: Which option best serves the community as a whole, not just some members?\n",
    "  - The virtue approach:: Which option leads me to act as the sort of person I want to be?\n",
    "\n",
    "\n",
    "  - 权利方法:: 哪种选择最能尊重所有利益相关者的权利？\n",
    "  - 司法方法:: 哪种选择对人一视同仁或者按比例对待？\n",
    "  - 功利主义的方法:: 哪种选择会产生最大的好处和伤害最小？\n",
    "  - 共同利益的方法:: 哪种选择最能为整个社区服务，而不仅限于某些成员？\n",
    "  - 美德方法:: 哪种选择可以引导我成为我想成为的那种人？\n",
    "\n",
    "Markkula's recommendations include a deeper dive into each of these perspectives, including looking at a project through the lenses of its *consequences*:\n",
    "\n",
    "\n",
    "马库拉的建议包括对这些观点的更深入研究，包括从其 *后果* 的角度看待一个项目：\n",
    "\n",
    "  - Who will be directly affected by this project? Who will be indirectly affected?\n",
    "  - Will the effects in aggregate likely create more good than harm, and what types of good and harm?\n",
    "  - Are we thinking about all relevant types of harm/benefit (psychological, political, environmental, moral, cognitive, emotional, institutional, cultural)?\n",
    "  - How might future generations be affected by this project?\n",
    "  - Do the risks of harm from this project fall disproportionately on the least powerful in society? Will the benefits go disproportionately to the well-off?\n",
    "  - Have we adequately considered \"dual-use\"?\n",
    "\n",
    "\n",
    " - 谁将直接受到该项目的影响？ 谁将间接受到影响？\n",
    " - 总体而言，这种影响会带来多于坏处的好处，以及哪种类型的好处和坏处呢？\n",
    " - 我们是否在考虑所有相关的伤害/利益 (心理的、政治的、环境的、道德的、认知的、情感的、制度的、文化的)？\n",
    " - 这个项目将对后代产生怎样的影响？\n",
    " - 这个项目带来的危害风险是否不成比例地落在社会上最弱势的群体身上？福利会不成比例地流向富人吗?\n",
    " - 我们是否充分考虑过 “双重用途”？\n",
    "\n",
    "The alternative lens to this is the *deontological* perspective, which focuses on basic concepts of *right* and *wrong*:\n",
    "\n",
    "\n",
    "对此的替代视角是 *义务论* 的视角，它侧重于 *对* 与 *错* 的基本概念:\n",
    "\n",
    "  - What rights of others and duties to others must we respect?\n",
    "  - How might the dignity and autonomy of each stakeholder be impacted by this project?\n",
    "  - What considerations of trust and of justice are relevant to this design/project?\n",
    "  - Does this project involve any conflicting moral duties to others, or conflicting stakeholder rights? How can we prioritize these?\n",
    "\n",
    "\n",
    " - 我们必须尊重他人的哪些权利和对他人的义务？\n",
    " - 这个项目会如何影响每个利益相关者的尊严和自主权？\n",
    " - 与该设计/项目相关的信任和公正方面的考虑因素是什么?\n",
    " - 该项目是否涉及对他人的道德责任冲突，或利益相关者的权利冲突?我们如何对这些进行优先排序？\n",
    "\n",
    "One of the best ways to help come up with complete and thoughtful answers to questions like these is to ensure that the people asking the questions are *diverse*.\n",
    "\n",
    "要想对这些问题给出完整而深思熟虑的答案，最好的方法之一就是确保提出问题的人们“各不相同”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Power of Diversity\n",
    "\n",
    "### 多样性的力量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, less than 12% of AI researchers are women, according to [a study from Element AI](https://medium.com/element-ai-research-lab/estimating-the-gender-ratio-of-ai-researchers-around-the-world-81d2b8dbe9c3). The statistics are similarly dire when it comes to race and age. When everybody on a team has similar backgrounds, they are likely to have similar blindspots around ethical risks. The *Harvard Business Review* (HBR) has published a number of studies showing many benefits of diverse teams, including:\n",
    "\n",
    "\n",
    "目前，只有不到12% 的人工智能研究人员是女性，根据 [来自人工智能元素的一项研究](https://medium.com/element-ai-research-lab/estimating-the-gender-ratio-of-ai-researchers-around-the-world-81d2b8dbe9c3)。有关种族和年龄的统计数据也同样糟糕。当团队中的每个人都有相似的背景时，他们很可能在道德风险方面有相似的盲点。*《哈佛商业评论》* (HBR) 发表了许多研究，这些研究表明了不同团队的许多好处，包括:\n",
    "\n",
    "- [\"How Diversity Can Drive Innovation\"](https://hbr.org/2013/12/how-diversity-can-drive-innovation)\n",
    "- [\"Teams Solve Problems Faster When They’re More Cognitively Diverse\"](https://hbr.org/2017/03/teams-solve-problems-faster-when-theyre-more-cognitively-diverse)\n",
    "- [\"Why Diverse Teams Are Smarter\"](https://hbr.org/2016/11/why-diverse-teams-are-smarter), and\n",
    "- [\"Defend Your Research: What Makes a Team Smarter? More Women\"](https://hbr.org/2011/06/defend-your-research-what-makes-a-team-smarter-more-women)\n",
    "\n",
    "\n",
    "- [“多样性如何推动创新”](https://hbr.org/2013/12/how-diversity-can-drive-innovation)\n",
    "- [“当团队在认知上更加多样化时，他们可以更快地解决问题”](https://hbr.org/2017/03/teams-solve-problems-faster-when-theyre-more-cognitively-diverse)\n",
    "- [“为什么多样化的团队更聪明”](https://hbr.org/2016/11/why-diverse-teams-are-smarter) 以及\n",
    "- [“捍卫你的研究: 是什么让团队更聪明？更多女性 ”](https://hbr.org/2011/06/defend-your-research-what-makes-a-team-smarter-more-women )\n",
    "\n",
    "Diversity can lead to problems being identified earlier, and a wider range of solutions being considered. For instance, Tracy Chou was an early engineer at Quora. She [wrote of her experiences](https://qz.com/1016900/tracy-chou-leading-silicon-valley-engineer-explains-why-every-tech-worker-needs-a-humanities-education/), describing how she advocated internally for adding a feature that would allow trolls and other bad actors to be blocked. Chou recounts, “I was eager to work on the feature because I personally felt antagonized and abused on the site (gender isn’t an unlikely reason as to why)... But if I hadn’t had that personal perspective, it’s possible that the Quora team wouldn’t have prioritized building a block button so early in its existence.” Harassment often drives people from marginalized groups off online platforms, so this functionality has been important for maintaining the health of Quora's community.\n",
    "\n",
    "\n",
    "多样性可以导致问题能被提前发现，并考虑更广泛的解决方案。例如，Tracy Chou是Quora的早期工程师。她 [写下了她的经历](https://qz.com/1016900/tracy-chou-leading-silicon-valley-engineer-explains-why-every-tech-worker-needs-a-humanities-education/ )，描述了她如何在内部倡导添加一项功能，以阻止钓鱼贴和其他不良行为者。Chou讲述道，“我渴望研究这个功能，因为我个人在网站上感到被激怒和辱骂 (性别是一个极可能的原因)。但是，如果我没有这种个人观点，Quora团队可能就不会在其成立之初就优先考虑构建阻止按钮。\"骚扰常常使边缘化群体的人们离开在线平台，因此此功能对于维持Quora社区的健康非常重要。\n",
    "\n",
    "A crucial aspect to understand is that women leave the tech industry at over twice the rate that men do, according to the [*Harvard Business Review*](https://www.researchgate.net/publication/268325574_By_RESEARCH_REPORT_The_Athena_Factor_Reversing_the_Brain_Drain_in_Science_Engineering_and_Technology) (41% of women working in tech leave, compared to 17% of men). An analysis of over 200 books, white papers, and articles found that the reason they leave is that “they’re treated unfairly; underpaid, less likely to be fast-tracked than their male colleagues, and unable to advance.” \n",
    "\n",
    "\n",
    "根据 [*《哈佛商业评论》*](https://www.researchgate.net/publication/268325574_By_RESEARCH_REPORT_The_Athena_Factor_Reversing_the_Brain_Drain_in_Science_Engineering_and_Technology )，要理解的一个关键方面是，女性离开科技行业的速度是男性的两倍以上，(41% 的女性休技术假，而男性为17%)。对200多种书籍、白皮书和文章的分析发现，她们之所以离开，是因为“她们受到了不公平的对待; 薪水不足，与男性同事相比，她们快速上手的可能性比较小，并且无法获得晋升。\"\n",
    "\n",
    "Studies have confirmed a number of the factors that make it harder for women to advance in the workplace. Women receive more vague feedback and personality criticism in performance evaluations, whereas men receive actionable advice tied to business outcomes (which is more useful). Women frequently experience being excluded from more creative and innovative roles, and not receiving high-visibility “stretch” assignments that are helpful in getting promoted. One study found that men’s voices are perceived as more persuasive, fact-based, and logical than women’s voices, even when reading identical scripts.\n",
    "\n",
    "\n",
    "研究证实了一些使妇女在工作场所更难晋升的因素。在绩效评估中，女性会收到更多模糊的反馈和个性批评，而男性会收到与业务成果相关的可行建议（这更有用）。女性经常被排除在更具创造性和创新性的职位之外，也得不到有助于晋升的高知名度“延展性”任务。一项研究发现，即使阅读相同的剧本，男人的声音也比女人的声音更具说服力，更基于事实和更符合逻辑。\n",
    "\n",
    "Receiving mentorship has been statistically shown to help men advance, but not women. The reason behind this is that when women receive mentorship, it’s advice on how they should change and gain more self-knowledge. When men receive mentorship, it’s public endorsement of their authority. Guess which is more useful in getting promoted?\n",
    "\n",
    "\n",
    "统计显示，接受指导有助于男性进步，但无助于女性。其背后的原因是，当女性获得指导时，就如何改变和获得更多自我知识提供了建议。而当男性接受指导时，这是对他们权威的公开认可。猜猜哪个更利于升职？\n",
    "\n",
    "As long as qualified women keep dropping out of tech, teaching more girls to code will not solve the diversity issues plaguing the field. Diversity initiatives often end up focusing primarily on white women, even though women of color face many additional barriers. In [interviews](https://worklifelaw.org/publications/Double-Jeopardy-Report_v6_full_web-sm.pdf) with 60 women of color who work in STEM research, 100% had experienced discrimination.\n",
    "\n",
    "只要有资质的女性不断退出技术领域，教更多的女孩编码将无法解决困扰该领域的多样性问题。尽管有色妇女面临许多其他障碍，但多样性倡议往往最终主要集中在白人妇女上。在对60名从事STEM研究的有色女性的[采访](https://worklifelaw.org/publications/Double-Jeopardy-Report_v6_full_web-sm.pdf )中，有100% 的人经历过歧视。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hiring process is particularly broken in tech. One study indicative of the disfunction comes from Triplebyte, a company that helps place software engineers in companies, conducting a standardized technical interview as part of this process. They have a fascinating dataset: the results of how over 300 engineers did on their exam, coupled with the results of how those engineers did during the interview process for a variety of companies. The number one finding from [Triplebyte’s research](https://triplebyte.com/blog/who-y-combinator-companies-want) is that “the types of programmers that each company looks for often have little to do with what the company needs or does. Rather, they reflect company culture and the backgrounds of the founders.”\n",
    "\n",
    "\n",
    "在科技领域，招聘过程尤其糟糕。一项来自Triplebyte的显示功能异常的研究，该公司帮助软件工程师进入公司，并在此过程中进行了标准化的技术面试。他们有一个有趣的数据集: 300多名工程师在考试中的表现，以及这些工程师在各种公司的面试过程中的表现。从 [Triplebyte的研究](https://triplebyte.com/blog/who-y-combinator-companies-want)中得出的第一个发现是 ““每个公司寻找的程序员的类型通常与公司的需求或做的事情没有多大关系。相反，反映了公司的文化和创始人的背景。\"\n",
    "\n",
    "This is a challenge for those trying to break into the world of deep learning, since most companies' deep learning groups today were founded by academics. These groups tend to look for people \"like them\"—that is, people that can solve complex math problems and understand dense jargon. They don't always know how to spot people who are actually good at solving real problems using deep learning.\n",
    "\n",
    "\n",
    "对于那些试图打入深度学习领域的人来说，这是一个挑战，因为当今大多数公司的深度学习小组都是由学者创建的。这些群体倾向于寻找 “像他们一样” 的人 -- 也就是说，能够解决复杂的数学问题并理解专业术语的人。他们并不总是知道如何找到真正擅长使用深度学习解决实际问题的人。\n",
    "\n",
    "This leaves a big opportunity for companies that are ready to look beyond status and pedigree, and focus on results!\n",
    "\n",
    "这给那些准备超越地位和背景，专注于结果的公司留下了一个巨大的机会!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness, Accountability, and Transparency\n",
    "\n",
    "### 公平性、问责制和透明度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The professional society for computer scientists, the ACM, runs a data ethics conference called the Conference on Fairness, Accountability, and Transparency. \"Fairness, Accountability, and Transparency\" which used to go under the acronym *FAT* but now uses to the less objectionable *FAccT*. Microsoft has a group focused on \"Fairness, Accountability, Transparency, and Ethics\" (FATE). In this section, we'll use \"FAccT\" to refer to the concepts of *Fairness, Accountability, and Transparency*.\n",
    "\n",
    "\n",
    "计算机科学家专业协会ACM举办了一个名为 “公平性、问责制和透明度会议” 的数据伦理会议。“公平、问责和透明”，过去用的是首字母缩略词 *FAT*，但现在用的是较不令人反感的 *FAccT*。微软有一个专注于 “公平、问责、透明和伦理” (FATE) 的小组。在本节中，我们将使用 “FAccT” 来代替 *公平性、问责制和透明度* 的概念。\n",
    "\n",
    "FAccT is another lens that you may find useful in considering ethical issues. One useful resource for this is the free online book [*Fairness and Machine Learning: Limitations and Opportunities*](https://fairmlbook.org/) by Solon Barocas, Moritz Hardt, and Arvind Narayanan, which \"gives a perspective on machine learning that treats fairness as a central concern rather than an afterthought.\" It also warns, however, that it \"is intentionally narrow in scope... A narrow framing of machine learning ethics might be tempting to technologists and businesses as a way to focus on technical interventions while sidestepping deeper questions about power and accountability. We caution against this temptation.\" Rather than provide an overview of the FAccT approach to ethics (which is better done in books such as that one), our focus here will be on the limitations of this kind of narrow framing.\n",
    "\n",
    "\n",
    "FAccT是你在考虑道德问题时可能会发现有用的另一个视角。一个有用的资源是免费的在线书籍 [*公平和机器学习: 局限和机遇*](https://fairmlbook.org/)，作者Solon Barocas, Moritz Hardt, 和Arvind Narayanan，他们“从机器学习的角度出发，将公平视为首要关注而不是事后考虑。”然而，他们也警告说，它 “故意缩小范围。机器学习伦理的狭隘框架可能会吸引技术人员和企业，将其作为侧重于技术干预的措施，同时回避关于权力和问责制的更深层次的问题的一种方式。我们要警惕这种诱惑。”与其提供FAccT伦理方法的概述（最好在诸如此类的书中进行介绍），不如说我们将重点放在这种狭窄框架的局限性上。\n",
    "\n",
    "One great way to consider whether an ethical lens is complete is to try to come up with an example where the lens and our own ethical intuitions give diverging results. Os Keyes, Jevan Hutson, and Meredith Durbin explored this in a graphic way in their paper [\"A Mulching Proposal:\n",
    "Analysing and Improving an Algorithmic System for Turning the Elderly into High-Nutrient Slurry\"](https://arxiv.org/abs/1908.06166). The paper's abstract says:\n",
    "\n",
    "考虑伦理视角是否完整的一个好方法是尝试想出一个例子，在这个例子中，视角和我们自己的伦理直觉给出了不同的结果。Os Keyes，Jevan Hutson和Meredith Durbin在他们的论文 [“覆盖提案:老年人高营养浆化算法系统的分析与改进”](https://arxiv.org/abs/1908.06166)的摘要中说:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> : The ethical implications of algorithmic systems have been much discussed in both HCI and the broader community of those interested in technology design, development and policy. In this paper, we explore the application of one prominent ethical framework - Fairness, Accountability, and Transparency - to a proposed algorithm that resolves various societal issues around food security and population aging. Using various standardised forms of algorithmic audit and evaluation, we drastically increase the algorithm's adherence to the FAT framework, resulting in a more ethical and beneficent system. We discuss how this might serve as a guide to other researchers or practitioners looking to ensure better ethical outcomes from algorithmic systems in their line of work.\n",
    "\n",
    ">: 在HCI以及对技术设计，开发和政策感兴趣的广大社区中，都对算法系统的伦理意义进行了广泛讨论。在本文中，我们探讨了一个突出的伦理框架 -- 公平性、问责制和透明度 -- 在解决围绕粮食安全和人口老龄化的各种社会问题的拟议算法中的应用。通过使用各种标准化形式的算法审计和评估，我们极大地提高了算法对FAT框架的依从性，从而建立了一个更具道德和良性的系统。我们讨论了如何作为其他研究人员或从业者的指南，以确保他们的工作中算法系统获得更好的伦理结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this paper, the rather controversial proposal (\"Turning the Elderly into High-Nutrient Slurry\") and the results (\"drastically increase the algorithm's adherence to the FAT framework, resulting in a more ethical and beneficent system\") are at odds... to say the least!\n",
    "\n",
    "\n",
    "在这篇论文中，相当有争议的建议 (“老年人高营养浆化”) 和结果 (“大大增加了算法对FAT框架的依从性，从而建立了一个更符合道德和良性的系统”)至少可以说...是不一致的!\n",
    "\n",
    "In philosophy, and especially philosophy of ethics, this is one of the most effective tools: first, come up with a process, definition, set of questions, etc., which is designed to resolve some problem. Then try to come up with an example where that apparent solution results in a proposal that no one would consider acceptable. This can then lead to a further refinement of the solution.\n",
    "\n",
    "\n",
    "在哲学中，尤其是伦理哲学中，这是最有效的工具之一: 首先，提出解决问题的流程，定义，问题集等。然后尝试举一个例子，该明显的解决方案导致提出了一个没人认为可以接受的提议。然后，这可以导致解决方案的进一步完善。\n",
    "\n",
    "So far, we've focused on things that you and your organization can do. But sometimes individual or organizational action is not enough. Sometimes, governments also need to consider policy implications.\n",
    "\n",
    "到目前为止，我们专注于你和你的组织可以做的事情。但有时个人或组织行动是不够的。有时，政府也需要考虑政策影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Role of Policy\n",
    "\n",
    "## 政策的作用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often talk to people who are eager for technical or design fixes to be a full solution to the kinds of problems that we've been discussing; for instance, a technical approach to debias data, or design guidelines for making technology less addictive. While such measures can be useful, they will not be sufficient to address the underlying problems that have led to our current state. For example, as long as it is incredibly profitable to create addictive technology, companies will continue to do so, regardless of whether this has the side effect of promoting conspiracy theories and polluting our information ecosystem. While individual designers may try to tweak product designs, we will not see substantial changes until the underlying profit incentives change.\n",
    "\n",
    "我们经常与渴望获得技术或设计修复的人员交谈，以全面解决我们一直在讨论的各种问题。例如，一种用于消除偏差的技术方法，或用于降低技术吸引力的设计指南。虽然这些措施可能是有用的，但不足以解决导致我们目前状况的根本问题。例如，只要创造令人上瘾的技术能够带来令人难以置信的利润，公司就会继续这样做，而不管这是否具有促进阴谋论和污染我们的信息生态系统的副作用。尽管个别设计师可能会尝试调整产品设计，但在潜在的利润激励发生变化之前，我们不会看到实质性的变化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Effectiveness of Regulation\n",
    "\n",
    "### 监管的效力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look at what can cause companies to take concrete action, consider the following two examples of how Facebook has behaved. In 2018, a UN investigation found that Facebook had played a “determining role” in the ongoing genocide of the Rohingya, an ethnic minority in Mynamar described by UN Secretary-General Antonio Guterres as \"one of, if not the, most discriminated people in the world.\" Local activists had been warning Facebook executives that their platform was being used to spread hate speech and incite violence since as early as 2013. In 2015, they were warned that Facebook could play the same role in Myanmar that the radio broadcasts played during the Rwandan genocide (where a million people were killed). Yet, by the end of 2015, Facebook only employed four contractors that spoke Burmese. As one person close to the matter said, \"That’s not 20/20 hindsight. The scale of this problem was significant and it was already apparent.\" Zuckerberg promised during the congressional hearings to hire \"dozens\" to address the genocide in Myanmar (in 2018, years after the genocide had begun, including the destruction by fire of at least 288 villages in northern Rakhine state after August 2017).\n",
    "\n",
    "\n",
    "要了解是什么导致公司采取具体行动，请考虑以下两个脸书行为的例子。2018年，联合国的一项调查发现，脸书在正在进行的罗兴亚人种族灭绝中起着“决定性作用”。罗兴亚人是缅甸的少数族裔，联合国秘书长安东尼奥·古特雷斯（Antonio Guterres）将其描述为“世界上最受歧视的人之一。”当地活动人士一直在警告脸书高管，自2013年以来，他们的平台就一直被用来传播仇恨言论和煽动暴力。2015年，他们被警告说脸书在缅甸可能起卢旺达种族灭绝大屠杀期间广播一样的作用（有一百万人被杀）。然而，到2015年底，脸书仅雇佣了四名说缅甸语的承包商。正如一位知情人士所说，“这不是事后诸葛亮。这个问题的规模很大，而且已经很明显了。\"扎克伯格在国会听证会上承诺雇佣 “数十人” 来解决缅甸的种族灭绝问题（在种族灭绝开始数年后的2018年，包括在2017年8月之后毁灭若开邦北部至少288个村庄的大火）。\n",
    "\n",
    "This stands in stark contrast to Facebook quickly [hiring 1,200 people in Germany](http://thehill.com/policy/technology/361722-facebook-opens-second-german-office-to-comply-with-hate-speech-law) to try to avoid expensive penalties (of up to 50 million euros) under a new German law against hate speech. Clearly, in this case, Facebook was more reactive to the threat of a financial penalty than to the systematic destruction of an ethnic minority.\n",
    "\n",
    "\n",
    "这与脸书迅速[在德国雇佣1,200人](http://thehill.com/policy/technology/361722-facebook-opens-second-german-office-to-comply-with-hate-speech-law )，以避免根据新的针对仇恨言论的德国法律避免昂贵的罚款（最高5000万欧元）形成了鲜明对比。显然，在这种情况下，Facebook对经济惩罚的威胁比对一个少数族裔的系统性破坏的反应更大。\n",
    "\n",
    "In an [article on privacy issues](https://idlewords.com/2019/06/the_new_wilderness.htm), Maciej Ceglowski draws parallels with the environmental movement: \n",
    "\n",
    "\n",
    "在一篇 [关于隐私问题的文章](https://idlewords.com/2019/06/the_new_wilderness.htm)，Maciej Ceglowski将其与环保运动相提并论:\n",
    "\n",
    "> : This regulatory project has been so successful in the First World that we risk forgetting what life was like before it. Choking smog of the kind that today kills thousands in Jakarta and Delhi was https://en.wikipedia.org/wiki/Pea_soup_fog[once emblematic of London]. The Cuyahoga River in Ohio used to http://www.ohiohistorycentral.org/w/Cuyahoga_River_Fire[reliably catch fire]. In a particularly horrific example of unforeseen consequences, tetraethyl lead added to gasoline https://en.wikipedia.org/wiki/Lead%E2%80%93crime_hypothesis[raised violent crime rates] worldwide for fifty years. None of these harms could have been fixed by telling people to vote with their wallet, or carefully review the environmental policies of every company they gave their business to, or to stop using the technologies in question. It took coordinated, and sometimes highly technical, regulation across jurisdictional boundaries to fix them. In some cases, like the https://en.wikipedia.org/wiki/Montreal_Protocol[ban on commercial refrigerants] that depleted the ozone layer, that regulation required a worldwide consensus. We’re at the point where we need a similar shift in perspective in our privacy law.\n",
    "\n",
    ">: 这个监管项目在第一世界非常成功，以至于我们有可能忘记之前的生活。今天在雅加达和德里造成数千人死亡的那种令人窒息的烟雾是 [曾经是伦敦的象征](https://en.wikipedia.org/wiki/Pea_soup_fog)。俄亥俄州的凯霍加河曾经[确实着火](http://www.ohiohistorycentral.org/w/Cuyahoga_River_Fire)。在无法预料的后果的一个特别恐怖的例子中，五十年来，四乙基铅被添加到汽油中，[提高了暴力犯罪率](https://en.wikipedia.org/wiki/Lead%E2%80%93crime_hypothesis)。告诉人们用钱包投票，或仔细检查与他们业务有关的每家公司的环境政策，或停止使用有问题的技术，都无法消除这些危害。需要跨越司法管辖区的协调监管，有时甚至是高度技术性的监管来解决这些问题。在某些情况下，比如[禁止商业制冷剂](https://en.wikipedia.org/wiki/Montreal_Protocol) 消耗臭氧层，该法规需要全球范围内的共识。我们正处于需要在隐私法方面进行类似转变的时候。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rights and Policy\n",
    "\n",
    "### 权利和政策"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean air and clean drinking water are public goods which are nearly impossible to protect through individual market decisions, but rather require coordinated regulatory action. Similarly, many of the harms resulting from unintended consequences of misuses of technology involve public goods, such as a polluted information environment or deteriorated ambient privacy. Too often privacy is framed as an individual right, yet there are societal impacts to widespread surveillance (which would still be the case even if it was possible for a few individuals to opt out).\n",
    "\n",
    "\n",
    "清洁的空气和清洁的饮用水是公共产品，几乎不可能通过单个的市场决策来保护，而是需要采取协调一致的监管行动。同样，由于滥用技术而产生意料之外的后果而造成的许多危害也涉及公共物品，例如污染的信息环境或恶化的环境隐私。隐私通常被定性为个人权利，但对广泛监视仍会产生社会影响（即使少数个人可能退出，情况仍会如此）。\n",
    "\n",
    "Many of the issues we are seeing in tech are actually human rights issues, such as when a biased algorithm recommends that Black defendants have longer prison sentences, when particular job ads are only shown to young people, or when police use facial recognition to identify protesters. The appropriate venue to address human rights issues is typically through the law.\n",
    "\n",
    "\n",
    "我们在技术中看到的许多问题实际上都是人权问题，例如当偏见算法建议黑人被告人被判更长的徒刑，特定的招聘广告仅向年轻人展示或警察使用面部识别来识别抗议者时。解决人权问题的适当方法通常是通过法律。\n",
    "\n",
    "We need both regulatory and legal changes, *and* the ethical behavior of individuals. Individual behavior change can’t address misaligned profit incentives, externalities (where corporations reap large profits while offloading their costs and harms to the broader society), or systemic failures. However, the law will never cover all edge cases, and it is important that individual software developers and data scientists are equipped to make ethical decisions in practice.\n",
    "\n",
    "我们需要监管和法律的变化，*以及* 个人的道德行为。个体行为的改变无法解决错位的利润激励机制，外部性（企业在获取巨额利润的同时却不承担成本和对整个社会的损害）或系统性失灵的问题。但是，法律永远不会涵盖所有极端情况，因此重要的是，各个软件开发人员和数据科学家必须具备在实践中做出伦理决策的能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cars: A Historical Precedent\n",
    "\n",
    "### 汽车: 历史先例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problems we are facing are complex, and there are no simple solutions. This can be discouraging, but we find hope in considering other large challenges that people have tackled throughout history. One example is the movement to increase car safety, covered as a case study in [\"Datasheets for Datasets\"](https://arxiv.org/abs/1803.09010) by Timnit Gebru et al. and in the design podcast [99% Invisible](https://99percentinvisible.org/episode/nut-behind-wheel/). Early cars had no seatbelts, metal knobs on the dashboard that could lodge in people’s skulls during a crash, regular plate glass windows that shattered in dangerous ways, and non-collapsible steering columns that impaled drivers. However, car companies were incredibly resistant to even discussing the idea of safety as something they could help address, and the widespread belief was that cars are just the way they are, and that it was the people using them who caused problems.\n",
    "\n",
    "\n",
    "我们面临的问题是复杂的，没有简单的解决办法。这可能令人沮丧，但我们在考虑人们在历史上应对的其他重大挑战时找到了希望。Timnit Gebru等人在 [\"数据集的数据表\"](https://arxiv.org/abs/1803.09010)以及在设计博客[99％不可见](https://99percentinvisible.org/episode/nut-behind-wheel/) 中进行了案例研究，其中包括提高汽车安全性的运动。早期的汽车没有安全带，仪表板上的金属旋钮在发生碰撞时可能会藏在人们的头盖骨中，规则的平板玻璃窗以危险的方式破碎，并且转向柱不可折叠会刺伤驾驶员。然而，汽车公司极度抵制甚至无法讨论他们可以帮助解决的安全性理念，并且普遍认为，汽车就是这样的，而使用它们的人才是造成问题的原因。\n",
    "\n",
    "It took consumer safety activists and advocates decades of work to even change the national conversation to consider that perhaps car companies had some responsibility which should be addressed through regulation. When the collapsible steering column was invented, it was not implemented for several years as there was no financial incentive to do so. Major car company General Motors hired private detectives to try to dig up dirt on consumer safety advocate Ralph Nader. The requirement of seatbelts, crash test dummies, and collapsible steering columns were major victories. It was only in 2011 that car companies were required to start using crash test dummies that would represent the average woman, and not just average men’s bodies; prior to this, women were 40% more likely to be injured in a car crash of the same impact compared to a man. This is a vivid example of the ways that bias, policy, and technology have important consequences.\n",
    "\n",
    "消费者安全活动人士和倡导者花了几十年的时间才改变全国范围内的讨论，认为汽车公司可能负有某些责任，应该通过监管来解决。当可折叠转向柱被发明时，由于没有经济诱因，好几年没有被应用。大型汽车公司通用汽车曾雇佣私人侦探试图挖掘消费者安全倡导者Ralph Nader的丑闻。安全带、碰撞测试假人和可折叠转向柱的要求是主要的胜利。直到2011年，汽车公司才被要求开始使用代表普通女性的碰撞测试假人，而不仅仅是普通男性的身体;在此之前，与男性相比，在相同碰撞事故中女性受伤的可能性要高40％。这是偏见、政策和技术产生重要后果的一个生动例子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "## 结论"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coming from a background of working with binary logic, the lack of clear answers in ethics can be frustrating at first.  Yet, the implications of how our work impacts the world, including unintended consequences and the work becoming weaponized by bad actors, are some of the most important questions we can (and should!) consider.  Even though there aren't any easy answers, there are definite pitfalls to avoid and practices to follow to move toward more ethical behavior.\n",
    "\n",
    "\n",
    "从使用二进制逻辑的背景出发，伦理学缺乏明确的答案可能一开始令人沮丧。然而，我们的工作如何影响世界的含义，包括意想不到的后果以及工作被坏行为者武器化，是我们可以（并且应该！）考虑的一些最重要的问题。即使没有简单的答案，也要避免一定的陷阱，并遵循一些实践以朝着更道德的行为迈进。\n",
    "\n",
    "Many people (including us!) are looking for more satisfying, solid answers about how to address harmful impacts of technology. However, given the complex, far-reaching, and interdisciplinary nature of the problems we are facing, there are no simple solutions. Julia Angwin, former senior reporter at ProPublica who focuses on issues of algorithmic bias and surveillance (and one of the 2016 investigators of the COMPAS recidivism algorithm that helped spark the field of FAccT) said in [a 2019 interview](https://www.fastcompany.com/90337954/who-cares-about-liberty-julia-angwin-and-trevor-paglen-on-privacy-surveillance-and-the-mess-were-in):\n",
    "\n",
    "\n",
    "许多人 (包括我们!) 正在寻找关于如何解决技术有害影响的更令人满意、更可靠的答案。然而，鉴于我们面临的问题的复杂、深远和跨学科性质，没有简单的解决办法。Julia Angwin，ProPublica的前高级记者，专注于算法偏差和监控问题 (也是2016年COMPAS累犯算法的研究者之一，该算法帮助激发了FAccT领域) 在[2019年的一场访谈](https://www.fastcompany.com/90337954/who-cares-about-liberty-julia-angwin-and-trevor-paglen-on-privacy-surveillance-and-the-mess-were-in) 中说 :\n",
    "\n",
    "> : I strongly believe that in order to solve a problem, you have to diagnose it, and that we’re still in the diagnosis phase of this. If you think about the turn of the century and industrialization, we had, I don’t know, 30 years of child labor, unlimited work hours, terrible working conditions, and it took a lot of journalist muckraking and advocacy to diagnose the problem and have some understanding of what it was, and then the activism to get laws changed. I feel like we’re in a second industrialization of data information... I see my role as trying to make as clear as possible what the downsides are, and diagnosing them really accurately so that they can be solvable. That’s hard work, and lots more people need to be doing it. \n",
    "\n",
    "\n",
    ">: 我坚信，为了解决一个问题，你必须对其进行诊断，我们仍处于诊断阶段。如果你想想世纪之交和工业化，我不确定，我们有30年的童工，无限的工作时间，糟糕的工作条件，花了很多记者揭发丑闻和宣传来诊断这个问题，并对它有所了解，然后是改变法律的行动主义。我觉得我们正处于数据信息的第二次工业化.我认为我的角色是试图尽可能清楚地说明缺点是什么，并真正准确地诊断它们，以便它们可以解决。这是一项艰苦的工作，需要更多的人去做。\n",
    "\n",
    "It's reassuring that Angwin thinks we are largely still in the diagnosis phase: if your understanding of these problems feels incomplete, that is normal and natural. Nobody has a “cure” yet, although it is vital that we continue working to better understand and address the problems we are facing.\n",
    "\n",
    "\n",
    "令人欣慰的是，Angwin认为我们大部分仍处于诊断阶段: 如果你对这些问题的理解不完整，那是正常和自然的。还没有人可以“解决问题”，尽管至关重要的是，我们必须继续努力，以更好地理解和解决我们面临的问题。\n",
    "\n",
    "One of our reviewers for this book, Fred Monroe, used to work in hedge fund trading. He told us, after reading this chapter, that many of the issues discussed here (distribution of data being dramatically different than what a model was trained on, the impact feedback loops on a model once deployed and at scale, and so forth) were also key issues for building profitable trading models. The kinds of things you need to do to consider societal consequences are going to have a lot of overlap with things you need to do to consider organizational, market, and customer consequences—so thinking carefully about ethics can also help you think carefully about how to make your data product successful more generally!\n",
    "\n",
    "我们这本书的评论家之一Fred Monroe曾在对冲基金交易领域工作。读完这一章后，他告诉我们，这里讨论的许多问题 (数据的分配与模型的训练有很大的不同，一旦模型被大规模部署，影响反馈就会循环到模型上，依此类推) 也是建立有利可图的交易模型的关键问题。考虑社会后果所需要做的事情与考虑组织，市场和客户后果需要做的事情有很多重叠 -- 因此，仔细思考道德规范也可以帮助你认真思考如何去让你的数据产品获得更广泛的成功！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questionnaire\n",
    "\n",
    "## 问卷调查"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Does ethics provide a list of \"right answers\"?\n",
    "1. How can working with people of different backgrounds help when considering ethical questions?\n",
    "1. What was the role of IBM in Nazi Germany? Why did the company participate as it did? Why did the workers participate?\n",
    "1. What was the role of the first person jailed in the Volkswagen diesel scandal?\n",
    "1. What was the problem with a database of suspected gang members maintained by California law enforcement officials?\n",
    "1. Why did YouTube's recommendation algorithm recommend videos of partially clothed children to pedophiles, even though no employee at Google had programmed this feature?\n",
    "1. What are the problems with the centrality of metrics?\n",
    "1. Why did Meetup.com not include gender in its recommendation system for tech meetups?\n",
    "1. What are the six types of bias in machine learning, according to Suresh and Guttag?\n",
    "1. Give two examples of historical race bias in the US.\n",
    "1. Where are most images in ImageNet from?\n",
    "1. In the paper [\"Does Machine Learning Automate Moral Hazard and Error\"](https://scholar.harvard.edu/files/sendhil/files/aer.p20171084.pdf) why is sinusitis found to be predictive of a stroke?\n",
    "1. What is representation bias?\n",
    "1. How are machines and people different, in terms of their use for making decisions?\n",
    "1. Is disinformation the same as \"fake news\"?\n",
    "1. Why is disinformation through auto-generated text a particularly significant issue?\n",
    "1. What are the five ethical lenses described by the Markkula Center?\n",
    "1. Where is policy an appropriate tool for addressing data ethics issues?\n",
    "\n",
    "\n",
    "1. 伦理道德是否提供“正确答案”的列表？\n",
    "1. 在考虑伦理问题时，如何与不同背景的人一起工作？\n",
    "1. IBM在纳粹德国扮演什么角色？为什么公司会像这样参与？工人为什么会参加？\n",
    "1. 在大众柴油丑闻中入狱的第一个人的角色是什么？\n",
    "1. 加州执法官员维护的涉嫌帮派成员数据库有什么问题？\n",
    "1. 为什么油管的推荐算法会推荐部分穿衣服的孩子的视频给恋童癖者，即使谷歌的员工都没有对此功能进行过编程？\n",
    "1. 指标的中心性有什么问题？\n",
    "1. 为什么Meetup.com的科技会议推荐系统中没有包含性别？\n",
    "1. 根据Suresh和Guttag的说法，机器学习中的六种偏见是什么？\n",
    "1. 举两个美国历史种族偏见的例子。\n",
    "1. ImageNet中的大多数图像来自哪里？\n",
    "1. 在论文 [《机器学习是否自动化了道德风险和错误》](https://scholar.harvard.edu/files/sendhil/files/aer.p20171084.pdf)中，为什么发现鼻窦炎可以预测中风？\n",
    "1. 什么是表征偏见？\n",
    "1. 机器和人在决策方面有什么不同？\n",
    "1. 虚假信息和 “假新闻” 一样吗？\n",
    "1. 为什么通过自动生成的文本进行虚假信息是一个特别重要的问题？\n",
    "1. 马库拉中心描述的五个伦理视角是什么？\n",
    "1. 政策在哪里是解决数据伦理问题的合适工具？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Research:\n",
    "\n",
    "### 进一步研究:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read the article \"What Happens When an Algorithm Cuts Your Healthcare\". How could problems like this be avoided in the future?\n",
    "1. Research to find out more about YouTube's recommendation system and its societal impacts. Do you think recommendation systems must always have feedback loops with negative results? What approaches could Google take to avoid them? What about the government?\n",
    "1. Read the paper [\"Discrimination in Online Ad Delivery\"](https://arxiv.org/abs/1301.6822). Do you think Google should be considered responsible for what happened to Dr. Sweeney? What would be an appropriate response?\n",
    "1. How can a cross-disciplinary team help avoid negative consequences?\n",
    "1. Read the paper \"Does Machine Learning Automate Moral Hazard and Error\". What actions do you think should be taken to deal with the issues identified in this paper?\n",
    "1. Read the article \"How Will We Prevent AI-Based Forgery?\" Do you think Etzioni's proposed approach could work? Why?\n",
    "1. Complete the section \"Analyze a Project You Are Working On\" in this chapter.\n",
    "1. Consider whether your team could be more diverse. If so, what approaches might help?\n",
    "\n",
    "\n",
    "1. 阅读文章 《当算法削减你的医疗保健时会发生什么》。将来如何避免发生这样的问题？\n",
    "1. 研究以了解更多关于YouTube推荐系统及其社会影响的信息。你是否认为推荐系统必须总是带有负面结果的反馈循环？谷歌可以采取什么方法来避免它们？政府呢？\n",
    "1. 阅读论文 [《网络广告投放中的歧视》](https://arxiv.org/abs/1301.6822)。你认为谷歌应该对Sweeney博士的遭遇负责吗？什么是适当的回应？\n",
    "1. 跨学科团队如何帮助避免负面后果？\n",
    "1. 阅读论文《机器学习是否自动化了道德风险和错误》。你认为应该采取什么行动来处理本文中指出的问题？\n",
    "1. 阅读文章 《我们如何防止基于人工智能的伪造？》 你认为Etzioni提出的方法可行吗？为什么？\n",
    "1. 完成本章中的 《分析你正在进行的项目》 一节。\n",
    "1. 考虑一下您的团队是否可以更多样化。 如果可以，什么方法可能会有所帮助？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning in Practice: That's a Wrap!\n",
    "\n",
    "## 实践中的深度学习: 完成了!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You've made it to the end of the first section of the book. In this section we've tried to show you what deep learning can do, and how you can use it to create real applications and products. At this point, you will get a lot more out of the book if you spend some time trying out what you've learned. Perhaps you have already been doing this as you go along—in which case, great! If not, that's no problem either... Now is a great time to start experimenting yourself.\n",
    "\n",
    "\n",
    "恭喜！你已经完成了这本书第一部分的结尾。在本节中，我们试图向你展示深度学习可以做什么，以及如何使用它来创建真正的应用程序和产品。在这一点上，如果你花一些时间尝试你所学到的东西，你将会有更多的收获。也许你已经这样做了 -- 如果是这样，就太好了！如果没有，那也没问题...现在是开始自己尝试的好时机。\n",
    "\n",
    "If you haven't been to the [book's website](https://book.fast.ai) yet, head over there now. It's really important that you get yourself set up to run the notebooks. Becoming an effective deep learning practitioner is all about practice, so you need to be training models. So, please go get the notebooks running now if you haven't already! And also have a look on the website for any important updates or notices; deep learning changes fast, and we can't change the words that are printed in this book, so the website is where you need to look to ensure you have the most up-to-date information.\n",
    "\n",
    "\n",
    "如果你还没有去过 [图书网站](https://book.fast.ai)，现在就去吧。做好运行笔记本的准备工作非常重要。成为一名有效的深度学习从业者与实践息息相关，因此你需要训练模型。所以，如果尚未开始运行笔记本，请立即开始！也可以在网站上查看任何重要的更新或通知; 深度学习日新月异，我们不能改变这本书里印的单词，因此，你需要在网站上查找，以确保你拥有最新的信息。\n",
    "\n",
    "Make sure that you have completed the following steps:\n",
    "\n",
    "\n",
    "确保你已完成以下步骤:\n",
    "\n",
    "- Connect to one of the GPU Jupyter servers recommended on the book's website.\n",
    "- Run the first notebook yourself.\n",
    "- Upload an image that you find in the first notebook; then try a few different images of different kinds to see what happens.\n",
    "- Run the second notebook, collecting your own dataset based on image search queries that you come up with.\n",
    "- Think about how you can use deep learning to help you with your own projects, including what kinds of data you could use, what kinds of problems may come up, and how you might be able to mitigate these issues in practice.\n",
    "\n",
    "\n",
    "- 连接到本书网站上推荐的Jupyter显卡服务器之一。\n",
    "- 自己运行第一个笔记本。\n",
    "- 上传你在第一个笔记本中找到的图像; 然后尝试不同种类的不同图像，看看会发生什么。\n",
    "- 运行第二个笔记本，根据你提出的图像搜索查询收集你自己的数据集。\n",
    "- 想想如何使用深度学习来帮助你完成自己的项目，包括你可以使用哪些类型的数据，可能会出现哪些类型的问题，以及如何在实践中减轻这些问题。\n",
    "\n",
    "In the next section of the book you will learn about how and why deep learning works, instead of just seeing how you can use it in practice. Understanding the how and why is important for both practitioners and researchers, because in this fairly new field nearly every project requires some level of customization and debugging. The better you understand the foundations of deep learning, the better your models will be. These foundations are less important for executives, product managers, and so forth (although still useful, so feel free to keep reading!), but they are critical for anybody who is actually training and deploying models themselves.\n",
    "\n",
    "在这本书的下一部分，你将了解深度学习是如何以及为什么工作的，而不仅仅是看看如何在实践中使用它。理解如何以及为什么对从业者和研究者都很重要，因为在这个相当新的领域中，几乎每个项目都需要一定程度的定制和调试。你对深度学习的基础理解得越好，你的模型就会越好。这些基础对高管、产品经理等而言不那么重要 (尽管仍然有用，所以请随时继续阅读!)，但是对于任何实际自己培训和部署模型的人来说，它们都是至关重要的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
