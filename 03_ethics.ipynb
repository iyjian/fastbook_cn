{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "!pip install -Uqq fastbook\n",
    "import fastbook\n",
    "fastbook.setup_book()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[[chapter_ethics]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ethics\n",
    "\n",
    "# 数据伦理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sidebar: Acknowledgement: Dr. Rachel Thomas\n",
    "\n",
    "### 侧边栏: 感谢: 雷切尔 · 托马斯博士"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter was co-authored by Dr. Rachel Thomas, the cofounder of fast.ai, and founding director of the Center for Applied Data Ethics at the University of San Francisco. It largely follows a subset of the syllabus she developed for the [Introduction to Data Ethics](https://ethics.fast.ai) course.\n",
    "\n",
    "本章由fast.ai的联合创始人、旧金山大学应用数据伦理中心的创始主任Rachel Thomas博士共同撰写。本章在很大程度上遵循了她为 [数据伦理导论](https://ethics.fast.ai) 课程开发的教学大纲的子集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End sidebar\n",
    "\n",
    "### 结束侧边栏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed in Chapters 1 and 2, sometimes machine learning models can go wrong. They can have bugs. They can be presented with data that they haven't seen before, and behave in ways we don't expect. Or they could work exactly as designed, but be used for something that we would much prefer they were never, ever used for.\n",
    "\n",
    "正如我们在第1章和第2章中讨论的那样，有时机器学习模型可能会出错。他们可能有错误。可以为它们提供以前从未见过的数据，并以我们意想不到的方式表现。或者它们可以完全按设计工作，但可以用于我们更希望它们从未使用过的东西。\n",
    "\n",
    "Because deep learning is such a powerful tool and can be used for so many things, it becomes particularly important that we consider the consequences of our choices. The philosophical study of *ethics* is the study of right and wrong, including how we can define those terms, recognize right and wrong actions, and understand the connection between actions and consequences. The field of *data ethics* has been around for a long time, and there are many academics focused on this field. It is being used to help define policy in many jurisdictions; it is being used in companies big and small to consider how best to ensure good societal outcomes from product development; and it is being used by researchers who want to make sure that the work they are doing is used for good, and not for bad.\n",
    "\n",
    "\n",
    "因为深度学习是一个如此强大的工具，可以用于很多事情，所以考虑我们选择的后果变得尤为重要。*伦理学* 的哲学研究是对与错的研究，包括我们如何定义那些术语，认识对与错的行动，理解行动与后果之间的联系。*数据伦理* 这个领域已经存在很长时间了，有很多学者关注这个领域。它被用来帮助定义许多司法管辖区的政策; 它被用于大大小小的公司，以考虑如何最好地确保产品开发的良好社会成果; 研究人员使用它，希望确保他们正在做的工作是好事，而不是坏事。\n",
    "\n",
    "As a deep learning practitioner, therefore, it is likely that at some point you are going to be put in a situation where you need to consider data ethics. So what is data ethics? It's a subfield of ethics, so let's start there.\n",
    "\n",
    "因此，作为一名深度学习从业者，很可能在某个时候你会陷入需要考虑数据伦理的境地。那么什么是数据伦理呢？这是伦理学的一个分支，所以让我们从这里开始。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> J: At university, philosophy of ethics was my main thing (it would have been the topic of my thesis, if I'd finished it, instead of dropping out to join the real world). Based on the years I spent studying ethics, I can tell you this: no one really agrees on what right and wrong are, whether they exist, how to spot them, which people are good, and which bad, or pretty much anything else. So don't expect too much from the theory! We're going to focus on examples and thought starters here, not theory.\n",
    "\n",
    "> J: 在大学里，伦理哲学是我的主要研究内容 (如果我完成了，它将是我论文的主题，而不是辍学加入现实世界)。根据我花在伦理学上的时间，我可以告诉你: 没有人真正同意什么是对的，什么是错的，它们是否存在，如何发现它们，哪些人是好的，哪些人不好，或者几乎其他什么。所以不要对理论期望过高！我们将在这里重点关注示例和思想开创者，而不是理论。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In answering the question [\"What Is Ethics\"](https://www.scu.edu/ethics/ethics-resources/ethical-decision-making/what-is-ethics/), The Markkula Center for Applied Ethics says that the term refers to:\n",
    "\n",
    "\n",
    "在回答 [“什么是伦理”](https://www.scu.edu/ethics/ethics-resources/ethical-decision-making/what-is-ethics/)的问题时，Markkula应用伦理中心表示，该术语指的是:\n",
    "\n",
    "- Well-founded standards of right and wrong that prescribe what humans ought to do\n",
    "- The study and development of one's ethical standards.\n",
    "\n",
    "\n",
    "-正确的对与错标准规定了人类应该做什么\n",
    "-研究和发展个人伦理标准。\n",
    "\n",
    "There is no list of right answers. There is no list of do and don't. Ethics is complicated, and context-dependent. It involves the perspectives of many stakeholders. Ethics is a muscle that you have to develop and practice. In this chapter, our goal is to provide some signposts to help you on that journey.\n",
    "\n",
    "\n",
    "没有正确答案的列表。没有可做和不可做的清单。伦理是复杂的，并且依赖于上下文环境。它涉及许多利益攸关方的观点。伦理是你必须发展和实践的力量。在本章中，我们的目标是提供一些路标来帮助你踏上旅程。\n",
    "\n",
    "Spotting ethical issues is best to do as part of a collaborative team. This is the only way you can really incorporate different perspectives. Different people's backgrounds will help them to see things which may not be obvious to you. Working with a team is helpful for many \"muscle-building\" activities, including this one.\n",
    "\n",
    "\n",
    "最好将伦理问题作为合作团队的一部分来做。这是你真正融入不同观点的唯一方法。不同人的背景会帮助他们看到对你来说可能不明显的东西。与团队合作有助于许多 “力量锻炼” 活动，包括这项活动。\n",
    "\n",
    "This chapter is certainly not the only part of the book where we talk about data ethics, but it's good to have a place where we focus on it for a while. To get oriented, it's perhaps easiest to look at a few examples. So, we picked out three that we think illustrate effectively some of the key topics.\n",
    "\n",
    "本当然不是本书中讨论数据伦理的唯一部分，但是最好有一段时间让我们专注于此。为了找明方向，看几个例子也许最容易。所以，我们挑选了三个我们认为可以有效说明一些关键主题的东西。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Examples for Data Ethics\n",
    "\n",
    "## 数据伦理的关键示例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to start with three specific examples that illustrate three common ethical issues in tech:\n",
    "\n",
    "\n",
    "我们将从三个具体的例子开始，说明技术中的三个常见伦理问题:\n",
    "\n",
    "1.  *Recourse processes*—Arkansas's buggy healthcare algorithms left patients stranded.\n",
    "2.  *Feedback loops*—YouTube's recommendation system helped unleash a conspiracy theory boom.\n",
    "3.  *Bias*—When a traditionally African-American name is searched for on Google, it displays ads for criminal background checks.\n",
    "\n",
    "\n",
    "1. *求助程序* - 阿肯色州的错误医疗算法让患者陷入困境。\n",
    "2. *反馈循环* - YouTube的推荐系统帮助释放了阴谋论热潮。\n",
    "3. *偏见* - 当在谷歌上搜索传统上属于非裔美国人的名字时，它会显示用于犯罪背景调查的广告。\n",
    "\n",
    "In fact, for every concept that we introduce in this chapter, we are going to provide at least one specific example. For each one, think about what you could have done in this situation, and what kinds of obstructions there might have been to you getting that done. How would you deal with them? What would you look out for?\n",
    "\n",
    "事实上，对于我们在本章中介绍的每个概念，我们将至少提供一个具体的例子。对于每个例子，请考虑一下在这种情况下你可以做些什么，以及你可能会遇到什么样的障碍。你将如何处理它们？你会寻求什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bugs and Recourse: Buggy Algorithm Used for Healthcare Benefits\n",
    "\n",
    "### Bug和求助: 错误算法被应用于医疗保健福利"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Verge investigated software used in over half of the US states to determine how much healthcare people receive, and documented their findings in the article [\"What Happens When an Algorithm Cuts Your Healthcare\"](https://www.theverge.com/2018/3/21/17144260/healthcare-medicaid-algorithm-arkansas-cerebral-palsy). After implementation of the algorithm in Arkansas, hundreds of people (many with severe disabilities) had their healthcare drastically cut. For instance, Tammy Dobbs, a woman with cerebral palsy who needs an aid to help her to get out of bed, to go to the bathroom, to get food, and more, had her hours of help suddenly reduced by 20 hours a week. She couldn’t get any explanation for why her healthcare was cut. Eventually, a court case revealed that there were mistakes in the software implementation of the algorithm, negatively impacting people with diabetes or cerebral palsy. However, Dobbs and many other people reliant on these healthcare benefits live in fear that their benefits could again be cut suddenly and inexplicably.\n",
    "\n",
    "The Verge调查了在美国超过一半的州使用的软件，以确定人们获得了多少医疗保健，并在文章 [“当算法削减你的医疗保健时会发生什么”](https://www.theverge.com/2018/3/21/17144260/healthcare-medicaid-algorithm-arkansas-cerebral-palsy)。在阿肯色州实施该算法后，数百人 (许多患有严重残疾的人) 的医疗保健大幅削减。例如，Tammy Dobbs，一个患有脑瘫的女人，她需要人来帮助她起床、上厕所、获取食物等等，她的帮助时间突然每周减少了20小时。对于为什么她的医疗保健被削减了，她没有获得任何解释。最终，一个法庭案例显示该算法的软件实施存在错误，对糖尿病或脑瘫患者产生了负面影响。然而，Dobbs和许多其他依赖这些医疗保健福利的人生活在担心之中，他们的福利可能会突然又莫名其妙地被削减。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback Loops: YouTube's Recommendation System\n",
    "\n",
    "### 反馈循环: YouTube的推荐系统"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feedback loops can occur when your model is controlling the next round of data you get. The data that is returned quickly becomes flawed by the software itself.\n",
    "\n",
    "\n",
    "当你的模型控制你获得的下一轮数据时，可能会出现反馈循环。快速返回的数据已被软件本身破坏。\n",
    "\n",
    "For instance, YouTube has 1.9 billion users, who watch over 1 billion hours of YouTube videos a day. Its recommendation algorithm (built by Google), which was designed to optimize watch time, is responsible for around 70% of the content that is watched. But there was a problem: it led to out-of-control feedback loops, leading the *New York Times* to run the headline [\"YouTube Unleashed a Conspiracy Theory Boom. Can It Be Contained?\"](https://www.nytimes.com/2019/02/19/technology/youtube-conspiracy-stars.html). Ostensibly recommendation systems are predicting what content people will like, but they also have a lot of power in determining what content people even see.\n",
    "\n",
    "例如，YouTube拥有19亿用户，每天观看超过10亿小时的YouTube视频。它的推荐算法 (由谷歌构建) 旨在优化观看时间，负责大约70% 的观看内容。但是有一个问题: 它导致了失控的反馈循环，因此《纽约时报》刊登了标题为 [“YouTube引发了阴谋论热潮。可以遏制它？”](https://www.nytimes.com/2019/02/19/technology/youtube-conspiracy-stars.html)的文章。表面上的推荐系统正在预测人们会喜欢什么内容，但是它们在决定人们甚至看到什么内容方面也具有强大的功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias: Professor Lantanya Sweeney \"Arrested\"\n",
    "\n",
    "### 偏见: Lantanya Sweeney教授 “被捕”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dr. Latanya Sweeney is a professor at Harvard and director of the university's data privacy lab. In the paper [\"Discrimination in Online Ad Delivery\"](https://arxiv.org/abs/1301.6822) (see <<lantanya_arrested>>) she describes her discovery that Googling her name resulted in advertisements saying \"Latanya Sweeney, arrested?\" even though she is the only known Latanya Sweeney and has never been arrested. However when she Googled other names, such as \"Kirsten Lindquist,\" she got more neutral ads, even though Kirsten Lindquist has been arrested three times.\n",
    "\n",
    "Latanya Sweeney博士是哈佛大学的教授，也是该大学数据隐私实验室的主任。在论文 [“网络广告投放中的歧视”](https://arxiv.org/abs/1301.6822) (见 <<lantanya_被捕>>) 她描述了自己的发现，谷歌搜索她的名字会出现广告说 “Latanya Sweeney被捕了？“ 尽管她是唯一已知的Latanya Sweeney，从未被捕。然而，当她在谷歌上搜索其他名字时，比如 “Kirsten Lindquist”，她得到了更多中立的广告，尽管Kirsten Lindquist已经被捕三次。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image1.png\" id=\"lantanya_arrested\" caption=\"Google search showing ads about Professor Lantanya Sweeney's arrest record\" alt=\"Screenshot of google search showing ads about Professor Lantanya Sweeney's arrest record\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being a computer scientist, she studied this systematically, and looked at over 2000 names. She found a clear pattern where historically Black names received advertisements suggesting that the person had a criminal record, whereas, white names had more neutral advertisements.\n",
    "\n",
    "\n",
    "作为一名计算机科学家，她系统地研究了这一点，并查看了2000多个名字。她发现了一个清晰的模式，历史上有黑人血统的名字收到的广告暗示该人有犯罪记录，而白人名字则带有更多中立的广告。\n",
    "\n",
    "This is an example of bias. It can make a big difference to people's lives—for instance, if a job applicant is Googled it may appear that they have a criminal record when they do not.\n",
    "\n",
    "这是偏见的一个例子。它会对人们的生活产生很大的影响 -- 例如，如果一个求职者被谷歌搜索，他们可能会有犯罪记录，然而实际上他们并没有。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Does This Matter?\n",
    "\n",
    "# 这有什么关系？[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One very natural reaction to considering these issues is: \"So what? What's that got to do with me? I'm a data scientist, not a politician. I'm not one of the senior executives at my company who make the decisions about what we do. I'm just trying to build the most predictive model I can.\"\n",
    "\n",
    "\n",
    "考虑这些问题的一个非常自然的反应是: “那又怎样？这和我有什么关系？我是数据科学家，不是政治家。我不是我公司的高级管理人员之一，他们决定我们做什么。我只是想建立我能做到的最具预测性的模型。”[机器翻译]\n",
    "\n",
    "These are very reasonable questions. But we're going to try to convince you that the answer is that everybody who is training models absolutely needs to consider how their models will be used, and consider how to best ensure that they are used as positively as possible. There are things you can do. And if you don't do them, then things can go pretty badly.\n",
    "\n",
    "\n",
    "这些是非常合理的问题。但是我们将试图说服你，答案是每个训练模型的人都绝对需要考虑如何使用他们的模型，并考虑如何最好地确保尽可能积极地使用它们。有些事情你可以做。如果你不做，事情会变得很糟糕。[机器翻译]\n",
    "\n",
    "One particularly hideous example of what happens when technologists focus on technology at all costs is the story of IBM and Nazi Germany. In 2001, a Swiss judge ruled that it was not unreasonable \"to deduce that IBM's technical assistance facilitated the tasks of the Nazis in the commission of their crimes against humanity, acts also involving accountancy and classification by IBM machines and utilized in the concentration camps themselves.\"\n",
    "\n",
    "\n",
    "当技术人员不惜一切代价关注技术时，一个特别可怕的例子是IBM和纳粹德国的故事。2001，一名瑞士法官裁定，“推断IBM的技术援助助长了纳粹犯下危害人类罪的任务并不是不合理的，行为也涉及会计和分类由IBM机器和使用在集中营本身。\"[机器翻译]\n",
    "\n",
    "IBM, you see, supplied the Nazis with data tabulation products necessary to track the extermination of Jews and other groups on a massive scale. This was driven from the top of the company, with marketing to Hitler and his leadership team. Company President Thomas Watson personally approved the 1939 release of special IBM alphabetizing machines to help organize the deportation of Polish Jews. Pictured in <<meeting>> is Adolf Hitler (far left) meeting with IBM CEO Tom Watson Sr. (second from left), shortly before Hitler awarded Watson a special “Service to the Reich” medal in 1937.\n",
    "\n",
    "你看，IBM向纳粹提供了大规模跟踪犹太人和其他团体灭绝所需的数据列表产品。这是由公司高层推动的，向希特勒和他的领导团队进行营销。公司总裁托马斯 · 沃森亲自批准了IBM按字母顺序排列的特殊机器的1939版本，以帮助组织对波兰犹太人的驱逐。在 <<meeting>> 中，阿道夫·希特勒 (最左边) 与IBM首席执行官老汤姆·沃森 (左二) 会面，就在希特勒于1937年授予沃森一枚特殊的 “为帝国服务” 奖章之前不久。[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image2.png\" id=\"meeting\" caption=\"IBM CEO Tom Watson Sr. meeting with Adolf Hitler\" alt=\"A picture of IBM CEO Tom Watson Sr. meeting with Adolf Hitler\" width=\"400\">\n",
    "\n",
    "<Img src = \"images/ethics/image2.png\" id = \"meeting\" caption = \"IBM首席执行官汤姆·沃森与阿道夫·希特勒会面\" alt = \"IBM首席执行官汤姆·沃森的照片与阿道夫·希特勒会面“ width = \"400\">[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this was not an isolated incident—the organization's involvement was extensive. IBM and its subsidiaries provided regular training and maintenance onsite at the concentration camps: printing off cards, configuring machines, and repairing them as they broke frequently. IBM set up categorizations on its punch card system for the way that each person was killed, which group they were assigned to, and the logistical information necessary to track them through the vast Holocaust system. IBM's code for Jews in the concentration camps  was 8: some 6,000,000 were killed. Its code for Romanis was 12 (they were labeled by the Nazis as \"asocials,\" with over 300,000 killed in the *Zigeunerlager*, or “Gypsy camp”). General executions were coded as 4, death in the gas chambers as 6.\n",
    "\n",
    "但这不是一个孤立的事件 -- 该组织的参与是广泛的。IBM及其子公司在集中营现场提供定期培训和维护: 打印卡片，配置机器，并在机器经常损坏时进行维修。IBM在其穿孔卡片系统上为每个人被杀的方式、他们被分配到哪个小组以及通过庞大的大屠杀系统跟踪他们所需的后勤信息设置了分类。IBM对集中营中犹太人的编码是8: 大约6,000,000人被杀。它的罗马法典是12个 (他们被纳粹标记为 “美国公民”，超过300,000人在 * Zigeunerlager * 或 “吉普赛营地” 中丧生)。一般处决被编码为4，毒气室死亡为6。[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image3.jpeg\" id=\"punch_card\" caption=\"A punch card used by IBM in concentration camps\" alt=\"Picture of a punch card used by IBM in concentration camps\" width=\"600\">\n",
    "\n",
    "<Img src = \"images/ethics/image3.jpeg\" id = \"punch_card\" caption = \"IBM在集中营中使用的穿孔卡\" alt = \"IBM使用的穿孔卡的图片在集中营\" width = \"600\">[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the project managers and engineers and technicians involved were just living their ordinary lives. Caring for their families, going to the church on Sunday, doing their jobs the best they could. Following orders. The marketers were just doing what they could to meet their business development goals. As Edwin Black, author of *IBM and the Holocaust* (Dialog Press) observed: \"To the blind technocrat, the means were more important than the ends. The destruction of the Jewish people became even less important because the invigorating nature of IBM's technical achievement was only heightened by the fantastical profits to be made at a time when bread lines stretched across the world.\"\n",
    "\n",
    "\n",
    "当然，所涉及的项目经理和工程技术人员只是过着他们平凡的生活。照顾家人，周日去教堂，尽最大努力做好工作。服从命令。营销人员只是尽他们所能来实现他们的业务发展目标。正如 * IBM和大屠杀 * (对话出版社) 的作者埃德温 · 布莱克所说: “对盲目的技术官僚来说，手段比目的更重要。犹太人的毁灭变得更加不重要，因为IBM技术成就的令人振奋的性质只是因为在面包线遍布世界的时候获得的幻想利润而加剧的。\"[机器翻译]\n",
    "\n",
    "Step back for a moment and consider: How would you feel if you discovered that you had been part of a system that ended up hurting society? Would you be open to finding out? How can you help make sure this doesn't happen? We have described the most extreme situation here, but there are many negative societal consequences linked to AI and machine learning being observed today, some of which we'll describe in this chapter.\n",
    "\n",
    "\n",
    "退一步考虑一下: 如果你发现自己是一个最终伤害社会的系统的一部分，你会有什么感觉？你愿意知道吗？你怎么能帮助确保这不会发生？我们已经在这里描述了最极端的情况，但是今天观察到的人工智能和机器学习有许多负面的社会后果，其中一些我们将在本章中描述。[机器翻译]\n",
    "\n",
    "It's not just a moral burden, either. Sometimes technologists pay very directly for their actions. For instance, the first person who was jailed as a result of the Volkswagen scandal, where the car company was revealed to have cheated on its diesel emissions tests, was not the manager that oversaw the project, or an executive at the helm of the company. It was one of the engineers, James Liang, who just did what he was told.\n",
    "\n",
    "\n",
    "这也不仅仅是道德负担。有时技术人员为他们的行为支付非常直接的费用。例如，第一个因大众丑闻而入狱的人，汽车公司被揭露在柴油排放测试中作弊，不是监督项目的经理，也不是公司的高管。是其中一名工程师詹姆斯 · 梁刚刚照他说的做了。[机器翻译]\n",
    "\n",
    "Of course, it's not all bad—if a project you are involved in turns out to make a huge positive impact on even one person, this is going to make you feel pretty great!\n",
    "\n",
    "\n",
    "当然，这并不全是坏事 -- 如果你参与的一个项目最终对一个人产生了巨大的积极影响，这将会让你感觉非常好![机器翻译]\n",
    "\n",
    "Okay, so hopefully we have convinced you that you ought to care. But what should you do? As data scientists, we're naturally inclined to focus on making our models better by optimizing some metric or other. But optimizing that metric may not actually lead to better outcomes. And even if it *does* help create better outcomes, it almost certainly won't be the only thing that matters. Consider the pipeline of steps that occurs between the development of a model or an algorithm by a researcher or practitioner, and the point at which this work is actually used to make some decision. This entire pipeline needs to be considered *as a whole* if we're to have a hope of getting the kinds of outcomes we want.\n",
    "\n",
    "\n",
    "好吧，希望我们已经说服你，你应该关心。但是你应该怎么做呢？作为数据科学家，我们自然倾向于通过优化某些指标或其他指标来改进我们的模型。但是优化这个指标实际上可能不会带来更好的结果。即使它确实有助于创造更好的结果，它几乎肯定不是唯一重要的事情。考虑在研究者或实践者开发模型或算法之间发生的步骤管道，以及这项工作实际用于做出一些决策的点。如果我们希望得到我们想要的结果，就需要把整个管道作为一个整体来考虑。[机器翻译]\n",
    "\n",
    "Normally there is a very long chain from one end to the other. This is especially true if you are a researcher, where you might not even know if your research will ever get used for anything, or if you're involved in data collection, which is even earlier in the pipeline. But no one is better placed to inform everyone involved in this chain about the capabilities, constraints, and details of your work than you are. Although there's no \"silver bullet\" that can ensure your work is used the right way, by getting involved in the process, and asking the right questions, you can at the very least ensure that the right issues are being considered.\n",
    "\n",
    "\n",
    "通常从一端到另一端有一条很长的链。如果你是一名研究人员，你甚至可能不知道你的研究是否会被用于任何事情，或者你是否参与了数据收集，这一点尤其正确，这是甚至更早的管道.但是没有人比你更有能力、约束和你工作的细节来告诉这个链中的每个人。尽管没有 “灵丹妙药” 可以确保你的工作得到正确的使用，通过参与这个过程，问正确的问题，你至少可以确保考虑正确的问题。[机器翻译]\n",
    "\n",
    "Sometimes, the right response to being asked to do a piece of work is to just say \"no.\" Often, however, the response we hear is, \"If I don’t do it, someone else will.\" But consider this: if you’ve been picked for the job, you’re the best person they’ve found to do it—so if you don’t do it, the best person isn’t working on that project. If the first five people they ask all say no too, even better!\n",
    "\n",
    "有时候，被要求做一件工作的正确反应是说 “不” 然而，我们经常听到的回答是，“如果我不做，别人会做的。\"但是考虑一下: 如果你被选中做这份工作，你是他们找到的最好的人 -- 所以如果你不做，最好的人没有在那个项目上工作。如果他们问的前五个人也都说不，那就更好了![机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating Machine Learning with Product Design\n",
    "\n",
    "# # 集成机器学习与产品设计[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presumably the reason you're doing this work is because you hope it will be used for something. Otherwise, you're just wasting your time. So, let's start with the assumption that your work will end up somewhere. Now, as you are collecting your data and developing your model, you are making lots of decisions. What level of aggregation will you store your data at? What loss function should you use? What validation and training sets should you use? Should you focus on simplicity of implementation, speed of inference, or accuracy of the model? How will your model handle out-of-domain data items? Can it be fine-tuned, or must it be retrained from scratch over time?\n",
    "\n",
    "\n",
    "大概你做这项工作的原因是因为你希望它能被用来做某事。否则，你只是在浪费时间。所以，让我们从假设你的工作会在某个地方结束开始。现在，当你收集数据和开发模型时，你会做出很多决定。您将数据存储在什么级别的聚合？你应该使用什么损失函数？您应该使用哪些验证和培训集？您是否应该关注实现的简单性、推理的速度或模型的准确性？您的模型将如何处理域外数据项？它能被微调吗，还是必须随着时间的推移从头开始重新训练？[机器翻译]\n",
    "\n",
    "These are not just algorithm questions. They are data product design questions. But the product managers, executives, judges, journalists, doctors… whoever ends up developing and using the system of which your model is a part will not be well-placed to understand the decisions that you made, let alone change them.\n",
    "\n",
    "\n",
    "这些不仅仅是算法问题。它们是数据产品设计问题。但是产品经理、高管、法官、记者，医生…… 无论是谁最终开发和使用了你的模型所组成的系统，都不会很好地理解你所做的决定，更不用说改变它们了。[机器翻译]\n",
    "\n",
    "For instance, two studies found that Amazon’s facial recognition software produced [inaccurate](https://www.nytimes.com/2018/07/26/technology/amazon-aclu-facial-recognition-congress.html) and [racially biased](https://www.theverge.com/2019/1/25/18197137/amazon-rekognition-facial-recognition-bias-race-gender) results. Amazon claimed that the researchers should have changed the default parameters, without explaining how this would have changed the biased results. Furthermore, it turned out that [Amazon was not instructing police departments](https://gizmodo.com/defense-of-amazons-face-recognition-tool-undermined-by-1832238149) that used its software to do this either. There was, presumably, a big distance between the researchers that developed these algorithms and the Amazon documentation staff that wrote the guidelines provided to the police. A lack of tight integration led to serious problems for society at large, the police, and Amazon themselves. It turned out that their system erroneously matched 28 members of congress to criminal mugshots!  (And the Congresspeople wrongly matched to criminal mugshots were disproportionately people of color, as seen in <<congressmen>>.)\n",
    "\n",
    "例如，两项研究发现亚马逊的面部识别软件产生了 [不准确] ( https://www.nytimes.com/2018/07/26/technology/amazon-aclu-facial-recognition-congress.html ) 和 [种族偏见] ( https://www.theverge.com/2019/1/25/18197137/amazon-rekognition-facial-recognition-bias-race-gender ) 结果。亚马逊声称，研究人员应该改变默认参数，但没有解释这将如何改变有偏见的结果。此外，事实证明 [亚马逊没有指示警察部门] ( https://gizmodo.com/defense-of-amazons-face-recognition-tool-undermined-by-1832238149 ) 也使用了它的软件来做这件事。据推测，开发这些算法的研究人员和编写向警方提供的指南的亚马逊文档工作人员之间有很大的距离。缺乏紧密的整合导致了整个社会、警察和亚马逊本身的严重问题。事实证明，他们的系统错误地将28名国会议员与犯罪照片相匹配!(国会议员错误地与犯罪照片相匹配，他们是不成比例的有色人种，如 <<国会议员>> 所示。)[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image4.png\" id=\"congressmen\" caption=\"Congresspeople matched to criminal mugshots by Amazon software\" alt=\"Picture of the congresspeople matched to criminal mugshots by Amazon software, they are disproportionatedly people of color\" width=\"500\">\n",
    "\n",
    "<Img src = \"图像/道德/图像4.png\" id = \"国会议员\" caption = \"国会议员与Amazon software的犯罪照片匹配\" alt = \"国会议员与犯罪照片匹配的图片亚马逊软件，他们是不相称的人的颜色\" 宽度 = \"500\">[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data scientists need to be part of a cross-disciplinary team. And researchers need to work closely with the kinds of people who will end up using their research. Better still is if the domain experts themselves have learned enough to be able to train and debug some models themselves—hopefully there are a few of you reading this book right now!\n",
    "\n",
    "\n",
    "数据科学家需要成为跨学科团队的一员。研究人员需要与那些最终会使用他们的研究的人密切合作。更好的是，如果领域专家自己已经学到了足够的知识，能够自己训练和调试一些模型 -- 希望现在有一些人在读这本书![机器翻译]\n",
    "\n",
    "The modern workplace is a very specialized place. Everybody tends to have well-defined jobs to perform. Especially in large companies, it can be hard to know what all the pieces of the puzzle are. Sometimes companies even intentionally obscure the overall project goals that are being worked on, if they know that their employees are not going to like the answers. This is sometimes done by compartmentalising pieces as much as possible.\n",
    "\n",
    "\n",
    "现代工作场所是一个非常专业化的地方。每个人都倾向于有明确的工作要做。尤其是在大公司，很难知道拼图的所有部分是什么。有时，如果公司知道他们的员工不会喜欢答案，他们甚至会故意模糊正在进行的总体项目目标。这有时是通过尽可能地划分碎片来实现的。[机器翻译]\n",
    "\n",
    "In other words, we're not saying that any of this is easy. It's hard. It's really hard. We all have to do our best. And we have often seen that the people who do get involved in the higher-level context of these projects, and attempt to develop cross-disciplinary capabilities and teams, become some of the most important and well rewarded members of their organizations. It's the kind of work that tends to be highly appreciated by senior executives, even if it is sometimes considered rather uncomfortable by middle management.\n",
    "\n",
    "换句话说，我们并不是说这一切都很容易。很难。真的很难。我们都必须尽力而为。我们经常看到，那些参与到这些项目的更高层次背景中，并试图发展跨学科能力和团队的人，成为他们组织中最重要和最有回报的成员。这种工作往往会受到高级管理人员的高度赞赏，即使中层管理人员有时认为这种工作相当不舒服。[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics in Data Ethics\n",
    "\n",
    "# # 数据伦理主题[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data ethics is a big field, and we can't cover everything. Instead, we're going to pick a few topics that we think are particularly relevant:\n",
    "\n",
    "\n",
    "数据伦理是一个很大的领域，我们不可能什么都覆盖。相反，我们将选择一些我们认为特别相关的主题:[机器翻译]\n",
    "\n",
    "- The need for recourse and accountability\n",
    "- Feedback loops\n",
    "- Bias\n",
    "- Disinformation\n",
    "\n",
    "-追索权和问责制的必要性\n",
    "-反馈回路\n",
    "-偏置\n",
    "-虚假信息[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at each in turn.\n",
    "\n",
    "让我们依次看看每一个。[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recourse and Accountability\n",
    "\n",
    "# 追索权和责任[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a complex system, it is easy for no one person to feel responsible for outcomes. While this is understandable, it does not lead to good results. In the earlier example of the Arkansas healthcare system in which a bug led to people with cerebral palsy losing access to needed care, the creator of the algorithm blamed government officials, and government officials blamed those who implemented the software. NYU professor [Danah Boyd](https://www.youtube.com/watch?v=NTl0yyPqf3E) described this phenomenon: \"Bureaucracy has often been used to shift or evade responsibility... Today's algorithmic systems are extending bureaucracy.\"\n",
    "\n",
    "\n",
    "在一个复杂的系统中，没有人容易对结果负责。虽然这是可以理解的，但并没有带来好的结果。在阿肯色州医疗保健系统的早期例子中，一个错误导致脑瘫患者无法获得所需的护理，该算法的创建者指责政府官员，政府官员指责那些实施该软件的人。NYU教授 [Danah Boyd]( https://www.youtube.com/watch?v=NTl0yyPqf3E ) 描述了这种现象: “官僚主义经常被用来转移或逃避责任.今天的算法系统正在扩展官僚作风。\"[机器翻译]\n",
    "\n",
    "An additional reason why recourse is so necessary is because data often contains errors. Mechanisms for audits and error correction are crucial. A database of suspected gang members maintained by California law enforcement officials was found to be full of errors, including 42 babies who had been added to the database when they were less than 1 year old (28 of whom were marked as “admitting to being gang members”). In this case, there was no process in place for correcting mistakes or removing people once they’d been added. Another example is the US credit report system: in a large-scale study of credit reports by the Federal Trade Commission (FTC) in 2012, it was found that 26% of consumers had at least one mistake in their files, and 5% had errors that could be devastating.  Yet, the process of getting such errors corrected is incredibly slow and opaque. When public radio reporter [Bobby Allyn](https://www.washingtonpost.com/posteverything/wp/2016/09/08/how-the-careless-errors-of-credit-reporting-agencies-are-ruining-peoples-lives/) discovered that he was erroneously listed as having a firearms conviction, it took him \"more than a dozen phone calls, the handiwork of a county court clerk and six weeks to solve the problem. And that was only after I contacted the company’s communications department as a journalist.\"\n",
    "\n",
    "\n",
    "追索权如此必要的另一个原因是数据经常包含错误。审计和纠错机制至关重要。加州执法官员维护的一个涉嫌帮派成员的数据库被发现充满错误，包括42名不到1岁时被添加到数据库中的婴儿 (其中28人被标记为 “承认是帮派成员”)。在这种情况下，一旦人们被添加，就没有纠正错误或删除他们的流程。另一个例子是美国的信用报告系统: 在2012年联邦贸易委员会 (FTC) 对信用报告的大规模研究中，发现26% 的消费者在他们的文件中至少有一个错误，5% 的人有可能是毁灭性的错误。然而，纠正这些错误的过程非常缓慢和不透明。当公共广播记者 [· 鲍比 · 阿林] ( https://www.washingtonpost.com/posteverything/wp/2016/09/08/how-the-careless-errors-of-credit-reporting-agencies-are-ruining-peoples-lives/ ) 发现他被错误地列为持枪定罪，他花了 “十几个电话，一个县法院书记员的杰作和六周时间来解决这个问题”。那是在我作为记者联系了公司的通信部门之后。\"[机器翻译]\n",
    "\n",
    "As machine learning practitioners, we do not always think of it as our responsibility to understand how our algorithms end up being implemented in practice. But we need to.\n",
    "\n",
    "作为机器学习从业者，我们并不总是认为理解我们的算法最终如何在实践中实现是我们的责任。但我们需要。[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback Loops\n",
    "\n",
    "# 反馈回路[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explained in <<chapter_intro>> how an algorithm can interact with its enviromnent to create a feedback loop, making predictions that reinforce actions taken in the real world, which lead to predictions even more pronounced in the same direction. \n",
    "As an example, let's again consider YouTube's recommendation system. A couple of years ago the Google team talked about how they had introduced reinforcement learning (closely related to deep learning, but where your loss function represents a result potentially a long time after an action occurs) to improve YouTube's recommendation system. They described how they used an algorithm that made recommendations such that watch time would be optimized.\n",
    "\n",
    "\n",
    "我们在 <<chapter_intro>> 中解释了算法如何与其环境交互以创建反馈循环，做出预测以加强现实世界中采取的行动，这导致预测在同一方向更加明显。\n",
    "作为一个例子，让我们再次考虑YouTube的推荐系统。几年前，谷歌团队谈到了他们是如何引入强化学习的 (与深度学习密切相关，但是你的损失函数代表了一个行动发生后可能很长时间的结果) 改善YouTube的推荐系统。他们描述了他们如何使用一种算法来提出建议，以便优化观看时间。[机器翻译]\n",
    "\n",
    "However, human beings tend to be drawn to controversial content. This meant that videos about things like conspiracy theories started to get recommended more and more by the recommendation system. Furthermore, it turns out that the kinds of people that are interested in conspiracy theories are also people that watch a lot of online videos! So, they started to get drawn more and more toward YouTube. The increasing number of conspiracy theorists watching videos on YouTube resulted in the algorithm recommending more and more conspiracy theory and other extremist content, which resulted in more extremists watching videos on YouTube, and more people watching YouTube developing extremist views, which led to the algorithm recommending more extremist content... The system was spiraling out of control.\n",
    "\n",
    "\n",
    "然而，人类倾向于被有争议的内容所吸引。这意味着关于阴谋论之类的东西的视频开始被推荐系统推荐得越来越多。此外，事实证明，对阴谋论感兴趣的人也是看很多在线视频的人!所以，他们开始越来越倾向于YouTube。越来越多的阴谋论者在YouTube上观看视频，导致算法推荐越来越多的阴谋论和其他极端主义内容，这导致越来越多的极端分子在YouTube上观看视频，更多的人在观看YouTube发展极端主义观点，这导致算法推荐更多极端主义内容.系统失控了。[机器翻译]\n",
    "\n",
    "And this phenomenon was not contained to this particular type of content. In June 2019 the *New York Times* published an article on YouTube's recommendation system, titled [\"On YouTube’s Digital Playground, an Open Gate for Pedophiles\"](https://www.nytimes.com/2019/06/03/world/americas/youtube-pedophiles.html). The article started with this chilling story:\n",
    "\n",
    "这种现象并不包含在这种特定类型的内容中。2019年6月，* 纽约时报 * 在YouTube的推荐系统上发表了一篇文章，标题为 [“在YouTube的数字游乐场，为恋童癖者打开的大门”] ( https://www.nytimes.com/2019/06/03/world/americas/youtube-pedophiles.html )。文章从这个令人不寒而栗的故事开始:[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> : Christiane C. didn’t think anything of it when her 10-year-old daughter and a friend uploaded a video of themselves playing in a backyard pool… A few days later… the video had thousands of views. Before long, it had ticked up to 400,000... “I saw the video again and I got scared by the number of views,” Christiane said. She had reason to be. YouTube’s automated recommendation system… had begun showing the video to users who watched other videos of prepubescent, partially clothed children, a team of researchers has found.\n",
    "\n",
    "\n",
    "> : Christiane C.当她10岁的女儿和一个朋友上传了一段他们在后院游泳池玩耍的视频时，我没有多想…… 几天后…… 这段视频有成千上万的浏览量。没过多久，它就涨到了400,000.“我又看了一遍视频，我被观看的次数吓到了，” 克里斯蒂娜说。她有理由这样做。一组研究人员发现，YouTube的自动推荐系统…… 已经开始向观看青春期前部分穿着衣服的儿童的其他视频的用户展示视频。[机器翻译]\n",
    "\n",
    "> : On its own, each video might be perfectly innocent, a home movie, say, made by a child. Any revealing frames are fleeting and appear accidental. But, grouped together, their shared features become unmistakable.\n",
    "\n",
    ">: 就其本身而言，每一段视频都可能是完全无辜的，比如说，一部由孩子制作的家庭电影。任何揭示的画面都是短暂的，看起来都是偶然的。但是，把它们组合在一起，它们的共同特征变得不会出错。[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YouTube's recommendation algorithm had begun curating playlists for pedophiles, picking out innocent home videos that happened to contain prepubescent, partially clothed children. \n",
    "\n",
    "\n",
    "YouTube的推荐算法已经开始为恋童癖者管理播放列表，挑选碰巧包含青春期前部分穿着衣服的孩子的无辜家庭视频。[机器翻译]\n",
    "\n",
    "No one at Google planned to create a system that turned family videos into porn for pedophiles. So what happened?\n",
    "\n",
    "\n",
    "谷歌没有人计划为恋童癖者创建一个将家庭视频变成色情的系统。什么事？[机器翻译]\n",
    "\n",
    "Part of the problem here is the centrality of metrics in driving a financially important system. When an algorithm has a metric to optimize, as you have seen, it will do everything it can to optimize that number. This tends to lead to all kinds of edge cases, and humans interacting with a system will search for, find, and exploit these edge cases and feedback loops for their advantage.\n",
    "\n",
    "\n",
    "这里的部分问题是指标在推动一个财务上重要的系统中的中心地位。当一个算法有一个要优化的指标时，正如你所看到的，它会尽一切努力优化这个数字。这往往会导致各种边缘案例，与系统交互的人类会搜索、发现和利用这些边缘案例和反馈回路来获得它们的优势。[机器翻译]\n",
    "\n",
    "There are signs that this is exactly what has happened with YouTube's recommendation system. *The Guardian* ran an article called [\"How an ex-YouTube Insider Investigated its Secret Algorithm\"](https://www.theguardian.com/technology/2018/feb/02/youtube-algorithm-election-clinton-trump-guillaume-chaslot) about Guillaume Chaslot, an ex-YouTube engineer who created AlgoTransparency, which tracks these issues. Chaslot published the chart in <<ethics_yt_rt>>, following the release of Robert Mueller's \"Report on the Investigation Into Russian Interference in the 2016 Presidential Election.\"\n",
    "\n",
    "有迹象表明，这正是YouTube的推荐系统所发生的事情。* The Guardian * 运行了一篇名为 [“一位前YouTube内部人士如何调查其秘密算法”] 的文章 ( https://www.theguardian.com/technology/2018/feb/02/youtube-algorithm-election-clinton-trump-guillaume-chaslot ) 关于纪尧姆 · 查斯洛特，一位前YouTube工程师，他创建了跟踪这些问题的AlgoTransparency。在罗伯特 · 穆勒的 “关于俄罗斯干涉2016总统选举的调查报告” 发布之后，查斯洛特在 <ethics_yt_rt> 中发表了这张图表[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image18.jpeg\" id=\"ethics_yt_rt\" caption=\"Coverage of the Mueller report\" alt=\"Coverage of the Mueller report\" width=\"500\">\n",
    "\n",
    "<Img src = \"图像/道德/图像18.jpeg\" id = \"ethics_yt_rt\" caption = \"Mueller报告的覆盖率\" alt = \"Mueller报告的覆盖率\" width = \"500\">[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Russia Today's coverage of the Mueller report was an extreme outlier in terms of how many channels were recommending it. This suggests the possibility that Russia Today, a state-owned Russia media outlet, has been successful in gaming YouTube's recommendation algorithm. Unfortunately, the lack of transparency of systems like this makes it hard to uncover the kinds of problems that we're discussing.\n",
    "\n",
    "\n",
    "《今日俄罗斯》对穆勒报告的报道就有多少频道推荐它而言是一个极端的异常值。这表明俄罗斯国有媒体 “今日俄罗斯” 有可能在YouTube推荐算法游戏中取得成功。不幸的是，像这样的系统缺乏透明度，使得我们很难发现我们正在讨论的各种问题。[机器翻译]\n",
    "\n",
    "One of our reviewers for this book, Aurélien Géron, led YouTube's video classification team from 2013 to 2016 (well before the events discussed here). He pointed out that it's not just feedback loops involving humans that are a problem. There can also be feedback loops without humans! He told us about an example from YouTube:\n",
    "\n",
    "\n",
    "我们这本书的评论家之一aur é lien g é ron在2013年至2016年期间领导了YouTube的视频分类团队 (远在这里讨论的事件之前)。他指出，问题不仅仅是涉及人类的反馈回路。也可以有没有人类的反馈循环!他告诉我们一个来自YouTube的例子:[机器翻译]\n",
    "\n",
    "> : One important signal to classify the main topic of a video is the channel it comes from. For example, a video uploaded to a cooking channel is very likely to be a cooking video. But how do we know what topic a channel is about? Well… in part by looking at the topics of the videos it contains! Do you see the loop? For example, many videos have a description which indicates what camera was used to shoot the video. As a result, some of these videos might get classified as videos about “photography.” If a channel has such a misclassified video, it might be classified as a “photography” channel, making it even more likely for future videos on this channel to be wrongly classified as “photography.” This could even lead to runaway virus-like classifications! One way to break this feedback loop is to classify videos with and without the channel signal. Then when classifying the channels, you can only use the classes obtained without the channel signal. This way, the feedback loop is broken.\n",
    "\n",
    "\n",
    ">: 对视频的主要主题进行分类的一个重要信号是它来自的频道。例如，上传到烹饪频道的视频很可能是烹饪视频。但是我们怎么知道一个频道是关于什么话题的呢？嗯…… 部分是通过查看它包含的视频的主题!你看到循环了吗？例如，许多视频都有一个描述，指示使用什么相机拍摄视频。因此，其中一些视频可能会被归类为关于 “摄影” 的视频。如果一个频道有这样一个错误分类的视频，它可能被归类为 “摄影” 频道，使得这个频道上的未来视频更有可能被错误地归类为 “摄影”。\"这甚至可能导致类似病毒的失控分类!打破这种反馈循环的一种方法是对有和没有频道信号的视频进行分类。然后在对通道进行分类时，您只能使用在没有通道信号的情况下获得的类。这样，反馈回路就中断了。[机器翻译]\n",
    "\n",
    "There are positive examples of people and organizations attempting to combat these problems. Evan Estola, lead machine learning engineer at Meetup, [discussed the example](https://www.youtube.com/watch?v=MqoRzNhrTnQ) of men expressing more interest than women in tech meetups. taking gender into account could therefore cause Meetup’s algorithm to recommend fewer tech meetups to women, and as a result, fewer women would find out about and attend tech meetups, which could cause the algorithm to suggest even fewer tech meetups to women, and so on in a self-reinforcing feedback loop. So, Evan and his team made the ethical decision for their recommendation algorithm to not create such a feedback loop, by explicitly not using gender for that part of their model. It is encouraging to see a company not just unthinkingly optimize a metric, but consider its impact. According to Evan, \"You need to decide which feature not to use in your algorithm... the most optimal algorithm is perhaps not the best one to launch into production.\"\n",
    "\n",
    "\n",
    "有一些积极的例子表明，人们和组织试图解决这些问题。Evan Estola，Meetup的首席机器学习工程师，[讨论了例子] ( https://www.youtube.com/watch?v=MqoRzNhrTnQ ) 表示男人比女人对科技聚会更感兴趣。因此，考虑到性别可能会导致Meetup的算法向女性推荐更少的技术会议，因此，更少的女性会发现并参加技术会议，这可能会导致算法在自我强化反馈回路中向女性推荐更少的技术会议，等等。因此，埃文和他的团队通过明确不使用性别作为他们模型的这一部分，为他们的推荐算法做出了不创建这样一个反馈循环的道德决定。令人鼓舞的是，看到一家公司不仅不假思索地优化了一个指标，还考虑了它的影响。根据埃文的说法，“你需要决定在你的算法中不使用哪个功能.最优算法也许不是投入生产的最佳算法。\"[机器翻译]\n",
    "\n",
    "While Meetup chose to avoid such an outcome, Facebook provides an example of allowing a runaway feedback loop to run wild. Like YouTube, it tends to radicalize users interested in one conspiracy theory by introducing them to more. As Renee DiResta, a researcher on proliferation of disinformation, [writes](https://www.fastcompany.com/3059742/social-network-algorithms-are-distorting-reality-by-boosting-conspiracy-theories):\n",
    "\n",
    "虽然Meetup选择避免这样的结果，但Facebook提供了一个允许失控的反馈循环失控的例子。像YouTube一样，它倾向于通过向用户介绍更多内容来激进化对一个阴谋论感兴趣的用户。正如虚假信息扩散的研究人员Renee DiResta [写道] ( https://www.fastcompany.com/3059742/social-network-algorithms-are-distorting-reality-by-boosting-conspiracy-theories ):[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> : Once people join a single conspiracy-minded [Facebook] group, they are algorithmically routed to a plethora of others. Join an anti-vaccine group, and your suggestions will include anti-GMO, chemtrail watch, flat Earther (yes, really), and \"curing cancer naturally groups. Rather than pulling a user out of the rabbit hole, the recommendation engine pushes them further in.\"\n",
    "\n",
    ">: 一旦人们加入一个有阴谋思想的 [脸书] 小组，他们就会在算法上被很多其他人所吸引。加入一个抗疫苗小组，你的建议将包括抗GMO，chemtrail手表，flat Earther (是的，真的) 和 “自然治愈癌症小组”。推荐引擎没有把用户从兔子洞拉出来，而是把他们推得更远。\"[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is extremely important to keep in mind that this kind of behavior can happen, and to either anticipate a feedback loop or take positive action to break it when you see the first signs of it in your own projects. Another thing to keep in mind is *bias*, which, as we discussed briefly in the previous chapter, can interact with feedback loops in very troublesome ways.\n",
    "\n",
    "记住这种行为可能发生是非常重要的，当你在自己的项目中看到反馈循环的第一个迹象时，要么预测反馈循环，要么采取积极行动打破它。要记住的另一件事是 * 偏见 *，正如我们在前一章中简要讨论的那样，它可以以非常麻烦的方式与反馈回路相互作用。[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias\n",
    "\n",
    "# 偏差[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussions of bias online tend to get pretty confusing pretty fast. The word \"bias\" means so many different things. Statisticians often think when data ethicists are talking about bias that they're talking about the statistical definition of the term bias. But they're not. And they're certainly not talking about the biases that appear in the weights and biases which are the parameters of your model!\n",
    "\n",
    "\n",
    "网上关于偏见的讨论往往会很快变得相当混乱。“偏见” 这个词意味着很多不同的东西。统计学家通常认为，当数据伦理学家谈论偏见时，他们谈论的是术语偏见的统计定义。但事实并非如此。他们当然不是在谈论体重和偏见中出现的偏见，这是你的模型的参数![机器翻译]\n",
    "\n",
    "What they're talking about is the social science concept of bias. In [\"A Framework for Understanding Unintended Consequences of Machine Learning\"](https://arxiv.org/abs/1901.10002) MIT's Harini Suresh and John Guttag describe six types of bias in machine learning, summarized in <<bias>> from their paper.\n",
    "\n",
    "他们谈论的是偏见的社会科学概念。在 [“一个理解机器学习的非预期后果的框架”] ( https://arxiv.org/abs/1901.10002 ) 麻省理工学院的Harini Suresh和John Guttag描述了机器学习中的六种偏见，总结在他们的论文中的 <<bias>> 中。[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/pipeline_diagram.svg\" id=\"bias\" caption=\"Bias in machine learning can come from multiple sources (courtesy of Harini Suresh and John V. Guttag)\" alt=\"A diagram showing all sources where bias can appear in machine learning\" width=\"700\">\n",
    "\n",
    "<Img src = \"images/ethics/pipeline_diagram.svg\" id = \"bias\" caption = \"机器学习中的偏差可以来自多个来源 (由Harini Suresh和John V提供。Guttag)\"alt =\" 显示机器学习中可能出现偏差的所有来源的图表 \"width =\" 700 \">[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll discuss four of these types of bias, those that we've found most helpful in our own work (see the paper for details on the others).\n",
    "\n",
    "我们将讨论其中四种类型的偏见，这些偏见是我们在自己的工作中发现最有帮助的 (有关其他偏见的详细信息，请参见论文)。[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Historical bias\n",
    "\n",
    "历史偏见[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Historical bias* comes from the fact that people are biased, processes are biased, and society is biased. Suresh and Guttag say: \"Historical bias is a fundamental, structural issue with the first step of the data generation process and can exist even given perfect sampling and feature selection.\"\n",
    "\n",
    "\n",
    "* 历史偏见 * 来源于人有偏见，过程有偏见，社会有偏见。Suresh和Guttag说: “历史偏差是数据生成过程第一步的一个基本的结构问题，即使给定完美的采样和特征选择，也可以存在。”[机器翻译]\n",
    "\n",
    "For instance, here are a few examples of historical *race bias* in the US, from the *New York Times* article [\"Racial Bias, Even When We Have Good Intentions\"](https://www.nytimes.com/2015/01/04/upshot/the-measuring-sticks-of-racial-bias-.html) by the University of Chicago's Sendhil Mullainathan:\n",
    "\n",
    "\n",
    "例如，这里有一些美国历史 * 种族偏见 * 的例子，来自 * 纽约时报 * 文章 [“种族偏见，即使我们有良好的意图”] ( https://www.nytimes.com/2015/01/04/upshot/the-measuring-sticks-of-racial-bias-.html ) 由芝加哥大学的森迪尔 · 穆拉纳坦:[机器翻译]\n",
    "\n",
    "  - When doctors were shown identical files, they were much less likely to recommend cardiac catheterization (a helpful procedure) to Black patients.\n",
    "  - When bargaining for a used car, Black people were offered initial prices $700 higher and received far smaller concessions.\n",
    "  - Responding to apartment rental ads on Craigslist with a Black name elicited fewer responses than with a white name.\n",
    "  - An all-white jury was 16 percentage points more likely to convict a Black defendant than a white one, but when a jury had one Black member it convicted both at the same rate.\n",
    "\n",
    "\n",
    "-当医生被展示相同的文件时，他们不太可能向黑人患者推荐心导管插入术 (一种有用的程序)。\n",
    "-当为一辆二手车讨价还价时，黑人的初始价格高出700美元，并获得小得多的优惠。\n",
    "-回应Craigslist上的公寓租赁广告，用黑人名字比用白人名字引起的回应少。\n",
    "-全白人陪审团判定黑人被告有罪的可能性比白人被告高16个百分点，但当陪审团有一名黑人成员时，他们以相同的速度定罪。[机器翻译]\n",
    "\n",
    "The COMPAS algorithm, widely used for sentencing and bail decisions in the US, is an example of an important algorithm that, when tested by [ProPublica](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing), showed clear racial bias in practice (<<bail_algorithm>>).\n",
    "\n",
    "在美国广泛用于判决和保释判决的COMPAS算法是一个重要算法的例子，当由 [ProPublica]( https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing )，在实践中表现出明显的种族偏见 (<<bail_algorithm>>)。[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image6.png\" id=\"bail_algorithm\" caption=\"Results of the COMPAS algorithm\" alt=\"Table showing the COMPAS algorithm is more likely to give bail to white people, even if they re-offend more\" width=\"700\">\n",
    "\n",
    "<Img src = \"images/ethics/image6.png\" id = \"bail_algorithm\" caption = \"COMPAS算法的结果\" alt = \"显示COMPAS算法的表格更有可能放弃白人，即使他们再次冒犯更多“ 宽度 = ”700“>[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any dataset involving humans can have this kind of bias: medical data, sales data, housing data, political data, and so on. Because underlying bias is so pervasive, bias in datasets is very pervasive. Racial bias even turns up in computer vision, as shown in the example of autocategorized photos shared on Twitter by a Google Photos user shown in <<google_photos>>.\n",
    "\n",
    "任何涉及人类的数据集都可能有这种偏见: 医疗数据、销售数据、住房数据、政治数据等等。因为潜在的偏差是如此普遍，数据集中的偏差是非常普遍的。种族偏见甚至出现在计算机视觉中，如 <<google_photos>> 中显示的谷歌照片用户在推特上共享的自动分类照片示例所示。[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image7.png\" id=\"google_photos\" caption=\"One of these labels is very wrong...\" alt=\"Screenshot of the use of Google photos labeling a black user and her friend as gorillas\" width=\"450\">\n",
    "\n",
    "<Img src = \"images/ethical/image7.png\" id = \"google_photos\" caption = \"其中一个标签非常错误.\" alt = \"使用Google照片的屏幕截图标签a黑人用户和她的朋友作为大猩猩\" 宽度 = \"450\">[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, that is showing what you think it is: Google Photos classified a Black user's photo with their friend as \"gorillas\"! This algorithmic misstep got a lot of attention in the media. “We’re appalled and genuinely sorry that this happened,” a company spokeswoman said. “There is still clearly a lot of work to do with automatic image labeling, and we’re looking at how we can prevent these types of mistakes from happening in the future.”\n",
    "\n",
    "\n",
    "是的，这显示了你的想法: 谷歌照片将黑人用户和他们朋友的照片归类为 “大猩猩”!这种算法失误引起了媒体的极大关注。“我们对此感到震惊和真诚的遗憾，” 公司发言人说。“很明显，自动图像标签还有很多工作要做，我们正在研究如何防止这类错误在未来发生。”[机器翻译]\n",
    "\n",
    "Unfortunately, fixing problems in machine learning systems when the input data has problems is hard. Google's first attempt didn't inspire confidence, as coverage by *The Guardian* suggested (<<gorilla-ban>>).\n",
    "\n",
    "不幸的是，当输入数据有问题时，修复机器学习系统中的问题是困难的。谷歌的第一次尝试并没有像《卫报》报道的那样激发信心 (<<大猩猩禁令>>)。[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image8.png\" id=\"gorilla-ban\" caption=\"Google's first response to the problem\" alt=\"Pictures of a headlines from the Guardian, showing Google removed gorillas and other moneys from the possible labels of its algorithm\" width=\"500\">\n",
    "\n",
    "<Img src = \\ \"images/ethics/image8.png \\\" id = \\ \"gorilla-ban \\\" caption = \\ \"Google对问题的首次回应 \\\" alt = \\ \"来自《卫报》的头条图片，显示谷歌从其算法“ width = \"500\"> 的可能标签中删除大猩猩和其他钱[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These kinds of problems are certainly not limited to just Google. MIT researchers studied the most popular online computer vision APIs to see how accurate they were. But they didn't just calculate a single accuracy number—instead, they looked at the accuracy across four different groups, as illustrated in <<face_recognition>>.\n",
    "\n",
    "这类问题当然不仅限于谷歌。麻省理工学院的研究人员研究了最流行的在线计算机视觉APIs，以了解它们的准确性。但他们不只是计算一个单一的准确性数字-相反，他们查看了四个不同组的准确性，如 <<face_recognition>> 所示。[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image9.jpeg\" id=\"face_recognition\" caption=\"Error rate per gender and race for various facial recognition systems\" alt=\"Table showing how various facial recognition systems perform way worse on darker shades of skin and females\" width=\"600\">\n",
    "\n",
    "<Img src = \"images/ethics/image9.jpeg\" id = \"face_recognition\" caption = \"各种面部识别系统的性别和种族错误率\" alt = \"表格显示各种面部识别系统在深色皮肤和女性身上表现更差\" width = \"600\">[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IBM's system, for instance, had a 34.7% error rate for darker females, versus 0.3% for lighter males—over 100 times more errors! Some people incorrectly reacted to these experiments by claiming that the difference was simply because darker skin is harder for computers to recognize. However, what actually happened was that, after the negative publicity that this result created, all of the companies in question dramatically improved their models for darker skin, such that one year later they were nearly as good as for lighter skin. So what this actually showed is that the developers failed to utilize datasets containing enough darker faces, or test their product with darker faces.\n",
    "\n",
    "\n",
    "例如，IBM的系统对深色女性的错误率为34.7%，而浅色男性的错误率为0.3%，误差超过100倍!一些人错误地对这些实验做出反应，声称这种差异仅仅是因为深色皮肤对计算机来说更难识别。然而，实际上发生的是，在这个结果造成负面宣传之后，所有有问题的公司都大幅改进了他们的深色皮肤模型，这样一年后，它们几乎和浅色皮肤一样好。因此，这实际上表明，开发人员未能利用包含足够深色面孔的数据集，或者用深色面孔测试他们的产品。[机器翻译]\n",
    "\n",
    "One of the MIT researchers, Joy Buolamwini, warned: \"We have entered the age of automation overconfident yet underprepared. If we fail to make ethical and inclusive artificial intelligence, we risk losing gains made in civil rights and gender equity under the guise of machine neutrality.\"\n",
    "\n",
    "\n",
    "麻省理工学院的一名研究人员乔伊 · 布奥拉姆维尼警告说: “我们已经进入了自动化时代，过于自信，但准备不足。如果我们不能制造道德和包容性的人工智能，我们就有可能在机器中立的幌子下失去公民权利和性别平等的成果。\"[机器翻译]\n",
    "\n",
    "Part of the issue appears to be a systematic imbalance in the makeup of popular datasets used for training models. The abstract to the paper [\"No Classification Without Representation: Assessing Geodiversity Issues in Open Data Sets for the Developing World\"](https://arxiv.org/abs/1711.08536) by Shreya Shankar et al. states, \"We analyze two large, publicly available image data sets to assess geo-diversity and find that these data sets appear to exhibit an observable amerocentric and eurocentric representation bias. Further, we analyze classifiers trained on these data sets to assess the impact of these training distributions and find strong differences in the relative performance on images from different locales.\" <<image_provenance>> shows one of the charts from the paper, showing the geographic makeup of what was, at the time (and still are, as this book is being written) the two most important image datasets for training models.\n",
    "\n",
    "问题的一部分似乎是用于训练模型的流行数据集组成的系统不平衡。论文摘要 [“没有代表性的分类: 评估发展中世界开放数据集中的地理多样性问题”] ( https://arxiv.org/abs/1711.08536 ) Shreya Shankar等人说，“我们分析了两个大的，公开可用的图像数据集，以评估地理多样性，并发现这些数据集似乎表现出可观察到的美国中心和欧洲中心表征偏差。此外，我们分析在这些数据集上训练的分类器，以评估这些训练分布的影响，并发现不同地区图像的相对性能有很大差异。“<<image_origin>> 显示了纸上的一个图表，显示了当时 (现在仍然是) 的地理组成正如本书正在编写) 训练模型的两个最重要的图像数据集。[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image10.png\" id=\"image_provenance\" caption=\"Image provenance in popular training sets\" alt=\"Graphs showing how the vast majority of images in popular training datasets come from the US or Western Europe\" width=\"800\">\n",
    "\n",
    "<Img src = \"images/ethics/image10.png\" id = \"image_origin\" caption = \"流行训练集中的图像出处\" alt = \"图表显示了流行训练中的绝大多数图像数据集来自美国或西欧\" width = \"800\">[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vast majority of the images are from the United States and other Western countries, leading to models trained on ImageNet performing worse on scenes from other countries and cultures. For instance, research found that such models are worse at identifying household items (such as soap, spices, sofas, or beds) from lower-income countries. <<object_detect>> shows an image from the paper, [\"Does Object Recognition Work for Everyone?\"](https://arxiv.org/pdf/1906.02659.pdf) by Terrance DeVries et al. of Facebook AI Research that illustrates this point.\n",
    "\n",
    "绝大多数图像来自美国和其他西方国家，导致在ImageNet上训练的模特在其他国家和文化的场景中表现更差。例如，研究发现，这种模式在识别低收入国家的家居用品 (如肥皂，香料，沙发或床) 方面更糟糕。<<Object_detect>> 显示了来自纸张的图像，[“对象识别对每个人都有效吗？”] ( https://arxiv.org/pdf/1906.02659.pdf ) Facebook AI Research的Terrance DeVries等人阐明了这一点。[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image17.png\" id=\"object_detect\" caption=\"Object detection in action\" alt=\"Figure showing an object detection algorithm performing better on western products\" width=\"500\">\n",
    "\n",
    "<Img src = \"images/ethics/image17.png\" id = \"object_detect\" caption = \"Object detection in action\" alt = \"在西方产品上表现更好的对象检测算法图\" 宽度 = \"500\">[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we can see that the lower-income soap example is a very long way away from being accurate, with every commercial image recognition service predicting \"food\" as the most likely answer!\n",
    "\n",
    "\n",
    "在这个例子中，我们可以看到低收入的soap例子离精确还有很长的路要走，每个商业图像识别服务都预测 “食物” 是最有可能的答案![机器翻译]\n",
    "\n",
    "As we will discuss shortly, in addition, the vast majority of AI researchers and developers are young white men. Most projects that we have seen do most user testing using friends and families of the immediate product development group. Given this, the kinds of problems we just discussed should not be surprising.\n",
    "\n",
    "\n",
    "正如我们将很快讨论的那样，此外，绝大多数AI研究人员和开发人员都是年轻的白人。我们看到的大多数项目都使用直接产品开发团队的朋友和家人进行大多数用户测试。鉴于此，我们刚刚讨论的各种问题并不奇怪。[机器翻译]\n",
    "\n",
    "Similar historical bias is found in the texts used as data for natural language processing models. This crops up in downstream machine learning tasks in many ways. For instance, it [was widely reported](https://nypost.com/2017/11/30/google-translates-algorithm-has-a-gender-bias/) that until last year Google Translate showed systematic bias in how it translated the Turkish gender-neutral pronoun \"o\" into English: when applied to jobs which are often associated with males it used \"he,\" and when applied to jobs which are often associated with females it used \"she\" (<<turkish_gender>>).\n",
    "\n",
    "在用作自然语言处理模型数据的文本中发现了类似的历史偏差。这以多种方式出现在下游机器学习任务中。例如，它 [被广泛报道] ( https://nypost.com/2017/11/30/google-translates-algorithm-has-a-gender-bias/ ) 直到去年，谷歌翻译在将土耳其语中性代词 “o” 翻译成英语方面表现出系统的偏见: 当应用于通常与男性相关的工作时，它使用 “他”，当应用于通常与女性相关的工作时，它使用 “她” (<<turkish_gender>>)。[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image11.png\" id=\"turkish_gender\" caption=\"Gender bias in text data sets\" alt=\"Figure showing gender bias in data sets used to train language models showing up in translations\" width=\"600\">\n",
    "\n",
    "<Img src = \"images/ethics/image11.png\" id = \"turkish_gender\" caption = \"文本数据集中的性别偏见\" alt = \"用于训练语言的数据集中显示性别偏见的图形在翻译中显示的模型\" width = \"600\">[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also see this kind of bias in online advertisements. For instance, a [study](https://arxiv.org/abs/1904.02095) in 2019 by Muhammad Ali et al. found that even when the person placing the ad does not intentionally discriminate, Facebook will show ads to very different audiences based on race and gender. Housing ads with the same text, but picture either a white or a Black family, were shown to racially different audiences.\n",
    "\n",
    "我们在网络广告中也看到这种偏见。例如，一项 [研究] ( https://arxiv.org/abs/1904.02095 ) Muhammad Ali等人在2019年发现，即使放置广告的人没有故意歧视，Facebook也会根据种族和性别向非常不同的受众展示广告。带有相同文本的房屋广告，但图片是白人或黑人家庭，向不同种族的观众展示。[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measurement bias\n",
    "\n",
    "测量偏差[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper [\"Does Machine Learning Automate Moral Hazard and Error\"](https://scholar.harvard.edu/files/sendhil/files/aer.p20171084.pdf) in *American Economic Review*, Sendhil Mullainathan and Ziad Obermeyer look at a model that tries to answer the question: using historical electronic health record (EHR) data, what factors are most predictive of stroke? These are the top predictors from the model:\n",
    "\n",
    "\n",
    "在论文 [“机器学习是否自动化了道德风险和错误”] ( https://scholar.harvard.edu/files/sendhil/files/aer.p20171084.pdf ) 在 * 美国经济评论 * 中，Sendhil Mullainathan和Ziad Obermeyer查看了一个试图回答问题的模型: 使用历史电子健康记录 (EHR) 数据，哪些因素最能预测中风？这些是来自模型的顶级预测因子:[机器翻译]\n",
    "\n",
    "  - Prior stroke\n",
    "  - Cardiovascular disease\n",
    "  - Accidental injury\n",
    "  - Benign breast lump\n",
    "  - Colonoscopy\n",
    "  - Sinusitis\n",
    "\n",
    "\n",
    "-先前的中风\n",
    "-心血管疾病\n",
    "-意外伤害\n",
    "-良性乳腺肿块\n",
    "-结肠镜检查\n",
    "-鼻窦炎[机器翻译]\n",
    "\n",
    "However, only the top two have anything to do with a stroke! Based on what we've studied so far, you can probably guess why. We haven’t really measured *stroke*, which occurs when a region of the brain is denied oxygen due to an interruption in the blood supply. What we’ve measured is who had symptoms, went to a doctor, got the appropriate tests, *and* received a diagnosis of stroke. Actually having a stroke is not the only thing correlated with this complete list—it's also correlated with being the kind of person who actually goes to the doctor (which is influenced by who has access to healthcare, can afford their co-pay, doesn't experience racial or gender-based medical discrimination, and more)! If you are likely to go to the doctor for an *accidental injury*, then you are likely to also go the doctor when you are having a stroke.\n",
    "\n",
    "\n",
    "然而，只有前两名与中风有关!根据我们到目前为止的研究，你可能会猜到为什么。我们还没有真正测量 * 中风 *，这种情况发生在大脑的一个区域由于血液供应中断而被剥夺氧气的情况下。我们测量的是谁有症状，去看医生，做了适当的检查，* 并 * 得到了中风的诊断。实际上，中风并不是唯一与这个完整列表相关的事情 -- 它也与真正去看医生的人相关医疗保健，能够负担得起他们的共同支付，不经历种族或基于性别的医疗歧视，等等)!如果你可能因为意外伤害去看医生，那么当你中风时，你也可能去看医生。[机器翻译]\n",
    "\n",
    "This is an example of *measurement bias*. It occurs when our models make mistakes because we are measuring the wrong thing, or measuring it in the wrong way, or incorporating that measurement into the model inappropriately.\n",
    "\n",
    "这是 * 测量偏差 * 的示例。当我们的模型出错时，就会发生这种情况，因为我们测量了错误的东西，或者以错误的方式测量了它，或者不恰当地将这种测量纳入了模型。[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregation bias\n",
    "\n",
    "聚合偏差[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Aggregation bias* occurs when models do not aggregate data in a way that incorporates all of the appropriate factors, or when a model does not include the necessary interaction terms, nonlinearities, or so forth. This can particularly occur in medical settings. For instance, the way diabetes is treated is often based on simple univariate statistics and studies involving small groups of heterogeneous people. Analysis of results is often done in a way that does not take account of different ethnicities or genders. However, it turns out that diabetes patients have [different complications across ethnicities](https://www.ncbi.nlm.nih.gov/pubmed/24037313), and HbA1c levels (widely used to diagnose and monitor diabetes) [differ in complex ways across ethnicities and genders](https://www.ncbi.nlm.nih.gov/pubmed/22238408). This can result in people being misdiagnosed or incorrectly treated because medical decisions are based on a model that does not include these important variables and interactions.\n",
    "\n",
    "* 聚合偏差 * 当模型没有以包含所有适当因素的方式聚合数据时，或者当模型不包括必要的交互项、非线性等时，会发生聚合偏差。这尤其可能发生在医疗环境中。例如，糖尿病的治疗方式通常基于简单的单变量统计和涉及一小群人的研究。结果分析通常是以不考虑不同种族或性别的方式进行的。然而，事实证明，糖尿病患者有 [不同种族的并发症] ( https://www.ncbi.nlm.nih.gov/pubmed/24037313 ) 和HbA1c水平 (广泛用于诊断和监测糖尿病) [在不同种族和性别中存在复杂的差异] ( https://www.ncbi.nlm.nih.gov/pubmed/22238408 )。这可能导致人们被误诊或错误治疗，因为医疗决策是基于不包括这些重要变量和相互作用的模型。[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Representation bias\n",
    "\n",
    "# 表征偏差[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The abstract of the paper [\"Bias in Bios: A Case Study of Semantic Representation Bias in a High-Stakes Setting\"](https://arxiv.org/abs/1901.09451) by Maria De-Arteaga et al. notes that there is gender imbalance in occupations (e.g., females are more likely to be nurses, and males are more likely to be pastors), and says that: \"differences in true positive rates between genders are correlated with existing gender imbalances in occupations, which may compound these imbalances.\"\n",
    "\n",
    "\n",
    "论文摘要 [“Bios中的偏见: 高风险设置中语义表征偏见的案例研究”] ( https://arxiv.org/abs/1901.09451 ) By Maria De-Arteaga等人指出，职业中存在性别不平衡 (e.g.，女性更有可能成为护士，而男性更有可能成为牧师)，并说: “性别之间的真实阳性率差异与职业中现有的性别不平衡相关，这可能会使这些不平衡加剧。\"[机器翻译]\n",
    "\n",
    "In other words, the researchers noticed that models predicting occupation did not only *reflect* the actual gender imbalance in the underlying population, but actually *amplified* it! This type of *representation bias* is quite common, particularly for simple models. When there is some clear, easy-to-see underlying relationship, a simple model will often simply assume that this relationship holds all the time. As <<representation_bias>> from the paper shows, for occupations that had a higher percentage of females, the model tended to overestimate the prevalence of that occupation.\n",
    "\n",
    "换句话说，研究人员注意到，预测职业的模型不仅 * 反映了 * 底层人群中实际的性别不平衡，而且实际上 * 放大了 * 它!这种类型的 * 表示偏差 * 非常常见，特别是对于简单模型。当存在一些清晰、易于理解的潜在关系时，一个简单的模型通常会简单地假设这种关系一直存在。如 <代表性偏见>> 所示，对于女性比例较高的职业，该模型倾向于高估该职业的患病率。[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image12.png\" id=\"representation_bias\" caption=\"Model error in predicting occupation plotted against percentage of women in said occupation\" alt=\"Graph showing how model predictions overamplify existing bias\" width=\"500\">\n",
    "\n",
    "<Img src = \"images/ethical/image12.png\" id = \"representation_bias\" caption = \"模型错误预测职业与所述职业中的女性百分比\" alt = \"图表显示如何建模预测过度放大现有偏差“ width = \"500\">[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, in the training dataset 14.6% of surgeons were women, yet in the model predictions only 11.6% of the true positives were women. The model is thus amplifying the bias existing in the training set.\n",
    "\n",
    "\n",
    "例如，在训练数据集中，14.6% 的外科医生是女性，然而在模型预测中，只有11.6% 的真阳性是女性。因此，该模型放大了训练集中存在的偏差。[机器翻译]\n",
    "\n",
    "Now that we've seen that those biases exist, what can we do to mitigate them?\n",
    "\n",
    "既然我们已经看到这些偏见存在，我们能做些什么来减轻它们呢？[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing different types of bias\n",
    "\n",
    "# 解决不同类型的偏差[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different types of bias require different approaches for mitigation. While gathering a more diverse dataset can address representation bias, this would not help with historical bias or measurement bias.  All datasets contain bias.  There is no such thing as a completely debiased dataset.  Many researchers in the field have been converging on a set of proposals to enable better documentation of the decisions, context, and specifics about how and why a particular dataset was created, what scenarios it is appropriate to use in, and what the limitations are. This way, those using a particular dataset will not be caught off guard by its biases and limitations.\n",
    "\n",
    "不同类型的偏差需要不同的缓解方法。虽然收集更多样化的数据集可以解决表示偏差，但这对历史偏差或测量偏差没有帮助。所有数据集都包含偏差。没有完全去偏数据集这样的东西。该领域的许多研究人员已经聚集在一组建议上，以便更好地记录决策、背景和关于如何以及为什么创建特定数据集的细节，它适合在哪些场景中使用，以及有哪些限制。这样，那些使用特定数据集的人不会因为其偏见和限制而措手不及。[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often hear the question—\"Humans are biased, so does algorithmic bias even matter?\" This comes up so often, there must be some reasoning that makes sense to the people that ask it, but it doesn't seem very logically sound to us! Independently of whether this is logically sound, it's important to realize that algorithms (particularly machine learning algorithms!) and people are different. Consider these points about machine learning algorithms:\n",
    "\n",
    "\n",
    "我们经常听到这样一个问题 -- “人类有偏见，所以算法偏见很重要吗？“ 这种情况经常出现，一定有一些推理对提出这个问题的人有意义，但对我们来说，这似乎不太符合逻辑!不管这在逻辑上是否合理，重要的是要意识到算法 (尤其是机器学习算法!) 和人是不同的。考虑机器学习算法的以下几点:[机器翻译]\n",
    "\n",
    "  - _Machine learning can create feedback loops_:: Small amounts of bias can rapidly increase exponentially due to feedback loops.\n",
    "  - _Machine learning can amplify bias_:: Human bias can lead to larger amounts of machine learning bias.\n",
    "  - _Algorithms & humans are used differently_:: Human decision makers and algorithmic decision makers are not used in a plug-and-play interchangeable way in practice.\n",
    "  - _Technology is power_:: And with that comes responsibility.\n",
    "\n",
    "\n",
    "-_ 机器学习可以创建反馈循环 _:: 由于反馈循环，少量偏差可以以指数方式快速增加。\n",
    "-_ 机器学习可以放大偏差 _:: 人为偏差会导致更多的机器学习偏差。\n",
    "-_ 算法 & 人类的使用方式不同 _:: 人类决策者和算法决策者在实践中不是以即插即用的可互换方式使用的。\n",
    "-_ 技术就是力量 _:: 随之而来的是责任。[机器翻译]\n",
    "\n",
    "As the Arkansas healthcare example showed, machine learning is often implemented in practice not because it leads to better outcomes, but because it is cheaper and more efficient. Cathy O'Neill, in her book *Weapons of Math Destruction* (Crown), described the pattern of how the privileged are processed by people, whereas the poor are processed by algorithms. This is just one of a number of ways that algorithms are used differently than human decision makers. Others include:\n",
    "\n",
    "\n",
    "正如阿肯色州医疗保健的例子所显示的，机器学习通常在实践中实施，不是因为它能带来更好的结果，而是因为它更便宜、更高效。凯茜 · 奥尼尔在她的书 * 数学毁灭的武器 * (皇冠) 中描述了特权是如何被人们处理的模式，而穷人是如何被算法处理的。这只是不同于人类决策者使用算法的多种方式之一。其他包括:[机器翻译]\n",
    "\n",
    "  - People are more likely to assume algorithms are objective or error-free (even if they’re given the option of a human override).\n",
    "  - Algorithms are more likely to be implemented with no appeals process in place.\n",
    "  - Algorithms are often used at scale.\n",
    "  - Algorithmic systems are cheap.\n",
    "\n",
    "\n",
    "-人们更有可能假设算法是客观的或无错误的 (即使他们被赋予了人类超驰的选项)。\n",
    "-算法更有可能在没有上诉程序的情况下实施。\n",
    "-算法经常在规模上使用。\n",
    "-算法系统便宜。[机器翻译]\n",
    "\n",
    "Even in the absence of bias, algorithms (and deep learning especially, since it is such an effective and scalable algorithm) can lead to negative societal problems, such as when used for *disinformation*.\n",
    "\n",
    "即使在没有偏见的情况下，算法 (特别是深度学习，因为它是如此有效和可扩展的算法) 也会导致负面的社会问题，例如用于 * 虚假信息 *。[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disinformation\n",
    "\n",
    "# 造谣[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Disinformation* has a history stretching back hundreds or even thousands of years. It is not necessarily about getting someone to believe something false, but rather often used to sow disharmony and uncertainty, and to get people to give up on seeking the truth.  Receiving conflicting accounts can lead people to assume that they can never know whom or what to trust.\n",
    "\n",
    "\n",
    "* 虚假信息 * 有一个历史可以追溯到数百年甚至数千年。这不一定是为了让某人相信一些虚假的东西，而是经常用来散布不和谐和不确定性，让人们放弃寻求真相。收到冲突的账户会导致人们认为他们永远不知道该信任谁或什么。[机器翻译]\n",
    "\n",
    "Some people think disinformation is primarily about false information or *fake news*, but in reality, disinformation can often contain seeds of truth, or half-truths taken out of context. Ladislav Bittman was an intelligence officer in the USSR who later defected to the US and wrote some books in the 1970s and 1980s on the role of disinformation in Soviet propaganda operations. In *The KGB and Soviet Disinformation* (Pergamon) he wrote, \"Most campaigns are a carefully designed mixture of facts, half-truths, exaggerations, and deliberate lies.\"\n",
    "\n",
    "\n",
    "有些人认为虚假信息主要是关于虚假信息或虚假新闻 *，但实际上，虚假信息往往包含真相的种子，或断章取义的半真半假。拉迪斯拉夫 · 比特曼 (Ladislav Bittman) 是苏联的一名情报官员，后来叛逃到美国，并在20世纪70年代和80年代写了一些关于虚假信息在苏联宣传行动中的作用的书。在克格勃和苏联的虚假信息中，他写道，“大多数竞选活动都是精心设计的事实、半真半假、夸张和蓄意谎言的混合物。”[机器翻译]\n",
    "\n",
    "In the US this has hit close to home in recent years, with the FBI detailing a massive disinformation campaign linked to Russia in the 2016 election. Understanding the disinformation that was used in this campaign is very educational. For instance, the FBI found that the Russian disinformation campaign often organized two separate fake \"grass roots\" protests, one for each side of an issue, and got them to protest at the same time! The [*Houston Chronicle*](https://www.houstonchronicle.com/local/gray-matters/article/A-Houston-protest-organized-by-Russian-trolls-12625481.php) reported on one of these odd events (<<texas>>).\n",
    "\n",
    "\n",
    "近年来，在美国，这一事件发生在离家近的地方，联邦调查局在2016的选举中详细描述了一场与俄罗斯有关联的大规模虚假信息运动。理解这场运动中使用的虚假信息非常有教育意义。例如，联邦调查局发现俄罗斯的虚假信息运动经常组织两次单独的假 “草根” 抗议活动，一次针对一个问题的每一方，并让他们同时抗议!The [* Houston Chronicle *]( https://www.houstonchronicle.com/local/gray-matters/article/A-Houston-protest-organized-by-Russian-trolls-12625481.php ) 报告了这些奇怪事件之一 (<<texas>>)。[机器翻译]\n",
    "\n",
    "> : A group that called itself the \"Heart of Texas\" had organized it on social media—a protest, they said, against the \"Islamization\" of Texas. On one side of Travis Street, I found about 10 protesters. On the other side, I found around 50 counterprotesters. But I couldn't find the rally organizers. No \"Heart of Texas.\" I thought that was odd, and mentioned it in the article: What kind of group is a no-show at its own event? Now I know why. Apparently, the rally's organizers were in Saint Petersburg, Russia, at the time. \"Heart of Texas\" is one of the internet troll groups cited in Special Prosecutor Robert Mueller's recent indictment of Russians attempting to tamper with the U.S. presidential election.\n",
    "\n",
    ">: 一个自称为 “德克萨斯之心” 的团体在社交媒体上组织了它 -- 他们说，这是反对德克萨斯 “伊斯兰化” 的抗议。在特拉维斯街的一边，我发现了大约10名抗议者。在另一边，我发现了大约50名反抗议者。但是我找不到集会组织者。没有 “德克萨斯的心脏”我觉得这很奇怪，并在文章中提到了这一点: 什么样的团体在自己的活动中没有出现？现在我知道为什么了。显然，集会的组织者当时在俄罗斯圣彼得堡。“德克萨斯之心” 是特别检察官罗伯特 · 穆勒最近对俄罗斯人试图篡改美国总统选举的指控中引用的互联网巨魔团体之一。[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image13.png\" id=\"texas\" caption=\"Event organized by the group Heart of Texas\" alt=\"Screenshot of an event organized by the group Heart of Texas\" width=\"300\">\n",
    "\n",
    "<Img src = \"images/ethics/image13.png\" id = \"texas\" caption = \"group Heart of Texas组织的活动\" alt = \"group Heart组织的活动截图德州的\" 宽度 = \"300\">[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disinformation often involves coordinated campaigns of inauthentic behavior.  For instance, fraudulent accounts may try to make it seem like many people hold a particular viewpoint.  While most of us like to think of ourselves as independent-minded, in reality we evolved to be influenced by others in our in-group, and in opposition to those in our out-group.  Online discussions can influence our viewpoints, or alter the range of what we consider acceptable viewpoints. Humans are social animals, and as social animals we are extremely influenced by the people around us. Increasingly, radicalization occurs in online environments; influence is coming from people in the virtual space of online forums and social networks.\n",
    "\n",
    "\n",
    "虚假信息通常涉及不真实行为的协调运动。例如，欺诈账户可能试图让许多人似乎持有特定的观点。虽然我们大多数人喜欢认为自己是独立的，但实际上，我们进化成受到团队内其他人的影响，与团队外的人相反。在线讨论可以影响我们的观点，或者改变我们认为可接受的观点的范围。人类是群居动物，作为群居动物，我们受到周围人的极大影响。激进化越来越多地发生在网络环境中; 影响来自在线论坛和社交网络虚拟空间中的人们。[机器翻译]\n",
    "\n",
    "Disinformation through autogenerated text is a particularly significant issue, due to the greatly increased capability provided by deep learning. We discuss this issue in depth when we delve into creating language models, in <<chapter_nlp>>.\n",
    "\n",
    "\n",
    "由于深度学习提供的能力大大增加，通过自动生成的文本产生的虚假信息是一个特别重要的问题。当我们在 <chapter_nlp>> 中深入研究创建语言模型时，我们会深入讨论这个问题。[机器翻译]\n",
    "\n",
    "One proposed approach is to develop some form of digital signature, to implement it in a seamless way, and to create norms that we should only trust content that has been verified. The head of the Allen Institute on AI, Oren Etzioni, wrote such a proposal in an article titled [\"How Will We Prevent AI-Based Forgery?\"](https://hbr.org/2019/03/how-will-we-prevent-ai-based-forgery): \"AI is poised to make high-fidelity forgery inexpensive and automated, leading to potentially disastrous consequences for democracy, security, and society. The specter of AI forgery means that we need to act to make digital signatures de rigueur as a means of authentication of digital content.\"\n",
    "\n",
    "\n",
    "一种提议的方法是开发某种形式的数字签名，以无缝的方式实现它，并创建我们应该只信任已经过验证的内容的规范。艾伦AI研究所的负责人Oren Etzioni在一篇名为 [“我们将如何防止基于AI的伪造？”] ( https://hbr.org/2019/03/how-will-we-prevent-ai-based-forgery ): “人工智能准备让高保真伪造变得廉价和自动化，给民主、安全和社会带来潜在的灾难性后果。人工智能伪造的幽灵意味着我们需要采取行动，使数字签名成为数字内容认证的一种手段。\"[机器翻译]\n",
    "\n",
    "Whilst we can't hope to discuss all the ethical issues that deep learning, and algorithms more generally, brings up, hopefully this brief introduction has been a useful starting point you can build on. We'll now move on to the questions of how to identify ethical issues, and what to do about them.\n",
    "\n",
    "虽然我们不能希望讨论深度学习和算法更广泛地提出的所有伦理问题，但希望这个简短的介绍是你可以建立的一个有用的起点。我们现在将继续讨论如何识别道德问题，以及如何处理这些问题。[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying and Addressing Ethical Issues\n",
    "\n",
    "# # 识别和解决伦理问题[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mistakes happen. Finding out about them, and dealing with them, needs to be part of the design of any system that includes machine learning (and many other systems too).  The issues raised within data ethics are often complex and interdisciplinary, but it is crucial that we work to address them.\n",
    "\n",
    "\n",
    "错误发生了。了解它们并处理它们，需要成为任何包括机器学习 (以及许多其他系统) 的系统设计的一部分。数据伦理中提出的问题通常是复杂和跨学科的，但我们必须努力解决这些问题。[机器翻译]\n",
    "\n",
    "So what can we do?  This is a big topic, but a few steps towards addressing ethical issues are:\n",
    "\n",
    "\n",
    "那我们能做什么呢？这是一个很大的话题，但是解决伦理问题的几个步骤是:[机器翻译]\n",
    "\n",
    "- Analyze a project you are working on.\n",
    "- Implement processes at your company to find and address ethical risks.\n",
    "- Support good policy.\n",
    "- Increase diversity.\n",
    "\n",
    "\n",
    "-分析你正在进行的项目。\n",
    "-在贵公司实施流程，以发现和解决道德风险。\n",
    "-支持良好的政策。\n",
    "-增加多样性。[机器翻译]\n",
    "\n",
    "Let's walk through each of these steps, starting with analyzing a project you are working on.\n",
    "\n",
    "让我们走完这些步骤中的每一个，从分析你正在进行的项目开始。[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze a Project You Are Working On\n",
    "\n",
    "# 分析您正在处理的项目[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to miss important issues when considering ethical implications of your work. One thing that helps enormously is simply asking the right questions. Rachel Thomas recommends considering the following questions throughout the development of a data project:\n",
    "\n",
    "\n",
    "当考虑你工作的道德含义时，很容易错过重要的问题。有一件事非常有帮助，那就是简单地问正确的问题。Rachel Thomas建议在整个数据项目开发过程中考虑以下问题:[机器翻译]\n",
    "\n",
    "  - Should we even be doing this?\n",
    "  - What bias is in the data?\n",
    "  - Can the code and data be audited?\n",
    "  - What are the error rates for different sub-groups?\n",
    "  - What is the accuracy of a simple rule-based alternative?\n",
    "  - What processes are in place to handle appeals or mistakes?\n",
    "  - How diverse is the team that built it?\n",
    "\n",
    "\n",
    "-我们应该这么做吗？\n",
    "-数据中有什么偏差？\n",
    "-代码和数据可以审计吗？\n",
    "-不同子组的错误率是多少？\n",
    "-简单的基于规则的替代方案的准确性是什么？\n",
    "-有什么程序来处理上诉或错误？\n",
    "-建造它的团队有多多样化？[机器翻译]\n",
    "\n",
    "These questions may be able to help you identify outstanding issues, and possible alternatives that are easier to understand and control. In addition to asking the right questions, it's also important to consider practices and processes to implement.\n",
    "\n",
    "\n",
    "这些问题可能有助于您确定悬而未决的问题，以及更容易理解和控制的可能替代方案。除了提出正确的问题之外，考虑要实施的实践和流程也很重要。[机器翻译]\n",
    "\n",
    "One thing to consider at this stage is what data you are collecting and storing. Data often ends up being used for different purposes than what it was originally collected for. For instance, IBM began selling to Nazi Germany well before the Holocaust, including helping with Germany’s 1933 census conducted by Adolf Hitler, which was effective at identifying far more Jewish people than had previously been recognized in Germany. Similarly, US census data was used to round up Japanese-Americans (who were US citizens) for internment during World War II. It is important to recognize how data and images collected can be weaponized later. Columbia professor [Tim Wu wrote](https://www.nytimes.com/2019/04/10/opinion/sunday/privacy-capitalism.html) that “You must assume that any personal data that Facebook or Android keeps are data that governments around the world will try to get or that thieves will try to steal.”\n",
    "\n",
    "在这个阶段要考虑的一件事是你正在收集和存储什么数据。数据最终通常被用于不同于最初收集目的的目的。例如，早在大屠杀之前，国际商用机器公司就开始向纳粹德国销售产品，包括帮助阿道夫·希特勒在1933进行的德国人口普查，这在识别犹太人方面比以前在德国被识别的要有效得多。同样，美国人口普查数据被用来在第二次世界大战期间围捕日裔美国人 (他们是美国公民) 进行拘留。重要的是要认识到收集的数据和图像以后如何武器化。哥伦比亚教授 [Tim Wu写道] ( https://www.nytimes.com/2019/04/10/opinion/sunday/privacy-capitalism.html ) “你必须假设脸谱网或安卓系统保存的任何个人数据都是世界各国政府试图获取或小偷试图窃取的数据。”[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processes to Implement\n",
    "\n",
    "# 要实施的流程[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Markkula Center has released [An Ethical Toolkit for Engineering/Design Practice](https://www.scu.edu/ethics-in-technology-practice/ethical-toolkit/) that includes some concrete practices to implement at your company, including regularly scheduled sweeps to proactively search for ethical risks (in a manner similar to cybersecurity penetration testing), expanding the ethical circle to include the perspectives of a variety of stakeholders, and considering the terrible people (how could bad actors abuse, steal, misinterpret, hack, destroy, or weaponize what you are building?). \n",
    "\n",
    "\n",
    "Markkula中心发布了 [工程/设计实践道德工具包] ( https://www.scu.edu/ethics-in-technology-practice/ethical-toolkit/ ) 这包括一些要在贵公司实施的具体做法，包括定期进行扫描以主动搜索道德风险 (以类似于网络安全渗透测试的方式)，扩大伦理圈，包括各种利益相关者的观点，并考虑可怕的人 (坏行为者如何虐待，偷窃，误解，黑客攻击，摧毁或武器化你正在建造的东西？)。[机器翻译]\n",
    "\n",
    "Even if you don't have a diverse team, you can still try to pro-actively include the perspectives of a wider group, considering questions such as these (provided by the Markkula Center):\n",
    "\n",
    "\n",
    "即使你没有一个多样化的团队，你仍然可以考虑这样的问题 (由马库拉中心提供)，积极主动地纳入更广泛群体的观点:[机器翻译]\n",
    "\n",
    "  - Whose interests, desires, skills, experiences, and values have we simply assumed, rather than actually consulted?\n",
    "  - Who are all the stakeholders who will be directly affected by our product? How have their interests been protected? How do we know what their interests really are—have we asked?\n",
    "  - Who/which groups and individuals will be indirectly affected in significant ways?\n",
    "  - Who might use this product that we didn’t expect to use it, or for purposes we didn’t initially intend?\n",
    "\n",
    "-我们简单地假设了谁的兴趣、愿望、技能、经验和价值观，而不是实际咨询了谁？\n",
    "-谁是所有会直接受到我们产品影响的利益相关者？他们的利益是如何得到保护的？我们怎么知道他们真正的兴趣是什么-我们问过吗？\n",
    "-谁/哪些群体和个人将受到重大间接影响？\n",
    "-谁会使用我们没想到会使用的这种产品，或者用于我们最初不打算使用的目的？[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ethical lenses\n",
    "\n",
    "# 道德镜片[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful resource from the Markkula Center is its [Conceptual Frameworks in Technology and Engineering Practice](https://www.scu.edu/ethics-in-technology-practice/conceptual-frameworks/). This considers how different foundational ethical lenses can help identify concrete issues, and lays out the following approaches and key questions:\n",
    "\n",
    "\n",
    "马库拉中心的另一个有用资源是它的 [技术和工程实践中的概念框架] ( https://www.scu.edu/ethics-in-technology-practice/conceptual-frameworks/ )。这考虑了不同的基本道德透镜如何帮助识别具体问题，并列出了以下方法和关键问题:[机器翻译]\n",
    "\n",
    "  - The rights approach:: Which option best respects the rights of all who have a stake?\n",
    "  - The justice approach:: Which option treats people equally or proportionately?\n",
    "  - The utilitarian approach:: Which option will produce the most good and do the least harm?\n",
    "  - The common good approach:: Which option best serves the community as a whole, not just some members?\n",
    "  - The virtue approach:: Which option leads me to act as the sort of person I want to be?\n",
    "\n",
    "\n",
    "-权利方法: 哪个选项最尊重所有有利害关系的人的权利？\n",
    "-司法方法:: 哪一种选择平等或按比例对待人？\n",
    "-功利主义方法:: 哪个选项会产生最大的好处和伤害最小？\n",
    "-共同的好方法:: 哪个选项最适合整个社区，而不仅仅是一些成员？\n",
    "-美德方法:: 哪个选项让我成为我想成为的那种人？[机器翻译]\n",
    "\n",
    "Markkula's recommendations include a deeper dive into each of these perspectives, including looking at a project through the lenses of its *consequences*:\n",
    "\n",
    "\n",
    "Markkula的建议包括更深入地了解这些观点，包括从项目的 * 后果 * 的角度来看待项目:[机器翻译]\n",
    "\n",
    "  - Who will be directly affected by this project? Who will be indirectly affected?\n",
    "  - Will the effects in aggregate likely create more good than harm, and what types of good and harm?\n",
    "  - Are we thinking about all relevant types of harm/benefit (psychological, political, environmental, moral, cognitive, emotional, institutional, cultural)?\n",
    "  - How might future generations be affected by this project?\n",
    "  - Do the risks of harm from this project fall disproportionately on the least powerful in society? Will the benefits go disproportionately to the well-off?\n",
    "  - Have we adequately considered \"dual-use\"?\n",
    "\n",
    "\n",
    "-谁会直接受到这个项目的影响？谁会受到间接影响？\n",
    "-总体上的影响可能会产生比伤害更多的好处，以及什么类型的好处和坏处？\n",
    "-我们是否在考虑所有相关类型的伤害/利益 (心理、政治、环境、道德、认知、情感、制度、文化)？\n",
    "-这个项目会对后代产生怎样的影响？\n",
    "-这个项目的危害风险是否不成比例地落在社会上最不强大的人身上？福利会不成比例地流向富人吗？\n",
    "-我们是否充分考虑过 “两用”？[机器翻译]\n",
    "\n",
    "The alternative lens to this is the *deontological* perspective, which focuses on basic concepts of *right* and *wrong*:\n",
    "\n",
    "\n",
    "对此的替代镜头是 * 义务 * 视角，它侧重于 * 对 * 和 * 错 * 的基本概念:[机器翻译]\n",
    "\n",
    "  - What rights of others and duties to others must we respect?\n",
    "  - How might the dignity and autonomy of each stakeholder be impacted by this project?\n",
    "  - What considerations of trust and of justice are relevant to this design/project?\n",
    "  - Does this project involve any conflicting moral duties to others, or conflicting stakeholder rights? How can we prioritize these?\n",
    "\n",
    "\n",
    "-我们必须尊重他人的哪些权利和对他人的义务？\n",
    "-这个项目会如何影响每个利益相关者的尊严和自主权？\n",
    "-什么信任和公正的考虑与这个设计/项目相关？\n",
    "-这个项目是否涉及对他人的任何冲突的道德义务，或冲突的利益相关者权利？我们如何对这些进行优先排序？[机器翻译]\n",
    "\n",
    "One of the best ways to help come up with complete and thoughtful answers to questions like these is to ensure that the people asking the questions are *diverse*.\n",
    "\n",
    "帮助对这些问题提出完整和深思熟虑的答案的最好方法之一是确保提问的人是 * 多样化的 *。[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Power of Diversity\n",
    "\n",
    "# 多样性的力量[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, less than 12% of AI researchers are women, according to [a study from Element AI](https://medium.com/element-ai-research-lab/estimating-the-gender-ratio-of-ai-researchers-around-the-world-81d2b8dbe9c3). The statistics are similarly dire when it comes to race and age. When everybody on a team has similar backgrounds, they are likely to have similar blindspots around ethical risks. The *Harvard Business Review* (HBR) has published a number of studies showing many benefits of diverse teams, including:\n",
    "\n",
    "\n",
    "目前，只有不到12% 的AI研究人员是女性，根据 [来自Element AI的一项研究] ( https://medium.com/element-ai-research-lab/estimating-the-gender-ratio-of-ai-researchers-around-the-world-81d2b8dbe9c3 )。谈到种族和年龄，统计数据也同样糟糕。当团队中的每个人都有相似的背景时，他们很可能在道德风险方面有相似的盲点。* 《哈佛商业评论》 * (HBR) 发表了许多研究，显示了不同团队的许多好处，包括:[机器翻译]\n",
    "\n",
    "- [\"How Diversity Can Drive Innovation\"](https://hbr.org/2013/12/how-diversity-can-drive-innovation)\n",
    "- [\"Teams Solve Problems Faster When They’re More Cognitively Diverse\"](https://hbr.org/2017/03/teams-solve-problems-faster-when-theyre-more-cognitively-diverse)\n",
    "- [\"Why Diverse Teams Are Smarter\"](https://hbr.org/2016/11/why-diverse-teams-are-smarter), and\n",
    "- [\"Defend Your Research: What Makes a Team Smarter? More Women\"](https://hbr.org/2011/06/defend-your-research-what-makes-a-team-smarter-more-women)\n",
    "\n",
    "\n",
    "-- [“多样性如何驱动创新”] ( https://hbr.org/2013/12/how-diversity-can-drive-innovation )\n",
    "-[“当团队在认知上更加多样化时，他们会更快地解决问题”] ( https://hbr.org/2017/03/teams-solve-problems-faster-when-theyre-more-cognitively-diverse )\n",
    "-[“为什么多样化的团队更聪明”] ( https://hbr.org/2016/11/why-diverse-teams-are-smarter ) 和\n",
    "-[“捍卫你的研究: 是什么让团队更聪明？更多女性 ”] ( https://hbr.org/2011/06/defend-your-research-what-makes-a-team-smarter-more-women )[机器翻译]\n",
    "\n",
    "Diversity can lead to problems being identified earlier, and a wider range of solutions being considered. For instance, Tracy Chou was an early engineer at Quora. She [wrote of her experiences](https://qz.com/1016900/tracy-chou-leading-silicon-valley-engineer-explains-why-every-tech-worker-needs-a-humanities-education/), describing how she advocated internally for adding a feature that would allow trolls and other bad actors to be blocked. Chou recounts, “I was eager to work on the feature because I personally felt antagonized and abused on the site (gender isn’t an unlikely reason as to why)... But if I hadn’t had that personal perspective, it’s possible that the Quora team wouldn’t have prioritized building a block button so early in its existence.” Harassment often drives people from marginalized groups off online platforms, so this functionality has been important for maintaining the health of Quora's community.\n",
    "\n",
    "\n",
    "多样性会导致问题被提前发现，并考虑更广泛的解决方案。例如，Tracy Chou是Quora的早期工程师。她 [写下了她的经历] ( https://qz.com/1016900/tracy-chou-leading-silicon-valley-engineer-explains-why-every-tech-worker-needs-a-humanities-education/ )，描述了她如何在内部倡导增加一个功能，允许巨魔和其他坏演员被阻止。周回忆道，“我渴望研究这个功能，因为我个人在网站上感到被激怒和虐待 (性别不是一个不太可能的原因).但是如果我没有那种个人观点，Quora团队可能不会在它存在的这么早就优先建立一个块按钮。\"骚扰通常会将边缘化群体的人赶出在线平台，因此这一功能对维护Quora社区的健康非常重要。[机器翻译]\n",
    "\n",
    "A crucial aspect to understand is that women leave the tech industry at over twice the rate that men do, according to the [*Harvard Business Review*](https://www.researchgate.net/publication/268325574_By_RESEARCH_REPORT_The_Athena_Factor_Reversing_the_Brain_Drain_in_Science_Engineering_and_Technology) (41% of women working in tech leave, compared to 17% of men). An analysis of over 200 books, white papers, and articles found that the reason they leave is that “they’re treated unfairly; underpaid, less likely to be fast-tracked than their male colleagues, and unable to advance.” \n",
    "\n",
    "\n",
    "至关重要的方面要理解的是，妇女离开科技行业超过两倍的男人，根据 [* 《哈佛商业评论》 (Harvard Business Review) *]( https://www.researchgate.net/publication/268325574_By_RESEARCH_REPORT_The_Athena_Factor_Reversing_the_Brain_Drain_in_Science_Engineering_and_Technology ) (41% 的女性休技术假，而男性为17%)。对200多本书、白皮书和文章的分析发现，他们离开的原因是 “他们受到了不公平的待遇; 报酬过低，比他们的男性同事更不容易被快速跟踪，并且无法前进。\"[机器翻译]\n",
    "\n",
    "Studies have confirmed a number of the factors that make it harder for women to advance in the workplace. Women receive more vague feedback and personality criticism in performance evaluations, whereas men receive actionable advice tied to business outcomes (which is more useful). Women frequently experience being excluded from more creative and innovative roles, and not receiving high-visibility “stretch” assignments that are helpful in getting promoted. One study found that men’s voices are perceived as more persuasive, fact-based, and logical than women’s voices, even when reading identical scripts.\n",
    "\n",
    "\n",
    "研究证实了一些使妇女在工作场所更难进步的因素。女性在绩效评估中得到更模糊的反馈和个性批评，而男性得到与商业成果相关的可操作建议 (这更有用)。女性经常经历被排除在更具创造性和创新性的角色之外，没有接受有助于升职的高知名度 “延伸” 任务。一项研究发现，男性的声音比女性的声音更有说服力、更基于事实、更符合逻辑，即使阅读相同的剧本。[机器翻译]\n",
    "\n",
    "Receiving mentorship has been statistically shown to help men advance, but not women. The reason behind this is that when women receive mentorship, it’s advice on how they should change and gain more self-knowledge. When men receive mentorship, it’s public endorsement of their authority. Guess which is more useful in getting promoted?\n",
    "\n",
    "\n",
    "统计显示，接受导师帮助男性进步，但不是女性。这背后的原因是，当女性接受指导时，这是关于她们应该如何改变和获得更多自我知识的建议。当男人接受指导时，这是对他们权威的公开认可。猜猜哪个更适合升职？[机器翻译]\n",
    "\n",
    "As long as qualified women keep dropping out of tech, teaching more girls to code will not solve the diversity issues plaguing the field. Diversity initiatives often end up focusing primarily on white women, even though women of color face many additional barriers. In [interviews](https://worklifelaw.org/publications/Double-Jeopardy-Report_v6_full_web-sm.pdf) with 60 women of color who work in STEM research, 100% had experienced discrimination.\n",
    "\n",
    "只要合格的女性不断退出科技，教更多的女孩编码并不能解决困扰该领域的多样性问题。多样性倡议最终往往主要关注白人女性，尽管有色人种女性面临许多额外的障碍。在【访谈】 ( https://worklifelaw.org/publications/Double-Jeopardy-Report_v6_full_web-sm.pdf ) 有60名有色人种女性在STEM研究中工作，100% 的人经历过歧视。[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hiring process is particularly broken in tech. One study indicative of the disfunction comes from Triplebyte, a company that helps place software engineers in companies, conducting a standardized technical interview as part of this process. They have a fascinating dataset: the results of how over 300 engineers did on their exam, coupled with the results of how those engineers did during the interview process for a variety of companies. The number one finding from [Triplebyte’s research](https://triplebyte.com/blog/who-y-combinator-companies-want) is that “the types of programmers that each company looks for often have little to do with what the company needs or does. Rather, they reflect company culture and the backgrounds of the founders.”\n",
    "\n",
    "\n",
    "在科技领域，招聘过程尤其糟糕。一项表明功能障碍的研究来自Triplebyte，一家帮助公司安置软件工程师的公司，作为这一过程的一部分进行标准化的技术面试。他们有一个迷人的数据集: 300多名工程师在考试中的表现，以及这些工程师在各种公司的面试过程中的表现。从 [Triplebyte的研究] 中得出的第一个发现 ( https://triplebyte.com/blog/who-y-combinator-companies-want ) 是 “每个公司寻找的程序员类型通常与公司需要或做的事情没有什么关系。相反，它们反映了公司文化和创始人的背景。\"[机器翻译]\n",
    "\n",
    "This is a challenge for those trying to break into the world of deep learning, since most companies' deep learning groups today were founded by academics. These groups tend to look for people \"like them\"—that is, people that can solve complex math problems and understand dense jargon. They don't always know how to spot people who are actually good at solving real problems using deep learning.\n",
    "\n",
    "\n",
    "对于那些试图进入深度学习世界的人来说，这是一个挑战，因为今天大多数公司的深度学习小组都是由学者创建的。这些群体倾向于寻找 “喜欢他们” 的人 -- 也就是说，能够解决复杂的数学问题并理解密集行话的人。他们并不总是知道如何发现那些真正擅长使用深度学习解决实际问题的人。[机器翻译]\n",
    "\n",
    "This leaves a big opportunity for companies that are ready to look beyond status and pedigree, and focus on results!\n",
    "\n",
    "这给那些准备超越地位和血统，专注于结果的公司留下了一个巨大的机会![机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness, Accountability, and Transparency\n",
    "\n",
    "# 公平、责任和透明[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The professional society for computer scientists, the ACM, runs a data ethics conference called the Conference on Fairness, Accountability, and Transparency. \"Fairness, Accountability, and Transparency\" which used to go under the acronym *FAT* but now uses to the less objectionable *FAccT*. Microsoft has a group focused on \"Fairness, Accountability, Transparency, and Ethics\" (FATE). In this section, we'll use \"FAccT\" to refer to the concepts of *Fairness, Accountability, and Transparency*.\n",
    "\n",
    "\n",
    "计算机科学家专业协会ACM举办了一个名为 “公平、问责和透明会议” 的数据伦理会议。“公平、问责和透明”，过去用的是首字母缩略词 * FAT *，但现在用的是不太令人反感的 * FAccT *。微软有一个专注于 “公平、问责、透明和道德” (命运) 的小组。在本节中，我们将使用 “FAccT” 来参考 * 公平性、问责制和透明度 * 的概念。[机器翻译]\n",
    "\n",
    "FAccT is another lens that you may find useful in considering ethical issues. One useful resource for this is the free online book [*Fairness and Machine Learning: Limitations and Opportunities*](https://fairmlbook.org/) by Solon Barocas, Moritz Hardt, and Arvind Narayanan, which \"gives a perspective on machine learning that treats fairness as a central concern rather than an afterthought.\" It also warns, however, that it \"is intentionally narrow in scope... A narrow framing of machine learning ethics might be tempting to technologists and businesses as a way to focus on technical interventions while sidestepping deeper questions about power and accountability. We caution against this temptation.\" Rather than provide an overview of the FAccT approach to ethics (which is better done in books such as that one), our focus here will be on the limitations of this kind of narrow framing.\n",
    "\n",
    "\n",
    "FAccT是另一个镜头，你可能会发现在考虑伦理问题有用。一个有用的资源是免费的在线书籍 [* 公平和机器学习: 限制和机会 *]( https://fairmlbook.org/ ) 作者梭伦 · 巴罗卡斯、莫里茨 · 哈特和阿尔温德 · 纳拉亚南，他们 “从机器学习的角度出发，将公平视为核心问题，而不是事后考虑。”然而，它也警告说，它 “故意缩小范围”.机器学习伦理的狭隘框架可能会吸引技术专家和企业，作为一种关注技术干预的方式，同时避开关于权力和问责制的更深层次的问题。我们警惕这种诱惑。\"我们在这里的重点将放在这种狭窄框架的局限性上，而不是提供对FAccT方法的伦理的概述 (这在诸如此类的书中更好)。[机器翻译]\n",
    "\n",
    "One great way to consider whether an ethical lens is complete is to try to come up with an example where the lens and our own ethical intuitions give diverging results. Os Keyes, Jevan Hutson, and Meredith Durbin explored this in a graphic way in their paper [\"A Mulching Proposal:\n",
    "Analysing and Improving an Algorithmic System for Turning the Elderly into High-Nutrient Slurry\"](https://arxiv.org/abs/1908.06166). The paper's abstract says:\n",
    "\n",
    "考虑道德镜头是否完整的一个好方法是尝试想出一个例子，在这个例子中，镜头和我们自己的道德直觉给出了不同的结果。Os Keyes，Jevan Hutson和Meredith Durbin在他们的论文 [“覆盖提案:\n",
    "分析和改进一个将老年人变成高营养泥浆的算法系统 ”] ( https://arxiv.org/abs/1908.06166 )。论文摘要说:[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> : The ethical implications of algorithmic systems have been much discussed in both HCI and the broader community of those interested in technology design, development and policy. In this paper, we explore the application of one prominent ethical framework - Fairness, Accountability, and Transparency - to a proposed algorithm that resolves various societal issues around food security and population aging. Using various standardised forms of algorithmic audit and evaluation, we drastically increase the algorithm's adherence to the FAT framework, resulting in a more ethical and beneficent system. We discuss how this might serve as a guide to other researchers or practitioners looking to ensure better ethical outcomes from algorithmic systems in their line of work.\n",
    "\n",
    ">: 算法系统的伦理含义在HCI和更广泛的对技术设计、开发和政策感兴趣的社区中已经讨论了很多。在本文中，我们探讨了一个突出的伦理框架 -- 公平性、问责制和透明度 -- 在解决围绕粮食安全和人口老龄化的各种社会问题的拟议算法中的应用。使用各种标准化形式的算法审计和评估，我们大大增加了算法对FAT框架的依从性，从而形成了一个更加道德和仁慈的系统。我们讨论了如何作为其他研究人员或从业者的指南，以确保他们的工作中算法系统获得更好的伦理结果。[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this paper, the rather controversial proposal (\"Turning the Elderly into High-Nutrient Slurry\") and the results (\"drastically increase the algorithm's adherence to the FAT framework, resulting in a more ethical and beneficent system\") are at odds... to say the least!\n",
    "\n",
    "\n",
    "在本文中，相当有争议的建议 (“将老年人变成高营养浆”) 和结果 (“大幅增加算法对脂肪框架的坚持，导致一个更加道德和仁慈的制度”) 是不一致的.至少可以说![机器翻译]\n",
    "\n",
    "In philosophy, and especially philosophy of ethics, this is one of the most effective tools: first, come up with a process, definition, set of questions, etc., which is designed to resolve some problem. Then try to come up with an example where that apparent solution results in a proposal that no one would consider acceptable. This can then lead to a further refinement of the solution.\n",
    "\n",
    "\n",
    "在哲学中，尤其是伦理学哲学中，这是最有效的工具之一: 首先，提出一个过程、定义、一系列问题等。，旨在解决一些问题。然后试着想出一个例子，在这个例子中，这个明显的解决方案产生了一个没有人认为可以接受的建议。然后，这可以导致解决方案的进一步细化。[机器翻译]\n",
    "\n",
    "So far, we've focused on things that you and your organization can do. But sometimes individual or organizational action is not enough. Sometimes, governments also need to consider policy implications.\n",
    "\n",
    "到目前为止，我们专注于您和您的组织可以做的事情。但有时个人或组织行动是不够的。有时，政府也需要考虑政策影响。[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Role of Policy\n",
    "\n",
    "# # 策略的角色[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often talk to people who are eager for technical or design fixes to be a full solution to the kinds of problems that we've been discussing; for instance, a technical approach to debias data, or design guidelines for making technology less addictive. While such measures can be useful, they will not be sufficient to address the underlying problems that have led to our current state. For example, as long as it is incredibly profitable to create addictive technology, companies will continue to do so, regardless of whether this has the side effect of promoting conspiracy theories and polluting our information ecosystem. While individual designers may try to tweak product designs, we will not see substantial changes until the underlying profit incentives change.\n",
    "\n",
    "我们经常与那些渴望技术或设计修复成为我们一直在讨论的问题的完整解决方案的人交谈; 例如，去偏差数据的技术方法，或者让技术不那么上瘾的设计指南。虽然这些措施可能是有用的，但不足以解决导致我们目前状况的根本问题。例如，只要创造令人上瘾的技术是令人难以置信的盈利，公司就会继续这样做，不管这是否有促进阴谋论和污染我们的信息生态系统的副作用。虽然个别设计师可能会尝试调整产品设计，但在潜在的利润激励发生变化之前，我们不会看到实质性的变化。[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Effectiveness of Regulation\n",
    "\n",
    "# 监管的有效性[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look at what can cause companies to take concrete action, consider the following two examples of how Facebook has behaved. In 2018, a UN investigation found that Facebook had played a “determining role” in the ongoing genocide of the Rohingya, an ethnic minority in Mynamar described by UN Secretary-General Antonio Guterres as \"one of, if not the, most discriminated people in the world.\" Local activists had been warning Facebook executives that their platform was being used to spread hate speech and incite violence since as early as 2013. In 2015, they were warned that Facebook could play the same role in Myanmar that the radio broadcasts played during the Rwandan genocide (where a million people were killed). Yet, by the end of 2015, Facebook only employed four contractors that spoke Burmese. As one person close to the matter said, \"That’s not 20/20 hindsight. The scale of this problem was significant and it was already apparent.\" Zuckerberg promised during the congressional hearings to hire \"dozens\" to address the genocide in Myanmar (in 2018, years after the genocide had begun, including the destruction by fire of at least 288 villages in northern Rakhine state after August 2017).\n",
    "\n",
    "\n",
    "要了解是什么导致公司采取具体行动，请考虑以下两个Facebook行为的例子。2018，联合国的一项调查发现脸谱网在持续的罗辛亚人种族灭绝中扮演了 “决定性的角色”，迈纳马尔的一个少数民族被联合国秘书长安东尼奥 · 古特雷斯描述为 “世界上最受歧视的人之一”。\"当地活动人士一直警告脸书高管，早在2013，他们的平台就被用来传播仇恨言论和煽动暴力。在2015，他们被警告说，Facebook可以在缅甸发挥同样的作用，无线电广播在卢旺达种族灭绝 (那里有一百万人被杀) 期间发挥的作用。然而，到2015年底，脸书只雇佣了四名说缅甸语的承包商。正如一位知情人士所说，“这不是事后20/20。这个问题的规模很大，而且已经很明显了。\"扎克伯格在国会听证会上承诺雇佣 “几十人” 来解决缅甸的种族灭绝问题 (2018，种族灭绝已经开始多年后，包括2017年8月后若开邦北部至少288个村庄被大火摧毁)。[机器翻译]\n",
    "\n",
    "This stands in stark contrast to Facebook quickly [hiring 1,200 people in Germany](http://thehill.com/policy/technology/361722-facebook-opens-second-german-office-to-comply-with-hate-speech-law) to try to avoid expensive penalties (of up to 50 million euros) under a new German law against hate speech. Clearly, in this case, Facebook was more reactive to the threat of a financial penalty than to the systematic destruction of an ethnic minority.\n",
    "\n",
    "\n",
    "这与Facebook很快形成鲜明对比 [在德国雇佣1,200人] ( http://thehill.com/policy/technology/361722-facebook-opens-second-german-office-to-comply-with-hate-speech-law )，以尽量避免昂贵的处罚 (高达5000万欧元)，根据一项新的德国法律反对仇恨言论。显然，在这种情况下，脸谱网对经济处罚的威胁比对少数民族的系统性破坏反应更大。[机器翻译]\n",
    "\n",
    "In an [article on privacy issues](https://idlewords.com/2019/06/the_new_wilderness.htm), Maciej Ceglowski draws parallels with the environmental movement: \n",
    "\n",
    "\n",
    "在一篇 [关于隐私问题的文章] ( https://idlewords.com/2019/06/the_new_wilderness.htm )，Maciej Ceglowski与环境运动相似:[机器翻译]\n",
    "\n",
    "> : This regulatory project has been so successful in the First World that we risk forgetting what life was like before it. Choking smog of the kind that today kills thousands in Jakarta and Delhi was https://en.wikipedia.org/wiki/Pea_soup_fog[once emblematic of London]. The Cuyahoga River in Ohio used to http://www.ohiohistorycentral.org/w/Cuyahoga_River_Fire[reliably catch fire]. In a particularly horrific example of unforeseen consequences, tetraethyl lead added to gasoline https://en.wikipedia.org/wiki/Lead%E2%80%93crime_hypothesis[raised violent crime rates] worldwide for fifty years. None of these harms could have been fixed by telling people to vote with their wallet, or carefully review the environmental policies of every company they gave their business to, or to stop using the technologies in question. It took coordinated, and sometimes highly technical, regulation across jurisdictional boundaries to fix them. In some cases, like the https://en.wikipedia.org/wiki/Montreal_Protocol[ban on commercial refrigerants] that depleted the ozone layer, that regulation required a worldwide consensus. We’re at the point where we need a similar shift in perspective in our privacy law.\n",
    "\n",
    ">: 这个监管项目在第一世界如此成功，以至于我们冒着忘记之前生活的风险。今天在雅加达和德里造成数千人死亡的那种令人窒息的烟雾是 https://en.wikipedia.org/wiki/Pea_soup_fog [曾经是伦敦的象征]。俄亥俄州的凯霍加河曾经 http://www.ohiohistorycentral.org/w/Cuyahoga_River_Fire [可靠地着火]。在一个特别可怕的不可预见的后果的例子中，四乙基铅添加到汽油中 https://en.wikipedia.org/wiki/Lead -犯罪假设 [提高了全球暴力犯罪率] 五十年。通过告诉人们用他们的钱包投票，或者仔细审查他们所经营的每家公司的环境政策，这些危害都无法得到解决，或者停止使用有问题的技术。需要跨越司法管辖区的协调监管，有时甚至是高度技术性的监管来解决这些问题。在某些情况下，比如 https://en.wikipedia.org/wiki/Montreal_Protocol [禁止商业制冷剂] 消耗臭氧层，该法规需要全球共识。我们正处于这样一个阶段，我们需要在隐私法中做出类似的转变。[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rights and Policy\n",
    "\n",
    "# 权利和政策[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean air and clean drinking water are public goods which are nearly impossible to protect through individual market decisions, but rather require coordinated regulatory action. Similarly, many of the harms resulting from unintended consequences of misuses of technology involve public goods, such as a polluted information environment or deteriorated ambient privacy. Too often privacy is framed as an individual right, yet there are societal impacts to widespread surveillance (which would still be the case even if it was possible for a few individuals to opt out).\n",
    "\n",
    "\n",
    "清洁空气和清洁饮用水是公共产品，通过个人市场决策几乎不可能保护，而是需要协调一致的监管行动。同样，许多由技术误用的意外后果造成的伤害涉及公共物品，例如污染的信息环境或恶化的环境隐私。隐私经常被定义为个人权利，但对广泛的监视有社会影响 (即使少数人有可能选择退出，情况仍然如此)。[机器翻译]\n",
    "\n",
    "Many of the issues we are seeing in tech are actually human rights issues, such as when a biased algorithm recommends that Black defendants have longer prison sentences, when particular job ads are only shown to young people, or when police use facial recognition to identify protesters. The appropriate venue to address human rights issues is typically through the law.\n",
    "\n",
    "\n",
    "我们在技术上看到的许多问题实际上是人权问题，例如当一个有偏见的算法建议黑人被告有更长的刑期，当特定的招聘广告只向年轻人展示时，或者当警察使用面部识别来识别抗议者时。解决人权问题的适当场所通常是通过法律。[机器翻译]\n",
    "\n",
    "We need both regulatory and legal changes, *and* the ethical behavior of individuals. Individual behavior change can’t address misaligned profit incentives, externalities (where corporations reap large profits while offloading their costs and harms to the broader society), or systemic failures. However, the law will never cover all edge cases, and it is important that individual software developers and data scientists are equipped to make ethical decisions in practice.\n",
    "\n",
    "我们需要监管和法律的变化，* 和 * 个人的道德行为。个人行为改变不能解决错位的利润激励、外部效应 (公司在降低成本的同时获得大量利润并对更广泛的社会造成伤害) 或系统性失败。然而，法律永远不会涵盖所有的边缘案例，而且重要的是，单个软件开发人员和数据科学家有能力在实践中做出道德决策。[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cars: A Historical Precedent\n",
    "\n",
    "# 汽车: 历史先例[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problems we are facing are complex, and there are no simple solutions. This can be discouraging, but we find hope in considering other large challenges that people have tackled throughout history. One example is the movement to increase car safety, covered as a case study in [\"Datasheets for Datasets\"](https://arxiv.org/abs/1803.09010) by Timnit Gebru et al. and in the design podcast [99% Invisible](https://99percentinvisible.org/episode/nut-behind-wheel/). Early cars had no seatbelts, metal knobs on the dashboard that could lodge in people’s skulls during a crash, regular plate glass windows that shattered in dangerous ways, and non-collapsible steering columns that impaled drivers. However, car companies were incredibly resistant to even discussing the idea of safety as something they could help address, and the widespread belief was that cars are just the way they are, and that it was the people using them who caused problems.\n",
    "\n",
    "\n",
    "我们面临的问题是复杂的，没有简单的解决办法。这可能令人沮丧，但我们在考虑人们在历史上应对的其他重大挑战时找到了希望。一个例子是增加汽车安全性的运动，在 [“数据集数据表”] ( https://arxiv.org/abs/1803.09010 ) 由Timnit Gebru等人和在设计播客 [99% 不可见] ( https://99percentinvisible.org/episode/nut-behind-wheel/ )。早期的汽车没有安全带，仪表板上的金属旋钮可以在撞车时藏在人们的头骨里，普通的平板玻璃窗以危险的方式破碎，和不可折叠的转向柱，阻碍了驾驶员。然而，汽车公司甚至非常反对将安全理念讨论为他们可以帮助解决的问题，人们普遍认为汽车就是这样，是使用它们的人造成了问题。[机器翻译]\n",
    "\n",
    "It took consumer safety activists and advocates decades of work to even change the national conversation to consider that perhaps car companies had some responsibility which should be addressed through regulation. When the collapsible steering column was invented, it was not implemented for several years as there was no financial incentive to do so. Major car company General Motors hired private detectives to try to dig up dirt on consumer safety advocate Ralph Nader. The requirement of seatbelts, crash test dummies, and collapsible steering columns were major victories. It was only in 2011 that car companies were required to start using crash test dummies that would represent the average woman, and not just average men’s bodies; prior to this, women were 40% more likely to be injured in a car crash of the same impact compared to a man. This is a vivid example of the ways that bias, policy, and technology have important consequences.\n",
    "\n",
    "消费者安全活动家和倡导者花了几十年的时间才改变全国对话，认为汽车公司可能有一些责任，应该通过监管来解决。当可折叠转向柱被发明时，它已经好几年没有实施了，因为没有财政激励这样做。大型汽车公司通用汽车雇佣私人侦探试图挖掘消费者安全倡导者拉尔夫 · 纳德的丑闻。安全带、碰撞测试假人和可折叠转向柱的要求是主要的胜利。直到2011年，汽车公司才被要求开始使用代表普通女性的碰撞测试假人，而不仅仅是普通男性的身体; 在此之前，女性在同样影响的车祸中受伤的可能性比男性高40%。这是偏见、政策和技术产生重要后果的一个生动例子。[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "# # 结论[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coming from a background of working with binary logic, the lack of clear answers in ethics can be frustrating at first.  Yet, the implications of how our work impacts the world, including unintended consequences and the work becoming weaponized by bad actors, are some of the most important questions we can (and should!) consider.  Even though there aren't any easy answers, there are definite pitfalls to avoid and practices to follow to move toward more ethical behavior.\n",
    "\n",
    "\n",
    "从使用二进制逻辑的背景来看，伦理学中缺乏明确的答案起初可能会令人沮丧。然而，我们的工作如何影响世界的影响，包括意想不到的后果和被坏行为者武器化的工作，是我们可以 (也应该!) 考虑的一些最重要的问题。尽管没有任何简单的答案，但有明确的陷阱需要避免，实践需要遵循，以走向更道德的行为。[机器翻译]\n",
    "\n",
    "Many people (including us!) are looking for more satisfying, solid answers about how to address harmful impacts of technology. However, given the complex, far-reaching, and interdisciplinary nature of the problems we are facing, there are no simple solutions. Julia Angwin, former senior reporter at ProPublica who focuses on issues of algorithmic bias and surveillance (and one of the 2016 investigators of the COMPAS recidivism algorithm that helped spark the field of FAccT) said in [a 2019 interview](https://www.fastcompany.com/90337954/who-cares-about-liberty-julia-angwin-and-trevor-paglen-on-privacy-surveillance-and-the-mess-were-in):\n",
    "\n",
    "\n",
    "许多人 (包括我们!) 正在寻找关于如何解决技术有害影响的更令人满意、更可靠的答案。然而，鉴于我们面临的问题的复杂、深远和跨学科性质，没有简单的解决办法。朱莉娅 · 安格温，ProPublica的前高级记者，专注于算法偏差和监控问题 (也是帮助激发FAccT领域的COMPAS累犯算法的2016名调查人员之一) 在【一场2019访谈】中说 ( https://www.fastcompany.com/90337954/who-cares-about-liberty-julia-angwin-and-trevor-paglen-on-privacy-surveillance-and-the-mess-were-in ):[机器翻译]\n",
    "\n",
    "> : I strongly believe that in order to solve a problem, you have to diagnose it, and that we’re still in the diagnosis phase of this. If you think about the turn of the century and industrialization, we had, I don’t know, 30 years of child labor, unlimited work hours, terrible working conditions, and it took a lot of journalist muckraking and advocacy to diagnose the problem and have some understanding of what it was, and then the activism to get laws changed. I feel like we’re in a second industrialization of data information... I see my role as trying to make as clear as possible what the downsides are, and diagnosing them really accurately so that they can be solvable. That’s hard work, and lots more people need to be doing it. \n",
    "\n",
    "\n",
    ">: 我坚信，为了解决一个问题，你必须对它进行诊断，我们仍处于诊断阶段。如果你想想世纪之交和工业化，我不知道，我们有30年的童工，无限的工作时间，糟糕的工作条件，花了很多记者揭发丑闻和宣传来诊断这个问题，并对它有所了解，然后是改变法律的行动主义。我觉得我们正处于数据信息的第二次工业化.我认为我的角色是试图尽可能清楚地说明缺点是什么，并真正准确地诊断它们，以便它们可以解决。这是一项艰苦的工作，需要更多的人去做。[机器翻译]\n",
    "\n",
    "It's reassuring that Angwin thinks we are largely still in the diagnosis phase: if your understanding of these problems feels incomplete, that is normal and natural. Nobody has a “cure” yet, although it is vital that we continue working to better understand and address the problems we are facing.\n",
    "\n",
    "\n",
    "令人欣慰的是，安格温认为我们大部分仍处于诊断阶段: 如果你对这些问题的理解不完整，那是正常和自然的。尽管我们继续努力更好地理解和解决我们面临的问题至关重要，但还没有人有 “治愈方法”。[机器翻译]\n",
    "\n",
    "One of our reviewers for this book, Fred Monroe, used to work in hedge fund trading. He told us, after reading this chapter, that many of the issues discussed here (distribution of data being dramatically different than what a model was trained on, the impact feedback loops on a model once deployed and at scale, and so forth) were also key issues for building profitable trading models. The kinds of things you need to do to consider societal consequences are going to have a lot of overlap with things you need to do to consider organizational, market, and customer consequences—so thinking carefully about ethics can also help you think carefully about how to make your data product successful more generally!\n",
    "\n",
    "我们这本书的评论家之一弗雷德 · 门罗曾在对冲基金交易领域工作。读完这一章后，他告诉我们，这里讨论的许多问题 (数据分布与模型的训练有很大不同，模型上的影响反馈回路一旦部署和规模，等等) 也是建立有利可图的交易模型的关键问题。事情你需要做的事情，考虑社会后果将有一个很大的重叠，你需要做的事情要考虑组织，市场，和客户后果 -- 所以仔细思考道德规范也可以帮助你仔细思考如何让你的数据产品更普遍地成功![机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questionnaire\n",
    "\n",
    "# # 问卷调查[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Does ethics provide a list of \"right answers\"?\n",
    "1. How can working with people of different backgrounds help when considering ethical questions?\n",
    "1. What was the role of IBM in Nazi Germany? Why did the company participate as it did? Why did the workers participate?\n",
    "1. What was the role of the first person jailed in the Volkswagen diesel scandal?\n",
    "1. What was the problem with a database of suspected gang members maintained by California law enforcement officials?\n",
    "1. Why did YouTube's recommendation algorithm recommend videos of partially clothed children to pedophiles, even though no employee at Google had programmed this feature?\n",
    "1. What are the problems with the centrality of metrics?\n",
    "1. Why did Meetup.com not include gender in its recommendation system for tech meetups?\n",
    "1. What are the six types of bias in machine learning, according to Suresh and Guttag?\n",
    "1. Give two examples of historical race bias in the US.\n",
    "1. Where are most images in ImageNet from?\n",
    "1. In the paper [\"Does Machine Learning Automate Moral Hazard and Error\"](https://scholar.harvard.edu/files/sendhil/files/aer.p20171084.pdf) why is sinusitis found to be predictive of a stroke?\n",
    "1. What is representation bias?\n",
    "1. How are machines and people different, in terms of their use for making decisions?\n",
    "1. Is disinformation the same as \"fake news\"?\n",
    "1. Why is disinformation through auto-generated text a particularly significant issue?\n",
    "1. What are the five ethical lenses described by the Markkula Center?\n",
    "1. Where is policy an appropriate tool for addressing data ethics issues?\n",
    "\n",
    "1.伦理道德是否提供 “正确答案” 列表？\n",
    "1.在考虑伦理问题时，如何与不同背景的人一起工作？\n",
    "1.IBM在纳粹德国的作用是什么？为什么公司会像它那样参与？工人为什么参加？\n",
    "1.在大众柴油丑闻中被监禁的第一个人的角色是什么？\n",
    "1.加州执法官员维护的可疑帮派成员数据库有什么问题？\n",
    "1.为什么YouTube的推荐算法向恋童癖者推荐部分穿着的孩子的视频，即使谷歌没有员工编程这个功能？\n",
    "1.度量的中心性有什么问题？\n",
    "1.为什么Meetup.com的科技会议推荐系统不包括性别？\n",
    "1.根据Suresh和Guttag的说法，机器学习中的六种偏差是什么？\n",
    "1.举两个美国历史种族偏见的例子。\n",
    "1.ImageNet中的大多数图像来自哪里？\n",
    "1.论文 [《机器学习是否自动化了道德风险和错误》] ( https://scholar.harvard.edu/files/sendhil/files/aer.p20171084.pdf ) 为什么发现鼻窦炎可以预测中风？\n",
    "1.什么是表征偏差？\n",
    "1.机器和人在决策方面有什么不同？\n",
    "1.虚假信息和 “假新闻” 一样吗？\n",
    "1.为什么通过自动生成的文本提供虚假信息是一个特别重要的问题？\n",
    "1.马库拉中心描述的五个伦理镜头是什么？\n",
    "1.政策在哪里是解决数据伦理问题的合适工具？[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Research:\n",
    "\n",
    "# 进一步研究:[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read the article \"What Happens When an Algorithm Cuts Your Healthcare\". How could problems like this be avoided in the future?\n",
    "1. Research to find out more about YouTube's recommendation system and its societal impacts. Do you think recommendation systems must always have feedback loops with negative results? What approaches could Google take to avoid them? What about the government?\n",
    "1. Read the paper [\"Discrimination in Online Ad Delivery\"](https://arxiv.org/abs/1301.6822). Do you think Google should be considered responsible for what happened to Dr. Sweeney? What would be an appropriate response?\n",
    "1. How can a cross-disciplinary team help avoid negative consequences?\n",
    "1. Read the paper \"Does Machine Learning Automate Moral Hazard and Error\". What actions do you think should be taken to deal with the issues identified in this paper?\n",
    "1. Read the article \"How Will We Prevent AI-Based Forgery?\" Do you think Etzioni's proposed approach could work? Why?\n",
    "1. Complete the section \"Analyze a Project You Are Working On\" in this chapter.\n",
    "1. Consider whether your team could be more diverse. If so, what approaches might help?\n",
    "\n",
    "1.阅读文章 “当一个算法削减你的医疗保健时会发生什么”。将来如何避免这样的问题？\n",
    "1.研究以了解更多关于YouTube推荐系统及其社会影响的信息。你认为推荐系统必须总是有负面结果的反馈循环吗？谷歌可以采取什么方法来避免它们？政府呢？\n",
    "1.阅读论文 [《网络广告投放中的歧视》] ( https://arxiv.org/abs/1301.6822 )。你认为谷歌应该对斯威尼博士的遭遇负责吗？什么是适当的回应？\n",
    "1.跨学科团队如何帮助避免负面后果？\n",
    "1.阅读论文《机器学习是否自动化了道德风险和错误》。你认为应该采取什么行动来处理本文中确定的问题？\n",
    "1.阅读文章 “我们如何防止基于人工智能的伪造？” 你认为Etzioni提出的方法可行吗？为什么？\n",
    "1。完成本章中的 “分析你正在进行的项目” 一节。\n",
    "1.考虑你的团队是否可以更加多样化。如果是这样，哪些方法可能有所帮助？[机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning in Practice: That's a Wrap!\n",
    "\n",
    "# # 实践中的深度学习: 这是一个包裹![机器翻译]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You've made it to the end of the first section of the book. In this section we've tried to show you what deep learning can do, and how you can use it to create real applications and products. At this point, you will get a lot more out of the book if you spend some time trying out what you've learned. Perhaps you have already been doing this as you go along—in which case, great! If not, that's no problem either... Now is a great time to start experimenting yourself.\n",
    "\n",
    "\n",
    "恭喜!你已经完成了这本书第一部分的结尾。在本节中，我们试图向您展示深度学习可以做什么，以及如何使用它来创建真正的应用程序和产品。在这一点上，如果你花些时间尝试你学到的东西，你会从书中得到更多。也许你已经这样做了-在这种情况下，太好了!如果没有，那也没问题.现在是开始尝试自己的好时机。[机器翻译]\n",
    "\n",
    "If you haven't been to the [book's website](https://book.fast.ai) yet, head over there now. It's really important that you get yourself set up to run the notebooks. Becoming an effective deep learning practitioner is all about practice, so you need to be training models. So, please go get the notebooks running now if you haven't already! And also have a look on the website for any important updates or notices; deep learning changes fast, and we can't change the words that are printed in this book, so the website is where you need to look to ensure you have the most up-to-date information.\n",
    "\n",
    "\n",
    "如果你没有去过 [书的网站] ( https://book.fast.ai ) 然而，现在去那边。让自己准备好运行笔记本电脑非常重要。成为一名有效的深度学习实践者是关于实践的，所以你需要成为培训模特。所以，如果你还没有运行笔记本，请现在就去运行!也可以在网站上查看任何重要的更新或通知; 深度学习变化很快，我们不能改变这本书里印的单词，因此，网站是你需要寻找的地方，以确保你拥有最新的信息。[机器翻译]\n",
    "\n",
    "Make sure that you have completed the following steps:\n",
    "\n",
    "\n",
    "确保您已完成以下步骤:[机器翻译]\n",
    "\n",
    "- Connect to one of the GPU Jupyter servers recommended on the book's website.\n",
    "- Run the first notebook yourself.\n",
    "- Upload an image that you find in the first notebook; then try a few different images of different kinds to see what happens.\n",
    "- Run the second notebook, collecting your own dataset based on image search queries that you come up with.\n",
    "- Think about how you can use deep learning to help you with your own projects, including what kinds of data you could use, what kinds of problems may come up, and how you might be able to mitigate these issues in practice.\n",
    "\n",
    "\n",
    "-连接到本书网站上推荐的GPU Jupyter服务器之一。\n",
    "-自己运行第一个笔记本。\n",
    "-上传您在第一个笔记本中找到的图像; 然后尝试不同种类的不同图像，看看会发生什么。\n",
    "-运行第二个笔记本，根据您提出的图像搜索查询收集您自己的数据集。\n",
    "-想想如何使用深度学习来帮助您完成自己的项目，包括您可以使用哪些类型的数据，可能会出现哪些类型的问题，以及如何在实践中减轻这些问题。[机器翻译]\n",
    "\n",
    "In the next section of the book you will learn about how and why deep learning works, instead of just seeing how you can use it in practice. Understanding the how and why is important for both practitioners and researchers, because in this fairly new field nearly every project requires some level of customization and debugging. The better you understand the foundations of deep learning, the better your models will be. These foundations are less important for executives, product managers, and so forth (although still useful, so feel free to keep reading!), but they are critical for anybody who is actually training and deploying models themselves.\n",
    "\n",
    "在这本书的下一部分，你将了解深度学习是如何以及为什么工作的，而不仅仅是看看你如何在实践中使用它。了解如何和为什么对从业者和研究人员都很重要，因为在这个相当新的领域，几乎每个项目都需要一定程度的定制和调试。你越了解深度学习的基础，你的模型就会越好。这些基础对高管、产品经理等来说不太重要 (尽管仍然有用，所以请随时阅读!)，但是它们对于任何真正自己训练和部署模型的人来说都是至关重要的。[机器翻译]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
